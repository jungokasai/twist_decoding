FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting. This paper presents a novel solution to an incremental classification problem based on a dual memory system.
Multi-view learning improves unsupervised sentence representation learning . Approach uses different, complementary encoders of the input sentence and consensus maximization. The paper presents a multi-view framework for improving sentence representation in NLP tasks using generative and discriminative objective architectures. This paper shows that multi-view frameworks are more effective than using individual encoders for learning sentence representations.
We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation. A method for learning object representations from pixels for doing reinforcement learning. The paper proposes a neural architecture to map video streams to a discrete collection of objects, without human annotations, using an unsupervised pixel reconstruction loss.
A large-scale dataset for training attention models for object recognition leads to more accurate, interpretable, and human-like object recognition. Argues recent gains in visual recognition stem from using visual attention mechanisms in deep convolutional networks, which learn where to focus through a weak form of supervision based on image class labels. Presents a new take on attention in which a large attention dataset is collected and used to train a NN in a supervised manner to exploit self-reported human attention. This paper proposes a new approach to use more informative signals, specifically, regions humans deem important on images, to improve deep convolutional neural networks.
We proposed a time-efficient defense method against one-step and iterative adversarial attacks. Propose a novel, computationaly efficient method named e2SAD which generates sets of two training adversarial samples for each clean training sample. The paper introduces a two-step adversarial defense method, to generate two adversarial examples per clean sample and include them in the actual training loop to achieve robustness and claiming it can outperform more expensive iterative methods. The paper presents a 2-step approach to generate strong adversarial examples at a far lesser cost as compared to recent iterative multi-step adversarial attacks.
A comparison of five deep neural network architectures for detection of malicious domain names shows surprisingly little difference. Authors propose using five deep architectures for the cybersecurity task of domain generation algorithm detection. Applies several NN architectures to classify url's between begign and malware related URLs. This paper proposes to automatically recognize domain names as malicious or benign by deep networks trained to directly classify the character sequence as such.
Adversarial learning methods encourage NLI models to ignore dataset-specific biases and help models transfer across datasets. The paper proposes an adversarial setup to mitigate annotation artifacts in natural language inference data . This paper presents a method for removing bias of a textual entailment model through an adversarial training objective.
A new state-of-the-art approach for knowledge graph embedding. Presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base. This paper proposes an approach to knowledge graph embedding by modeling relations as rotations in the complex vector space. Proposes a method for graph embedding to be used for link prediction .
We modified the CNN using HyperNetworks and observed better robustness against adversarial examples. Improving the robustness and reliability of deep convolution neural networks by using data-dependent convolution kernels .
We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression. The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task as having its own final layer which could be a ridge regression or a logistic regression. This paper proposes a meta-learning approach for the problem of few-shot classification, they use a  method based on parametrizing the learner for each task by a closed-form solver.
We provide a new perspective on training a machine learning model from scratch in hierarchical label setting, i.e. thinking of it as two-way communication between human and algorithms, and study how we can both measure and improve the efficiency. Introduces a new Active Learning setting where the oracle offers a partial or weak label instead of querying for a particular example's label, leading to a simpler retrieval of information. This paper proposes a method of active learning with partial feedback that outperforms existing baselines under a limited budget. The paper considers a multiclass classification problem in which labels are grouped in a given number M of subsets, which contain all individual labels as singletons.
We show that Wasserstein spaces are good targets for embedding data with complex semantic structure. Learns embeddings in a discrete space of probability distributions, using a minimized, regularised version of Wasserstein distances. The paper describes a new embedding method that embeds data to the space of probability measures endowed with the Wasserstein distance. The paper proposes embedding the data into low-dimensional Wasserstein spaces, which can capture the underlying structure of the data more accurately.
A clustering algorithm that performs joint nonlinear dimensionality reduction and clustering by optimizing a global continuous objective. Presents a clustering algorithm by jointly solving deep autoencoder and clustering as a global continuous objective, showing better results than state-of-the-art clustering schemas. Deep Continuous Clustering is a clustering method that integrates the autoencoder objective with the clustering objective then train using SGD.
To accelerate the computation of convolutional neural networks, we propose a new two-step pruning technique which achieves a higher Winograd-domain weight sparsity without changing the network structure. Proposes a spatial-Winograd pruning framework which allows pruned weight from the spatial domain to be kept in the Winograd domain and improves the sparsity of the Winograd domain. Proposes two techniques for pruning convolutional layers which use the Winograd algorithm .
We propose a Bayesian nonparametric model for federated learning with neural networks. Uses beta process to do federated neural matching. The paper considers federate learning of neural networks, where data is distributed on multiple machines and the allocation of data points is potentially inhomogenous and unbalanced.
General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space. Proposes a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly.
We show that rare but catastrophic failures may be missed entirely by random testing, which poses issues for safe deployment. Our proposed approach for adversarial testing fixes this. Proposes a method which learns a failure probability predictor for a learned agent, leading to predictions of which initial states cause a system to fail. This paper proposes an importance sampling approach to sampling failure cases for RL algorithms based on a function learned via a neural network on failures that occur during agent training . This paper proposed an adversarial approach to identifying catastrophic failure cases in reinforcement learning.
To address posterior collapse in VAEs, we propose a novel yet simple training procedure that aggressively optimizes inference network with more updates. This new training procedure mitigates posterior collapse and leads to a better VAE model. Looks into the phenomenon of posterior collapse, showing that increased training of the inference network can reduce the problem and lead to better optima. Authors propose changing the training procedure of VAEs only as a solution to posterior collapse, leaving the model and objective untouched.
Generatively discover meaningful, novel entity pairs with a certain medical relationship by purely learning from the existing meaningful entity pairs, without the requirement of additional text corpus for discriminative extraction. Presents a variational autoencoder for generating entity pairs given a relation in a medical setting. In the medical context, this paper describes the classic problem of "knowledge base completion" from structured data only.
We closely analyze the VAE objective function and draw novel conclusions that lead to simple enhancements. Proposes a two-stage VAE method to generate high-quality samples and avoid blurriness. This paper analyzes the Gaussian VAEs. The paper provides a number of theoretical results on "vanilla" Gaussian Variational Auto-Encoders, which are then used to build a new algorithm called "2 stage VAEs".
A hyperparameter tuning algorithm using discrete Fourier analysis and compressed sensing . Investigates problem of optimizing hyperparameters under the assumption that the unknown function can be approximated, showing that the approximate minimization can be performed over the boolean hypercube. The paper explores hyperparameter optimization by assuming structure in the unknown function mapping hyperparameters to classification accuracy .
A new method for gradient-descent inference of permutations, with applications to latent matching inference and supervised learning of permutations with neural networks . The paper utilizes finite approximation of the Sinkhorn operator to describe how one can construct a neural network for learning from permutation valued training data. The paper proposes a new method that approximates the discrete max-weight for learning latent permutations .
A novel differentiable neural architecture search framework for mixed quantization of ConvNets. The authors introduce a new method for neural architecture search which selects the precision quantization of weights at each neural network layer, and use it in the context of network compression. The paper presents a new approach in network quantization by quantizing different layers with different bit-widths and introduces a new differentiable neural architecture search framework.
Smooth Loss Function for Top-k Error Minimization . Proposes using top-k loss with deep models to address the problem of class confusion with similar classes both present or absent of the training dataset. Smoothes the top-k losses. This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks.
We introduce Mol-CycleGAN - a new generative model for optimization of molecules to augment drug design. The paper presents an approach for optimizing molecular properties based on the application of CycleGANs to variational autoencoders for molecules and employs a domain-specific VAE called Junction Tree VAE (JT-VAE). This paper uses a variational autoencoders to learn a translation function, from the set of molecules without the interested property to the set of molecules with the property.
We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification . This paper proposes to transfer the classifier from the model for face classification to the task of alignment and verification. The manuscript presents experiments on distilling knowledge from a face classification model to student models for face alignment and verification.
Improving session-based recommendations with RNNs (GRU4Rec) by 35% using newly designed loss functions and sampling. This paper analyzes existing loss functions for session-based recommendations and proposes two novel losses functions which add a weighting to existing ranking-based loss functions . Presents modifications on top of earlier work for session-based recommendation using RNN by weighting negative examples by their "relevance" This paper discusses the issues for optimizing the loss functions in GRU4Rec, proposes tricks for optimizing, and suggests an enhanced version.
We develop a lifelong learning approach to transfer learning based on PAC-Bayes theory, whereby priors are adjusted as new tasks are encountered thereby facilitating the learning of novel tasks. A novel PAC-Bayesian risk bound that serves as an objective function for multi-task machine learning, and an algorithm for minimizing a simplified version of that objective function. Extends existing PAC-Bayes bounds to multi-task learning, to allow the prior to be adapted across different tasks.
A tailored version of Adam for training DNNs, which bridges the generalization gap between Adam and SGD. Proposes a variant of ADAM optimization algorithm that normalizes weights of each hidden unit using batch normalization . Extension of the Adam optimization algorithm to preserve the update direction by adapting the learning rate for the incoming weights to a hidden unit jointly using the L2 norm of the gradient vector .
We show how we can use the successor representation to discover eigenoptions in stochastic domains, from raw pixels. Eigenoptions are options learned to navigate the latent dimensions of a learned representation. Extends the idea of eigenoptions to domains with stochastic transitions and where state features are learned. Shows equivalence between proto value functions and successor representations and derives the idea of eigen options as a mechanism in option discovery . The paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions can be used to define options called “eigenoptions”.
We provide improved upper bounds for the number of linear regions used in network expressivity, and an highly efficient algorithm (w.r.t. exact counting) to obtain probabilistic lower bounds on the actual number of linear regions. Contributes to the study of the number of linear regions in RELU neural networks by using an approximate probabilistic counting algorithm and analysis . Builds off previous work studying the counting of linear regions in deep neural networks, and improves the upper bound previously proposed by changing the dimensionality constraint . The paper deals with expressiveness of a piecewise linear neural network, characterized by the number of linear regions of the function modeled, and leverages probabilistic algorithms to compute the bounds faster, and proves tighter bounds.
A biologically inspired working memory that can be integrated in recurrent visual attention models for state of the art performance . Introduces a new network architecture inspired by visual attentive working memory and applies it to classification tasks and using it as a generative model . The paper augments the recurrent attention model with a novel Hebb-Rosenblatt working memory model and achieves competitive results on MNIST .
The paper uses Variational Auto-Encoding and network conditioning for Musical Timbre Transfer, we develop and generalize our architecture for many-to-many instrument transfers together with visualizations and evaluations. Proposes a Modulated Variational auto-Encoder to perform musical timbre transfer by replacing the usual adversarial translation criterion by a Maxiimum Mean Discrepancy . Describes a many-to-many model for musical timbre transfer which builds on recent developments in domain and style transfer . Proposes a hybrid VAE-based model to perform timbre transfer on recordings of musical instruments.
We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena. A theoretical analysis of autoencoders with weights tied between encoder and decoder (weight-tied) via mean field analysis . Analyses the performances of weighted tied auto-encoders by building on recent progress in analysis of high-dimensional statistics problems and specifically, the message passing algorithm . This paper studies auto-encoders under several assumptions, and points out that this model of random autoencoder can be elegantly and rigorously analysed with one-dimensional equations.
Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and kernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE). This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution . Introduces a variation on the Wasserstein AudoEncoders which is a novel regularized auto-encoder architecture that proposes a specific choice of the divergence penalty . This paper proposes the Cramer-Wold autoencoder, which uses the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem.
We use a GAN discriminator to perform an approximate rejection sampling scheme on the output of the GAN generator. Proposes a rejection sampling algorithm for sampling from the GAN generator. This paper proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling, to help filter ‘good’ samples from GANs’ generator.
A simple fast method for extracting visual features from convolutional neural networks . Proposes a fast way to learn convolutional features that later can be used with any classifier by using reduced numbers of training epocs and specific schedule delays of learning rate . Use a learning rate decay scheme that is fixed relative to the number of epochs used in training and extract the penultimate layer output as features to train a conventional classifier.
We provide new insights and interpretations of RNNs from a max-affine spline operators perspective. Rewrites equations of Elman RNN in terms of so-called max-affine spline operators . Provide a novel approach toward understanding RNNs using max-affline spline operators (MASO) by rewriting them with piecewise affine and convex activations MASOs . The authors build upon max-affine spline operator interpetation of a substantial class of deep networks, focusing on Recurrent Neural Networks using noise in initial hidden state acts as regularization .
We scale Neural Theorem Provers to large datasets, improve the rule learning process, and extend it to jointly reason over text and Knowledge Bases. Proposes an extension of the Neural Theorem Provers system that addresses the main issues of this model by reducing the time and space complexity of the model . Scales NTPs by using approximate nearest neighbour search over facts and rules during unification and suggests parameterizing predicates using attention over known predicates . improves upon the previously proposed Neural Theorem Prover approach by using nearest neighbor search.
Generalization of the relationships learnt between pairs of images using a small training data to previously unseen types of images using an explainable dynamical systems model, Reservoir Computing, and a biologically plausible learning technique based on analogies. Claims results of "combining transformations" in the context of RC by using an echo-state network with standard tanh acctivations with the difference that recurrent weights are not trained . Novel method of classifying different distorions of MNIST data . The paper uses an echo state network to learn to classify image transformations between pairs of images into one of fives classes.
We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations with certified privacy/fairness guarantees . This paper uses a GAN model to provide an overview of the related work to Private/Fair Representation Learning (PRL). This paper presents an adversarial-based approach for private and fair representations by learned distortion of data that minimises the dependency on sensitive variables while the degree of distortion is constrained. The authors describe a framework of how to learn a demographic parity representation that can be used to train certain classifiers.
We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding . This paper introduces a family of attack on confidence thresholding algortihms, focusing mainly on evaluation methodologies. Proposes an evaluation method for confidence thresholding defense models and an approach for generating adversarial examples by choosing the wrong class with the most confidence when using targeted attacks . The paper presents an evaluation methodology for evaluating attacks on confidence thresholding methods and proposes a new kind of attack.
a novel method to learn with sparse reward using adversarial reward re-labeling . Proposes to use a competitive multi-agent setting for encouraging exploration and shows that CER + HER > HER ~ CER . Propose a new method for learning from sparse rewards in model-free reinforcement learning settings and densifying reward . To address the sparse reward problems and encourage exploration in RL algorithms, the authors propose a relabeling strategy called Competitive Experience Reply (CER).
Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more. Describes the conditioned GAN model to generate speaker conditioned Mel spectra by augmenting the z-space corresponding to the identification . This paper proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitating fine-grained control over various attributes . This paper proposes a model that can control non-annotated attributes such as speaking style, accent, background noise, etc.
Enabling Visual Question Answering models to count by handling overlapping object proposals. This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count. Focuses on a counting problem in visual question answering using attention mechanism and propsoe a differentiable counting compoent which explicitly counts the number of objects . This paper tackles the object counting problem in visual question answering, it proposes many heuristics to find the correct count.
A simple and training-free approach for sentence embeddings with competitive performance compared with sophisticated models requiring either large amount of training data or prolonged training time. Presented a new training-free way of generating sentence embedding with systematic analysis . Proposes a new geometry-based method for sentence embedding from word embedding vectors by quantifying the novelty, significance, and corpus-uniqueness of each word . This paper explores sentence embedding based on orthogonal decomposition of the spanned space by word embeddings .
We propose novel extensions of Prototypical Networks that are augmented with the ability to use unlabeled examples when producing prototypes. This paper is an extension of a prototypical network that considers employing the unlabeled examples available to help train each episode . Studies the problem of semi-supervised few-shot classification by extending the prototypical networks into the setting of semi-supervised learning with example from distractor classes . Extends the Prototypical Network to the semi-supervised setting by updating prototypes using assigned pseudo-labels, dealing with distractors, and weighing samples using distance to the original prototypes.
We theoretically prove that linear interpolations are unsuitable for analysis of trained implicit generative models. Studies the problem of when the linear interpolant between two random variables follows the same distribution, related to prior distribution of an implicit generative model . This work asks how to interpolate in the latent space given a latent variable model.
Detection of lung nodule starting from projection data rather than images. DNNs are used for patch-based lung nodule detection in CT projection data. Jointly modeling computed tomography reconstruction and lesion detection in the lung by training the mapping from raw sinogram to detection outputs in an end-to-end manner . Presents an end to end training of a CNN architecture that combines CT image signal processing and image analysis.
We quantitatively and qualitatively evaluate deep reinforcement learning based navigation methods under a variety of conditions to answer the question of how close they are to replacing classical path planners and mapping algorithms. Evaluate a Deep RL-based model on training mazes by measuring repeated latency to goal and comparison to shortest route .
We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters . Proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-toend through differentiable Bayesian Filters and two different versions of the Unscented Kalman Filter . Revisits Bayes filters and evaluates the benefit of training the observation and process noise models while keeping all other models fixed . This paper presents a method to learn and use state and observation dependent noise in traditional Bayesian filtering algorithms. The approach consists of constructing a neural network model which takes as input the raw observation data and produces a compact representation and an associated diagonal covariance.
We rethink the way information can be exploited more efficiently in the knowledge graph in order to improve performance on the Zero-Shot Learning task and propose a dense graph propagation (DGP) module for this purpose. This authors propose a solution to the problem of over-smoothing in Graph conv networks by allowing dense propagation between all related nodes, weighted by the mutual distance. Proposes a novel graph convolutional neural network to tackle the problem of zero-shot classification by using relational structures between classes as input of graph convolutional networks to learn classifiers of unseen classes .
A capsule-based semantic segmentation, in which the probabilities of the class labels are traced back through capsule pipeline. The authors present a trace-back mechanism to associate lowest level of Capsules with their respective classes . Proposes a traceback layer for capsule networks to do semantic segmentation and makes explicit use of part-whole relationship in the capsule layers . Proposes a trace-back method based on the CapsNet concept of Sabour to perform a semantic segmentation in parallel to classification.
We look at SGD as a trajectory in the space of probability measures, show its connection to Markov processes, propose a simple Markov model of SGD learning, and experimentally compare it with SGD using information theoretic quantities. Constructs a Markov chain that follows a shorted path in TV metric on P and shows that trajectories of SGD and \alpha-SMLC have similar conditional entropy . Studies the trajectory of H(\hat{y}) versus H(\hat{y}|y) on the information plane for stochastic gradient descent methods for training neural networks . Describes SGD from the point of view of the distribution p(y',y) where y is (a possibly corrupted) true class-label and y' a model prediction.
This paper proposes a method to automate the design of stochastic gradient MCMC proposal using meta learning approach. Prsents a meta-learning approach to automatically design MCMC sampler based on Hamiltonian dynamics to mix faster on problems similar to training problems . Parameterizes diffusion and curl matrices by neural networks and meta-learn and optimize an sg-mcmc algorithm.
Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks . Proposes an energy-based formulation to the BEGAN modeal and modifies it to include an image quality assessment based term . Proposes some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score.
We introduce a simple variant of momentum optimization which is able to outperform classical momentum, Nesterov, and Adam on deep learning tasks with minimal hyperparameter tuning. Introduces a variant of momentum that aggregates several velocities with different dampening coefficients that significantly decreases oscillation . Proposed an aggregated momentum methods for gradient based optimization by using multiple velocity vectors with different damping factors instead of a single velocity vector to improve stability . The authors combine several update steps together to achieve aggregated momentum also demonstrating it is more stable than the other momentum methods .
We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time . This paper proposes the Recurrent Discounted Attention (RDA), an extension to Recurrent Weighted Average (RWA) by adding a discount factor. Extends the recurrent weight average to overcome the limitation of the original method while maintaining its advantage and proposes the method of using Elman nets as the base RNN .
It is possible to learn a zero-centered Gaussian distribution over the weights of a neural network by learning only variances, and it works surprisingly well. This paper investigates the effects of mean of variational posterior and proposes variance layer, which only uses variance to store information . Studies variance neural networks which approximate the posterior of Bayesian neural networks with zero-mean Gaussian distributions .
We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification. Proposes a new network of GCNs with two approaches: a fully connected layer on top of stacked features and attention mechanism that uses scalar weight per GCN. Presents a Network of Graph Convolutional Networks that uses random walk statistics to extract information from near and distant neighbors in the graph .
A fast pruning algorithm for fully connected DNN layers with theoretical analysis of degradation in Generalisation Error. Presents a cheap pruning algorithm for dense layers of DNNs. Proposes a solution to the problem of pruning DNNs by posing the Net-trim objective function as a Difference of convex(DC) function.
We propose a new hybrid temporal network that achieves state-of-the-art performance on video action segmentation on three public datasets. Discusses the problem of action segmentation in long videos, up to 10 minutes long by using a temporal convolutional encoder-decoder architecture . Proposes a combination of temporal convolutional and recurrent network for video action segmentation .
This paper proposes to transfer knowledge from deep model to shallow one by mimicking features stage by stage. Explains a stage by stage knowledge transer method by using different structures of resnets . This paper proposes dividing a network into multiple parts and distilling each part sequentially to improve distillation performance in deep teacher networks .
Improvements to adversarial robustness, as well as provable robustness guarantees, are obtained by augmenting adversarial training with a tractable Lipschitz regularization . Explores augmenting the training loss with an additional gradient regularization term to improve robustness of models against adversarial examples . Uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form.
A model combining elimination and selection for answering multiple choice questions . Gives an elaboration on the Gated Attention Reader adding gates based on answer elimination in multiple choice reading comprehension . This paper proposes the use of an elimination gate in model architectures for reading comprehension tasks but does not achieve state-of-the-art results . This paper propses a new multi-choice reading comprehension model based on the idea that some options should be eliminated to infer better passage/question representations.
We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks. Proposes a new approach for fully decentralized training in multi-agent reinforcement learning . Tackles the problem of endowing RL agents with recursive reasoning capabilities in a multi-agent setting based on the hypothesis that recursive reasoning is beneficial for them to converge to non-trival equilibria . The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making.
A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing ‘unambiguous’ gradients. Proposes a variance-based gradient compression method to reducee the communication overhead of distributed deep learning . Proposes a novel way of compressing gradient updates for distributed SGD in order to speed up overall execution . Introduces variance-based gradient compression method for efficient distributed training of neural networks and measuring ambuiguity.
A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization . Improves the correlation alignment approach to domain adaptation by replacing the Euclidean distance with the geodesic Log-Euclidean distance between two covariance matices, and automatically selecting the balancing cost by the entropy on the target domain. Proposal for minimal-entropy correlation alignment, an unsupervised domain adaptation algorithm which links together entropy minimization and correlation alignment methods.
We propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks. This paper applies the notion of conceptors, a form a regulariser, to prevent forgetting in continual learning in the training of neural networks on sequential tasks. Introduces a method for learning new tasks, without interfering previous tasks, using conceptors.
Analyze the reason for neural response generative models preferring universal replies; Propose a method to avoid it. Investigates the problem of universal replies plaguing the Seq2Seq neural generation models . The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase.
We leverage the syntactic structure of source code to generate natural language sequences. Presents a method for generating sequences from code by parsing and producing a syntax tree . This paper introduces an AST-based encoding for programming code and shows its effectiveness in the tasks of extreme code summarization and code captioning. This paper presents a new code-to-sequence model that leverages the syntactic structure of programming languages to encode source code snippets and then decode them to natural language .
We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets. Describes a novel attentional mechanism applid to fine-grained recognition that consistently improves the recognition accuracy of the baseline . This paper proposes a feed-forward attention mechanism for fine-grained image classification . This paper presents an interesting attention mechanism for fine-grained image classification.
We replace normal convolutions with adaptive convolutions to improve GANs generator. Proposes to replace convolutions in the generator with an Adaptive Convolution Block that learns to generate convolution weigths adn biases of upsampling operations adaptively per pixel location . Uses Adaptive Convolution in the context of GANs with a block called AdaConvBlock that replaces regular Convolution, this gives more local context per kernel weight so that it can generate locally flexible objects.
We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines. Proposes a method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent . Proposal for an online distillation method called co-distillation, applied at scale, where two different models are trained to match predictions of the other model in addition to minimizing its own loss. Online distillation technique is introduced to accelerate traditional algorithms for large-scaled distributed neural network training .
We present an algorithm for speeding up SVM training on massive data sets by constructing compact representations that provide efficient and provably approximate inference. Studies the approach of coreset for SVM and aims at sampling a small set of weighted points such that the loss function over the points provably approximates that over the whole dataset . The paper suggests an importance sampling based Coreset construction to represent large training data for SVMs .
We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning. Provided a convergence analysis of Sign SGD algorithm for non-covex cases . The paper explores an algorithm that uses the sign of the gradients instead of actual gradients for training deep models .
We introduce a transparent middleware for neural network acceleration, with own compiler engine, achieving up to 11.8x speed up on CPUs and 2.3x on GPUs. This paper proposes a transparent middleware layer for neural network acceleration and obtains some acceleration results on basic CPU and GPU architectures .
We propose conic convolution and the 2D-DFT to encode rotation equivariance into an neural network. In the context of image classification, the paper proposes a convolutional neural network architecture with rotation-equivariant feature maps that are eventually made rotation-invariant by using the magnitude of the 2D discrete Fourier transform (DFT). Authors provide a rotation invariant neural network via combining conic convolution and 2D-DFT .
We introduce a novel feed-forward framework to generate visual metamers . Proposes a NeuroFovea model for generation of point-of-fixation metamers by using a style transfer approach via and Encoder-Decoder style architecture . An analysis of metamerism and a model capable of rapidly producing metamers of value for experimental psychophysics and other domains. The paper proposes a fast method for generating visual metamers – physically different images that cannot be told apart from an original – via foveated, fast, arbitrary style transfer .
We show that training feedforward relu networks with a weak regularizer results in a maximum margin and analyze the implications of this result. Studies margin theory for neural sets  and shows that max margin is monotonically increasing in size of the network . This paper studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations, obtaining a generalization upper bound which does not increase with the network size.
A distributed architecture for deep reinforcement learning at scale, using parallel data-generation to improve the state of the art on the Arcade Learning Environment benchmark in a fraction of the wall-clock training time of previous approaches. Examines a distirbuted Deep RL system in which experiences, rather than gradients, are shared between the parallel works and the cetralized learner . A parallel approach to DQN training, based on the idea of having multiple actors collecting data in parallel while a single learner trains the model from experiences sampled from central replay memory. This paper proposes a distributed architecture for deep reinforcement learning at scale, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework .
Neural architectures providing representations of irregularly observed signals that provably enable signal reconstruction. Proves that convolutional neural networks with Leaky ReLU activation function are nonlinear frames, with similar results for non-uniformly sampled time-series . This article considers neural networks over time-series and show that the first convolutional filters can be chosen to represent a discrete wavelet transform.
Phrase-based attention mechanisms to assign attention on phrases, achieving token-to-phrase, phrase-to-token, phrase-to-phrase attention alignments, in addition to existing token-to-token attentions. Paper presents an attention mechanism that computes a weighted sum over not only single tokens but ngrams(phrases).
Deep networks are more likely to be confidently wrong when testing on unexpected data. We propose an experimental methodology to study the problem, and two methods to reduce confident errors on unknown input distributions. Proposes two ideas for reducing overconfident wrong predictions: "G-distillation" of am ensemble with extra unsupervised data and Novelty Confidence Reduction using novelty detector . The authors propose two methods for estimating classification confidence on novel unseen data distributions. The first idea is to use ensemble methods as the base approach to help identify uncertain cases and then use distillation methods to reduce the ensemble into a single model mimicking behavior of the ensemble. The second idea is to use a novelty detector classifier and weight the network output by the novelty score.
We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms. Proposes a new algorithm where they claim to use Hessian implicitly and are using a motivation from power-series . Presents a new 2nd-order algorithm that implicitly uses curvature information and shows the intuition behind the approximation schemes in the algorithms and validates the heuristics in various experiments.
We introduce fractional bitwidth approximation and show it has significant advantages. Suggests a method for varying the degree of quantization in a neural network during the forward propagation phase . Maintaining the accuracy of 2bits netword while using less than 2bits weights .
Mean Replacement is an efficient method to improve the loss after pruning and Taylor approximation based scoring functions works better with absolute values. Proposes a simple improvement to methods for unit pruning using "mean replacement" This paper presents a mean-replacement pruning strategy and utilizes the absolute-valued Taylor expansion as the scoring function for the pruning .
Avoid posterior collapse by lower bounding the rate. Presents an approach to preventing posterior collapse in VAEs by limiting the family of the variational approximation to the posterior . This paper introduces a constraint on the family of variational posteriors such that the KL term can be controlled to combat posterior collapse in deep generative models such as VAEs .
We developed a batch adaptive momentum that can achieve lower loss compared with mini-batch methods after scanning same epochs of data, and it is more robust against large step size. This paper addresses the problem of automatically tuning batch size during deep learning training, and claims to extend batch adaptive SGD to adaptive momentum and adopt the algorithms to complex neural networks problems. The paper proposes generalizing an algorithm which performs SGD with adaptive batch sizes by adding momentum to the utility function .
We use meta-gradients to attack the training procedure of deep neural networks for graphs. Studies the problem of learning a better poisoned graph parameters that can maximize the loss of a graph neural network. An algorithm to alter graph structure by adding/deleting edges so as to degrade the global performance of node classification, and the idea to use meta-learning to solve the bilevel optimization problem.
We show that modular structured models are the best in terms of systematic generalization and that their end-to-end versions don't generalize as well. This paper evaluates systemic generalization between modular neural networks and otherwise generic models via introduction of a new, spatial reasoning dataset . A targeted empirical evaluation of generalization in models for visual reasoning, focused on the problem of recognizing (object, relation, object) triples in synthetic scenes featuring letters and numbers.
Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents. A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations .
The normalized solution of gradient descent on logistic regression (or a similarly decaying loss) slowly converges to the L2 max margin solution on separable data. The paper offers a formal proof that gradient descent on the logistic loss converges very slowly to the hard SVM solution in the case where the data are linearly separable. This paper focuses on characterising the behaviour of log loss minimisation on linearly separable data, and shows that log-loss, minimised with gradient descent, leads to convergence to the max-margin solution.
Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. A domain generalization approach to reveal semantic information based on a linear projection scheme from CNN and NGLCM output layers. The paper proposes an unsupervised approach to identify image features that are not meaningful for image classification tasks .
We propose a method to learn physical vehicle camouflage to adversarially attack object detectors in the wild. We find our camouflage effective and transferable. The authors investigate the problem of learning a camouflage pattern which, when applied to a simulated vehicle, will prevent an object detector from detecting it. This paper targets adversarial learning for interfering car detection by learning camouflage patterns .
We combine differentiable decision trees with supervised variational autoencoders to enhance interpretability of classification. This paper proposes a hybrid model of a variational autoencoder composed with a differentiable decision tree, and an accompanying training scheme, with experiments demonstrating tree classification performance, neg. log likelihood performance, and latent space interpretability. The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE and a differentiable decision tree .
A practical and provably guaranteed approach for training efficiently classifiers in the presence of label shifts between Source and Target data sets . The authors propose a new algorithm for improving the stability of class importance weighting estimation procedure with a two-step procedure. The authors consider the problem of learning under label shifts, where label proportions differ while conditionals are equal, and propose an improved estimator with regularization.
Approach to improve classification accuracy on classes in the tail. The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution. Proposal for a Bayesian framework with a Gaussian mixture model to address an issue in classification applications, that the number of training data from different classes is unbalanced.
We present a method to synthesize states of interest for reinforcement learning agents in order to analyze their behavior. This paper proposes a generative model of visual observations in RL that is capable of generating observations of interests. An approach for visualizing states of interest that involves a variational autoencoder that learns to reconstruct state space and an optimization step that finds conditioning parameters to generate synthetic images.
A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise. A new loss function for training a deep neural network which can abstain, with performance looked at from angles in existence of structured noise, in existence of unstructured noise, and open world detection. This manuscript introduces deep abstaining classifiers which modifies the multiclass cross-entropy loss with an abstention loss, which is then applied to perturbed image classification tasks .
A regularization technique for TD learning that avoids temporal over-generalization, especially in Deep Networks . A variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The paper introduces HR-TD, a variation of the TD(0) algorithm, meant to improve the over-generalization problem in conventional TD .
We present a new CNN kernel for unstructured grids for spherical signals, and show significant accuracy and parameter efficiency gain on tasks such as 3D classfication and omnidirectional image segmentation. An efficient method enabling deep learning on spherical data that reaches competitive/state-of-the-art numbers with much less parameters than popular approaches. The paper proposes a novel convolutional kernel for CNN on the unstructured grids and formulates the convolution by a linear combination of differential operators.
In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent "bottleneck state" predictions, which are useful for planning. A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction. Reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead is trained to generate frames that happen at any point in the future.
We used an LSTM to detect when a smartphone walks into a building. Then we predict the device's floor level using data from sensors aboard the smartphone. The paper introduces a system to estimate a floor-level via their mobile device's sensor data using an LSTM and changes in barometric pressure . Proposal for a two-step method to determine which floor a mobile phone is on inside a tall building.
Combine language goal representation with hindsight experience replays. This paper considers the assumption implicit in hindsight experience replay, that there is access to a mapping from states to goals, and proposes a natural language goal representation. This submission uses Hindsight Experience Replay framework with natural language goals to improve the sample-efficiency of instruction-following models.
We propose a joint codebook and factorization scheme to improve second order pooling. This paper presents a way to combine existing factorized second order representations with a codebook style hard assignment. Proposal for a novel bilinear representation based on a codebook model, and an efficient formulation in which codebook-based projections are factorized via shared projection to further reduce parameter size.
We propose and apply a meta-learning methodology based on Weak Supervision, for combining Semi-Supervised and Ensemble Learning on the task of Biomedical Relationship Extraction. A semi-supervised method for relation classification, which trains multiple base learners using a small labeled dataset and applies some of them to annotate unlabeled examples for semi-supervised learning. This paper addresses the problem of generating training data for biological relation extraction, and uses predictions from data labeled by weak classifiers as additional training data for a meta learning algorithm. This paper proposes a combination of semi-supervised learning and ensemble learning for information extraction, with experiments conducted on a biomedical relation extraction task .
A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability. The authors claim that the previous art directly integrate neural networks into the graphical models as components, which renders the models uninterpretable. Proposal for a combination of neural nets and graphical models by using a deep neural net to predict the parameters of a graphical model.
We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL. Proposes to extend the determinist policy gradient algorithm to learn from demonstrations, while combined with a type of density estimation of the expert. This paper considers the problem of model-free imitation learning and proposes an extension of the generative adversarial imitation learning algorithm by replacing the stochastic policy of the learner with a deterministic one. The paper combines IRL, adversarial training, and ideas from deterministic policy gradients with the goal of decreasng sample complexity .
Low computational complexity graph CNN (without approximation) with better classification accuracy . Proposes a new CNN approach to graph classification using a filter based on outgoing walks of increasing length to incorporate information from more distant vertices in one propagation step. Proposal for a new neural network architecture for semi-supervised graph classification, building upon graph polynomial filters and utilizing them on successive neural network layers with ReLU activation functions. The paper introduces Topology Adaptive GCN to generalize convolutional networks to graph-structured data .
We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization. Studies the forgetting behavior of training examples during SGD, and shows there exist "support examples" in neural network training across different network architectures. This paper analyzes the extent to which networks learn to correctly classify specific examples and then forget these examples over the course of training. The paper studies whether some examples in training neural networks are harder to learn than others. Such examples are forgotten and relearned multiple times through learning.
An unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos. Designs a feature representation from video sequences captured from a scene from different view points. Proposal for an unsupervised representation learning method for visual inputs that incorporates a metric learning approach pulling nearest neighbor pairs of image patches close in embedding space while pushing apart other pairs. This paper explores self-supervised learning of object representations, with the main idea to encourage objects with similar features to get further ‘attracted’ to each other.
We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space. This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.
We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning. Proposes a new DQN where the targets are computed on a full episode by a backward update (end to start) for faster propagation of rewards by the episode end. The authors propose to modify the DQN algorithm by applying the max Bellman operator recursively on a trajectory with some decay to prevent accumulating errors with the nested max. In deep-Q networks, update Q values starting from the end of the episode in order to facilitate quick propagation of rewards back along the episode.
In this work we introduce a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events. This paper introduces siamese neural networks to the competing risks framework by optimizing for the c-index directly . The authors address issues of estimating risk in a survival analysis setting with competing risks and propose directly optimizing the time-dependent discrimination index using a siamese survival network .
We present a type-based pointer network model together with a value-based loss method to effectively train a neural model to translate natural language to SQL. The paper claims to develop a novel method to map natural language queries to SQL by using a grammar to guide decoding and using a new loss function for pointer / copy mechanism .
An unbiased and low-variance gradient estimator for discrete latent variable models . Proposes a new variance-reduction technique to use when computing an expected loss gradient where the expectation is with respect to independent binary random variables. An algorithm combining Rao-Blackwellization and common random numbers for lowering the variance of the score-function gradient estimator in the special case of stochastic binary networks . An unbiased and low variance augment-REINFORCE-merge (ARM) estimator for calculating and backpropagating gradients in binary neural networks .
We prove that parallel local SGD achieves linear speedup with much lesser communication than parallel mini-batch SGD. Provides a convergence proof for local SGD, and proves that local SGD can provide the same speedup gains as minibatch, but may be able to communicate significantly less. This paper presents an analysis of local SGD and bounds on how frequent the estimators obtained by running SGD required to be averaged in order to yield linear parallelization speedups. The authors analyze the local SGD algorithm, where $K$ parallel chains of SGD are run, and the iterates are occasionally synchronized across machines by averaging .
Compact perception of dynamical process . Studies the problem of compactly representing the model of a complex dynamic system while preserving information by using an information bottleneck method. This paper studied the Gaussian linear dynamic and proposed an algorithm for computing the Information Bottleneck Hierarchy (IBH).
Dense RNN that has fully connections from each hidden state to multiple preceding hidden states of all layers directly. Proposes a new RNN architecture that models long-term dependencies better, can learn multiscale representation of sequential data, and sidestep the gradients problem by using parametrized gating units. This paper proposes a fully connected dense RNN architecture with gated connections to every layer and preceding layer connections, and it's results on PTB charcter-level modelling task.
SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person). This paper investigates the problem of controlled image generation and proposes an algorithm that produces a pair of images with the same identity. This paper proposes, SD-GAN, a method of training GANs to disentangle the identity and non-identity information in the latent vector input Z.
We propose a learning framework for cross-domain translations which is exactly cycle-consistent and can be learned via adversarial training, maximum likelihood estimation, or a hybrid. Proposes AlignFlow, an efficient way of implementing cycle consistency principle using invertible flows. Flow models for unpaired image to image translation .
In a program synthesis context where the input is a set of examples, we reduce the cost by computing a subset of representative examples . Proposes a method for identifying representative examples for program synthesis to increase the scalability of existing constraint programming solutions. A method for choosing a subset of examples on which to run a constraint solver in order to solve program synthesis problems. This paper proposes a method for speeding up the general-purpose program synthesizers.
We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks. Introduced recurrent relational network (RRNs) that can be added to any neural networks to add relational reasoning capacity. Introduction of a deep neural network for structured prediction that achieves state-of-the-art performance on Soduku puzzles and the BaBi task. This paper describes a method called relational network to add relational reasoning capacity to deep neural networks.
Three class priors are all you need to train deep models from only U data, while any two should not be enough. Proposes an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors and discusses theoretical properties of the estimators. A methodology for training any binary classifier from only unlabeled data, and an empirical risk minimization method for two sets of unlabeled data where class priors are given.
Aggregating class evidence from many small image patches suffices to solve ImageNet, yields more interpretable models and can explain aspects of the decision-making of popular DNNs. This paper suggests a novel and compact neural network architecture which uses the information within bag-of-words features. The proposed algorithm only uses the patch information independently and performs majority voting using independently classified patches.
Current somatic mutation methods do not work with liquid biopsies (ie low coverage sequencing), we apply a CNN architecture to a unique representation of a read and its ailgnment, we show significant improvement over previous methods in the low frequency setting. Proposes a CNN based solution called Kittyhawk for somatic mutation calling at ultra low allele frequencies. A new algorithm to detect cancer mutations from sequencing cell free DNA that will identify the sequence context that characterize sequencing errors from true mutations. This paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA .
The paper introduces a new gold-standard corpus corpus of biomedical scientific literature manually annotated with UMLS concept mentions. Details the construction of a manually annotated dataset covering biomedical concepts that is larger and covered by a larger ontology than previous datasets. This paper uses MedMentions, a TaggerOne semi-Markov model for end-to-end concept recognition and linking on a set of Pubmed abstracts to label papers with biomedical concepts/entities .
We propose a deep clustering method where instead of a centroid each cluster is represented by an autoencoder . Presents deep clustering based on a mixture of autoencoders, where data points are allocated to a cluster based the representation error if the autoencoder network were used to represent it. A deep clustering approach that uses an autoencoder framework to learn a low-dimensional embedding of the data simultaneously while clustering data using a deep neural network. A deep clustering method which represents each cluster with different auto-encoders, works in an end-to-end manner, and also can be used to cluster new incoming data without redoing the whole clustering procedure.
We define a new Integral Probability Metric (Sobolev IPM) and show how it can be used for training GANs for text generation and semi-supervised learning. Suggests a novel regularization scheme for GANs based on a Sobolev norm, measuring deviations between L2 norms of derivatives. The authors provide another type of GAN using the typical setup of a GAN but with a different function class, and produce a recipe for training GANs with that sort of function class. The paper proposes a different gradient penalty for GAN critics that forces the expected squared norm of the gradient to be equal to 1 .
We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem. Address the problem of mode collapse in GANs using a constrained mixture distribution for the generator and an auxiliary classifier which predicts the source mixture component. The paper proposes a mixture of generators to train GANs without extra computational cost . The authors present that using MGAN, which aims to overcome model collapsing problem by mixture generators, achieves state-of-art results .
We present the BabyAI platform for studying data efficiency of language learning with a human in the loop . Presents a research platform with a bot in the loop for learning to execute language instructions in which language has compositional structures . Introduces a platform for grounded language learning that replaces any human in the loop with a heuristic teacher and uses a synthetic language mapped to a 2D grid world .
A k-means prior combined with L1 regularization yields state-of-the-art compression results. This paper explores soft parameter tying and compression of DNNs/CNNs .
The SVRG method fails on modern deep learning problems . This paper presents an analysis of SVRG style methods, showing that dropout, batch norm, data augmentation (random crop/rotation/translations) tend to increase bias and/or variance of the updates. This paper investigates the applicability of SVGD to modern neural networks and shows the naive application of SVGD typically fails.
A method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere. Presents a polar anisotropic convolution scheme on a unit sphere by replacing filter translation with filter rotation. This paper explores deep learning of 3D shapes using alt-az anisotropic 2-sphere convolution .
Training binary/ternary networks using local reparameterization with the CLT approximation . Trains binary and ternary weight distribution networks using backpropagation to sample neuron pre-activations with reparameterization trick . This paper suggests using stochastic parameters in combination with the local reparametrisation trick to train neural networks with binary or ternary weights, which leads to state of the art results.
Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks. Alternative approach to training seq2seq models using a dynamic program to compute optimal continuations of predicted prefixes . A training algorithm for auto-regressive models that does not require any MLE pre-training and can directly optimize from the sampling. The paper considers a shortcoming of sequence to sequence models trained using maximum likelihood estimation and propose an approach based on edit distances and the implicit use of given label sequences during training .
a joint model and gradient sparsification method for federated learning . Applies variational dropout to reduce the communication cost of distributed training of neural networks, and does experiments on mnist, cifar10 and svhn datasets. The authors propose an algorithm that reduces communication costs in federated learning by sending sparse gradients from device to server and back. Combines distributed optimization algorithm with variational dropout to sparsify the gradients sent to master server from local learners.
We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures. Presents a boosting-style algorithm for training deep residual networks, a convergence analysis for training error, and a analysis of generalization ability. A learning method for ResNet using the boosting framework that decomposes the learning of complex networks and uses less computational costs. Authors propose the deep ResNet as a boosting algorithm, and they claim this is more efficient than standard end-to-end backropagation.
The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. This paper studies the problem of learning one-hidden layer neural networks, establishes a connection between least squares population loss and Hermite polynomials, and proposes a new loss function. A tensor factorization-type method for leaning one hidden-layer neural netowrk .
An Open Information Extraction Corpus and its in-depth analysis . Builds a new corpus for information extraction which is larger than the prior public corpora and contains information not existing in current corpora. Presents a dataset of open-IE triples that were collected from Wikipedia with the help of a recent extraction system. The paper describes the creation of an Open IE corpus over English Wikipedia through an automatic manner .
We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures. Introduces a new method to generate RNNs architectures using a domain-specific language for two types of generators (random and RL-based) together with a ranking function and evaluator. This paper casts the search of good RNN Cell architectures as a black-box optimization problem where examples are represented as an operator tree and scored based on learnt functions or generated by a RL agent. This paper investigates meta-learning strategy for automated architecture search in the context of RNN by using a DSL that specifies RNN recurrent operations.
We apply training and inference with only low-bitwidth integers in DNNs . A method called WAGE which quantizes all operands and operators in a neural network to reduce the number of bits for representation in a network. The authors propose discretized weights, activations, gradients, and errors at both training and testing time on neural networks .
In this paper, we develop fast retraining-free  sparsification methods that can be deployed for on-the-fly sparsification of CNNs in many industrial contexts. This paper proposes approaches for pruning CNNs without retraining by introducing three schemes to determine the thresholds of pruning weights. This paper describes a method for sparsification of CNNs without retraining.
We propose that training with growing sets stage-by-stage provides an optimization for neural networks. The authors compare curriculum learning to learning in a random order with stages that add a new sample of examples to the previously, randomly constructed set . This paper studies the influence of ordering in the Curriculum and Self paced learning, and shows that to some extent the ordering of training instances is not important.
An image to image translation method which adds to one image the content of another thereby creating a new image. This paper tackles the task of content transfer, with the novalty being on the loss.
A dataset for testing mathematical reasoning (and algebraic generalization), and results on current sequence-to-sequence models. Presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models, and uses it to evaluate several models. Model for solving basic math problems.
This paper introduces efficient and economic parametrizations of convolutional neural networks motivated by partial differential equations . Introduces four "low cost" alternatives to the standard convolution operation that can be used in place of the standard convolution operation to reduce their computational complexity. This paper introduces methods for reducing the computational cost of CNN implementations, and introduces new parameterizations of CNN like architectures that limit parameter coupling. The paper proposes a PDE-based perspective to understand and parameterize CNNs .
Use rate-distortion theory to bound how much a latent variable model can be improved . Addresses problems of optimization of the prior in the latent variable model and the selection of the likelihood function by proposing criteria based on a lower-bound on the negative log-likelihood. Presents a theorem which gives a lower bound on negative log likelihood of rate-distortion for latent-variable modeling . The authors argue that the rate-distortion theory for lossy compression provides a natural toolkit for studying latent variable models proposes a lower bound.
We ignore non-linearities and do not compute gradients in the backward pass to save computation and to ensure gradients always flow. The author proposed linear backprop algorithms to ensure gradients flow for all parts during backpropagation.
Understand the VQ-VAE discrete autoencoder systematically using EM and use it to design non-autogressive translation model matching a strong autoregressive baseline. This paper introduces a new way of interpreting the VQ-VAE and proposes a new training algorithm based on the soft EM clustering. The paper presents an alternative view on the training procedure for the VQ-VAE using the soft EM algorithm .
This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically. Presents a PAC-Bayesian bound for a margin loss .
We seek to understand learned representations in compressed networks via an experimental regime we call deep net triage . Compares various initialization and training methods of transferring knowledge from VGG network to a smaller student network by replacing blocks of layers with single layers. This paper presents five methods for doing triaging or block layer compression for deep networks. The paper proposes a method to compress a block of layers in a NN that evaluates several different sub-approaches .
Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization. The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates . The authors analyze training of residual networks using large cyclic learning rates, and demonstrate fast convergence with cyclic learning rates and evidence of large learning rates acting as regularization.
We propose a method for the construction of arbitrarily deep infinite-width networks, based on which we derive a novel weight initialisation scheme for finite-width networks and demonstrate its competitive performance. Proposes a weight initialization approach to enable infinitely deep and infinite-width networks with experimental results on small datasets. Proposes deep neural networks of infinite width.
We derived biologically plausible synaptic plasticity learning rules for a recurrent neural network to store stimulus representations. A neural network model consisting of recurrently connected neurons and one or more redouts which aims to retain some output over time. This paper presents a self-organizing memory mechanism in a neural model, and introduces an objective function that minimizes changes in the signal to be memorized.
To understand GAN training, we define simple GAN dynamics, and show quantitative differences between optimal and first order updates in this model. The authors study the impact of GANs in settings where at each iteration, the discriminator trains to convergence and the generator updates with gradient steps, or where a few gradient steps are done for the disciminator and generator. This paper studies the dynamics of adversarial training of GANs on a Gaussian mixture model .
We propose a gradient-based method to transfer knowledge from multiple sources across different domains and tasks. This paper proposes to combine the gradients of source domains to help the learning in the target domain.
The first variational Bayes formulation of phylogenetic inference, a challenging inference problem over structures with intertwined discrete and continuous components . Explores an approximate inference solution to the problem of Bayesian inference of phylogenetic trees by leveraging recently proposed subsplit Bayesian networks and modern gradient estimators for VI. Proposes a variational approach to Bayesian posterior inference in phylogenetic trees.
It is a hybrid neural architecture to speed-up autoregressive model. Concludes that in order to scale up the model size without increasing inference time for sequential prediction, use a model that predicts multiple timesteps at once. This paper presents HybridNet, a neural speech and other audio synthesis system that combines the WaveNet model with an LSTM with the goal of offering a model with faster inference-time audio generation.
Interpretation by Identifying model-learned features that serve as indicators for the task of interest. Explain model decisions by highlighting the response of these features in test data. Evaluate explanations objectively with a controlled dataset. This paper proposes a method for producing visual explanations for deep neural network outputs and releases a new synthetic dataset. A method for Deep Neural Networks that identifies automatically relevant features of the set of the classes, supporting interpretation and explanation without relying on additional annotations.
A framework for learning high-quality sentence representations efficiently. Proposes a faster algorithm for learning SkipThought-style sentence representations from corpora of ordered sentences that swaps the word-level decoder for a contrastive classification loss. This paper proposes a framework for unsupervised learning of sentence representations by maximizing a model of the probability of true context sentences relative to random candidate sentences .
We derive a norm penalty on the output of the neural network from the information bottleneck perspective . Puts forward Activation Norm Penalty, an L_2 type regularization on the activations, deriving it from the Information Bottleneck principle . This paper creates a mapping between activation norm penalties and information bottleneck framework using variational dropout framework.
A fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework. Proposes an algorithm that integrates autoencoder with time-series data clustering using a network structure that suits time-series data. An algorithm for jointly performing dimensionality reduction and temporal clustering in a deep learning context, utilizing an autoencoder and clustering objective. The authors proposed an unsupervised time series clustering methods built with deep neural networks and equipped with an encoder-decoder and a clustering mode to shorten the time series, extract local temporal features, and to get the encoded representations.
A memory-augmented neural network that addresses many-class few-shot problem by leveraging class hierarchy in both supervised learning and meta-learning. This paper presents methods for adding inductive bias to a classifier through coarse-to-fine prediction along a class hierarchy and learning a memory-based KNN classifier that keeps track of mislabeled instances during learning. This paper formulates the many-class-few-shot classification problem from a supervised learning perspective and a meta-learning perspective.
A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task. This paper proposes two regularization terms based on a compound hinge loss over the KL divergence between two softmax-normalized input arguments to encourage learning disentangled representations . Proposal for two regularizers intended to make the representations learned in the penultimate layer of a classifier more conforming to inherent structure in the data.
We show how to get good representations from the point of view of Simiarity Search. Studies the impact of changing the image classification part on top of the DNN on the ability to index the descriptors with a LSH or a kd-tree algorithm. Proposes to use softmax cross-entropy loss to learn a network that tries to reduce the angles between inputs and the corresponding class vectors in a supervised framework using.
We introduce a technique that allows for gradient based training of quantized neural networks. Proposes a unified and general way of training neural networks with reduced precision quantized synaptic weights and activations. A new approach to quantizing activations which is state of the art or competitive on several real image problems. A method for learning neural networks with quantized weights and activations by stochastically quantizing values and replacing the resulting categotical distribution with a continuous relaxation .
We show that the mode collapse problem in GANs may be explained by a lack of information sharing between observations in a training batch, and propose a distribution-based framework for globally sharing information between gradients that leads to more stable and effective adversarial training. Proposes to replace single-sample discriminators in adversarial training with discriminators that explicitly operate on distributions of examples. Theory on two-sample tests and MMD and how can be beneficially incorporated into GAN framework.
We designed an end-to-end framework using sequence to sequence model to do the  chemical names standardization. Standardizes non systematic names in chemical information extraction by creating a parallel corpus of non-systematic and systematic names and building a seq2seq model. This work presents a method to translate non-systematic names of chemical compounds into their systematic equivalents using a combination of mechanisms .
SGD is steered early on in training towards a region in which its step is too large compared to curvature, which impacts the rest of training. Analyzes the relationship between the convergence/generalization and the update on largest eigenvectors of Hessian of the empirical losses of DNNs. This work studies the relationship between the SGD step size and the curvature of the loss surface .
We introduce a novel reinforcement learning algorithm, that predicts multiple actions and samples from them. This work introduces a uniform mixture of deterministic policies, and find that this parametrization of stochastic policies outperforms DDPG on several OpenAI gym benchmarks. The authors investigate a method for improving the performance of networks trained with DDPG, and show improved performance on a large number of standard continuous control environment.
Realizing the drawbacks when applying original dropout on DenseNet, we craft the design of dropout method from three aspects, the idea of which could also be applied on other CNN models. Application of different binary dropout structures and schedules with the specific aim to regularise the DenseNet architecture. Proposes a pre-dropout technique for densenet which implements the dropout before the non-linear activation function.
Guiding relation-aware deep models towards better learning with human knowledge. This work proposes a variant of the column network based on the injection of human guidance by modifying calculations in the network. A method to incorporate human advices to deep learning by extending Column Network, a graph neural network for collective classification.
Recent successes of Binary Neural Networks can be understood based on the geometry of high-dimensional binary vectors . Investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. This paper analyzes the effectiveness of binary neural networks and why binarization is able to preserve model performance.
After proving that a neuron acts as an inverse problem solver for superresolution and a network of neurons is guarantied to provide a solution, we proposed a double network architecture that performs faster than state-of-the-art. Discusses using neural networks for super-resolution . A new architecture for solving image super-resolution tasks, and an analysis aiming to establish a connection between CNNs for solving super resolution and solving sparse regularized inverse problems.
Dynamic model that learns divide and conquer strategies by weak supervision. Proposes to add new inductive bias to neural network architecture by using a divide and conquer strategy. This paper studies problems that can be solved using a dynamic programming approach, and proposes a neural network architecture to solve such problems that beats sequence to sequence baselines. The paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks.
a Rep-like gradient for non-reparameterizable continuous/discrete distributions; further generalized to deep probabilistic models, yielding statistical back-propagation . Presents a gradient estimator for expectation-based objectives that is unbiased, has low variance, and applies to either continuous and discrete random variables. An improved method for computing derivates of the expectation, and a new gradient estimator of low variance that allows training of generative models in which observations or latent variables are discrete. Designs a low variance gradient for distributions associated with continuous or discrete random variables.
We show that NN parameter and hyperparameter cost landscapes can be generated as quantum states using a single quantum circuit and that these can be used for training and meta-training. Describes a method where a deep learning framework can be quantised by considering the two state form of a Bloch sphere/qubit and creating a quantum binary neural network. This paper proposes quantum amplitude amplification, a new algorithm for training and model selection in binary neural networks. Proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN).
A general method for training certified cost-sensitive robust classifier against adversarial perturbations . Calculates and plugs in the costs of adversarial attack into the objective of optimization to get a model that is cost-sensitively robust against adversarial attacks. Build on semnial work by Dalvi et al. and extends approach to certifiable robustness with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples.
Using triplets to learn a metric for comparing neural responses and improve the performance of a prosthesis. Authors develop new spike train distance metrics, including neural networks and quadratic metrics. These metrics are shown to outperform the naive Hamming distance metric, and implicitly captures some structure in neural code. With the application of improving neural prosthesis in mind, the authors propose to learn a metric between neural responses by either optimizing a quadratic form or a deep neural network .
This paper introduces a method to generate questions (cues) and queries (suggestions) to help users perform mind-mapping. Presents a tool to assist mind-mapping through suggested context related to existing nodes and through questions that expand on less developed branches. This paper presents an approach for assisting people with mindmapping tasks, designing an interface and algorithmic features to suppport mindmapping, and contributes a evaluative study.
Detecting out-of-distribution samples by using low-order feature statistics without requiring any change in underlying DNN. Presents an algorithm to detect out-of-distribution samples by using the running estimate of mean and variance within BatchNorm layers to construct feature representations later fed into a linear classifier. An approach for detecting out-of-distribution samples in which the authors propose to use logistic regression over simple statistics of each batch normalization layer of CNN. The paper suggests using Z-scores for comparing ID and OOD samples to evaluate what deep nets are trying to do.
We propose a novel method named Maximal Divergence Sequential Auto-Encoder that leverages Variational AutoEncoder representation for binary code vulnerability detection. This paper proposes a variational autoencoder-based architecture for code embeddings for binary software vulnerability detection, with learned embeddings more effective at distinguishing between vulnerable and non-vulnerable binary code than baselines. This paper proposes a model to automatically extract features for vulnerability detection using deep learning technique.
Computing attention based on posterior distribution leads to more meaningful attention and better performance . This paper proposes a sequence to sequence model where attention is treated as a latent variable, and derives novel inference procedures for this model, obtaining improvements in machine translation and morphological inflection generation tasks. This paper presents a novel posterior attention model for seq2seq problems .
Compressing trained DNN models by minimizing their complexity while constraining their loss. This paper proposes a method for deep neural network compression under accuracy constraints. This paper presents a loss value constrained k-means encoding method for network compression and develops an iterative algorithm for model optimization.
We develop a technique to visualize attention mechanisms in arbitrary neural networks. Proposes to learn a Latent Attention Network that can help to visualize the inner structure of a deep neural network. The authors of this paper propose a data-driven black-box visualization scheme.
We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best. Considers model evaluation for molecule generation by proposing 19 benchmarks, expanding small data sets to a large, standardized dataset, and exploring how to apply RL techniques for molecular design. This paper shows that the most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO as the exception, when modeling and synthesizing molecules.
The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems . This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.
A Goal-oriented Neural Conversation Model by Self-Play . A self-play model for goal oriented dialog generation, aiming to enforce a stronger coupling between the task reward and the language model. This paper describes a method for improving a goal oriented dialogue system using selfplay.
realtime search query completion using character-level LSTM language models . This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU. The authors propose an algorithm for solving the query completion problem with error correction, and adopt character-level RNN-based modeling and optimize the inference part to achieve targets in real time.
In this paper we prove convergence to criticality of (stochastic and deterministic) RMSProp and deterministic ADAM for smooth non-convex objectives and we demonstrate an interesting beta_1 sensitivity for ADAM on autoencoders. This paper presents a convergence analysis of RMSProp and ADAM in the case of smooth non-convex functions .
Devising unsupervised defense mechanisms against adversarial attacks is crucial to ensure the generalizability of the defense. This paper presents a method for detecting adversarial examples in a deep learning classification setting . This paper presents an unsupervised method for detecting adversarial examples of neural networks.
Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training. This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on "proxy" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size.
A new framework based variational inference for out-of-distribution detection . Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.
We learn a representation of an agent's action space from pure visual observations. We use a recurrent latent variable approach with a novel composability loss. Proposes a compositional latent-variable model to learn models that predict what will happen next in scenarios where action-labels are not available in abundance. A variational IB based approach to learn action representations directly from videos of actions being taken, achieving better efficiency of subsequent learning methods while requiring lesser amount of action label videos. This paper proposes an approach to video prediction which autonomously finds an action space encoding differences between subsequent frames .
Reinforcement learning can be used to train agents to negotiate team formation across many negotiation protocols . This paper studies deep multi-agent RL in settings where all of the agents must cooperate to accomplish a task (e.g., search and rescue, multi-player video games), and uses simple cooperative weighted voting games to study the efficacy of deep RL and to compare solutions found by deep RL to a fair solution. A reinforcement learning approach for negotiating coalitions in cooperative game theory settings that can be used in cases where unlimited training simulations are available.
Unsupervised methods for finding, analyzing, and controlling important neurons in NMT . This paper presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.
We propose a DRL framework that disentangles task and environment specific knowledge. The authors propose to decompose reinforcement learning into a PATH function and a GOAL function . A modular architecture with the aim of separating environment specific knowledge and task-specific knowledge into different modules, on par with standard A3C across a wide range of tasks.
pix2scene: a deep generative based approach for implicitly modelling the geometrical properties of a 3D scene from images . Explores explaining scenes with surfels in a neural recognition model, and demonstrate results on image reconstruction, synthesis, and mental shape rotation. Authors introduce a method to create a 3D scene model given a 2D image and a camera pose using a self-superfised model .
Identifying the relations that connect words is important for various NLP tasks. We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations. This paper presents a novel method for representing lexical relations as vectors using just pre-trained word embeddings and a novel loss function operating over pairs of word pairs. A novel solution to the relation compositon problem when you already have pre trained word/entity embeddings and are interested only in learning to compose relation representations.
We propose to train two identical copies of an recurrent neural network (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. Presents Fraternal dropout as an improvement over Expectation-linear dropout in terms of convergence, and demonstrates the utility of Fraternal dropout on a number of tasks and datasets.
Learning weighting and deformations of space-time data sets for highly efficient approximations of liquid behavior. A neural-network based model is used to interpolate simulations for novel scene conditions from densely registered 4D implicit surfaces for a structured scene. This paper presents a coupled deep learning approach for generating realistic liquid simulation data that can be useful for real-time decision support applications. This paper introduces a deep learning approach for physical simulation that combines two networks for synthesizing 4D data that represents 3D physical simulations .
We construct and evaluate color invariant neural nets on a novel realistic data set . Proposes a method to make neural networks for image recognition color invariant and evaluates it on the cifar 10 dataset. The authors investigate a modified input layer that results in color invariant networks, and show that certain color invariant input layers can improve accuracy for test-images from a different color distribution than the training images. The authors test a CNN on images with color channels modified to be invariant to permutations, with performance not degraded by too much.
We analyze how the degree of overlaps between the receptive fields of a convolutional network affects its expressive power. The paper studies the expressive power provided by "overlap" in convolution layers of DNNs by considering linear activations with product pooling. This paper analyzes the expressivity of convolutional arithmetic circuits and shows that an exponentialy large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs.
A theoretical algorithm for testing local optimality and extracting descent directions at nondifferentiable points of empirical risks of one-hidden-layer ReLU networks. Proposes an algorithm to check whether a given point is a generalized second-order stationary point. A theoretical algorithm, involving solving convex and non-convex quadratic programs, for checking local optimality and escaping saddles when training two-layer ReLU networks. Author proposes a method to check if a point is a stationary point or not and then classify stationary points as either local min or second-order stationary .
A new loss based on relatively hard negatives that achieves state-of-the-art performance in image-caption retrieval. Learning joint embedding of sentences and images using triplet loss that is applied to hardest negatives instead of averaging over all triplets .
We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders. Alternating minimization framework for training autoencoder and encoder-decoder networks . The authors explore an alternating optimization approach for training Auto Encoders, treating each layer as a generalized linear model, and suggest using the stochastic normalized GD as the minimization algorithm in each phase.
Transfer learning for estimating causal effects using neural networks. Develops algorithms to estimate conditional average treatment effect by auxiliary dataset in different environments, both with and without base learner. The authors propose methods to address a novel task of transfer learning for estimating the CATE function, and evaluate them using a synthetic setting and a real-world experimental dataset. Using neural network regression and comparing transfer learning frameworks to estimate a conditional average treatment effect under string ignorability assumptions .
We present LeMoNADe, an end-to-end learned motif detection method directly operating on calcium imaging videos. This paper proposes a VAE-style model for identifying motifs from calcium imaging videos, relying on Bernouli variables and requires Gumbel-softmax trick for inference.
We propose a framework to learn a good policy through imitation learning from a noisy demonstration set via meta-training a demonstration suitability assessor. Contributes a MAML based algorithm to imitation learning which automatically determines if provided demonstrations are "suitable". A method for doing imitation learning from a set of demonstrations that includes useless behavior, which selects the useful demonstrations by their provided performance gains at the meta-training time.
We introduce causal implicit generative models, which can sample from conditional and interventional distributions and also propose two new conditional GANs which we use for training them. A method of combining a casual graph, describing the dependency structure of labels with two conditional GAN architechtures that generate images conditioning on the binary label . The authors address the issue of learning a causal model between image variables and the image itself from observational data, when given a causal structure between image labels.
We prove that NCE is self-normalized and demonstrate it on datasets . Presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix. This paper considers the problem of self-normalizing models and explains the self-normalizing mechanism by interpreting NCE in terms of matrix factorization.
Enriching word embeddings with affect information improves their performance on sentiment prediction tasks. Proposes to use affect lexica to improve word embeddings to outperform the standard Word2vec and Glove. This paper proposes integrating information from a semantic resource quantifying the affect of words into a text-based word embedding algorithm to make language models more reflective of semantic and pragmatic phenomena. This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings.
For unsupervised and inductive network embedding, we propose a novel approach to explore most relevant neighbors and preserve previously learnt knowledge of nodes by utilizing bi-attention architecture and introducing global bias, respectively . This proposes an extension to GraphSAGE using a global embedding bias matrix in the local aggregating functions and a method to sample interesting nodes.
We argue that the generalization of linear graph embedding is not due to the dimensionality constraint but rather the small norm of embedding vectors. The authors show that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors rather than dimensionality constraints . The authors propose a theoretical bound on the generalization performance of learning graph embeddings and argue that the norm of the coordinates determines the success of the learnt representation.
Mix plain SGD and momentum (or do something similar with Adam) for great profit. The paper proposes simple modifications to SGD and Adam, called QH-variants, that can recover the “parent” method and a host of other optimization tricks. A variant of classical momentum which takes a weighted average of momentum and gradient update, and an evaluation of its relationships between other momentum based optimization schemes.
A novel way to generalize lambda-returns by allowing the RL agent to decide how much it wants to weigh each of the n-step returns. Extends the A3C algorithm with lambda returns, and proposes an approach for learning the weights of the returns. The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state.
we proposed a new self-driving model which is composed of perception module for see and think and driving module for behave to acquire better generalization  and accident explanation ability. Presents a multitask learning architecture for depth and segmentation map estimation and the driving prediction using a perception module and a driving decision module. A method for a modified end-to-end architecture that has better generalization and explanation ability, is more robust to a different testing setting, and has decoder output that can help with debugging the model. The authors present a multi-task convolutional neural network for end-to-end driving and provide evaluations with the CARLA open source simulator showing better generalization performance in new driving conditions than baselines .
A generic framework to scale existing graph embedding techniques to large graphs. This paper proposes a multi-level embedding framework to be applied on top of existing network embedding methods in order to scale to large scale networks with faster speed. The authors propose a three-stage framework for large-scale graph embedding with improved embedding quality.
A novel method to increase the resistance of OCSVMs against targeted, integrity attacks by selective nonlinear transformations of data to lower dimensions. The authors propose a defense against attacks on the security of one-class SVM based anomaly detectors . This paper explores how random projections can be used to make OCSVM robust to adversarially perturbed training data.
Learning a better neural networks' representation with Information Bottleneck principle . Proposes a learning method based on the information bottleneck framework, where hidden layers of deep nets compress the input X while maintaining sufficient information to predict the output Y. This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck.
We propose an estimator for the maximum mean discrepancy, appropriate when a target distribution is only accessible via a biased sample selection procedure, and show that it can be used in a generative network to correct for this bias. Proposes an importance-weighted estimator of the MMD to estimate the MMD between distributions based on samples biased according to a known or estimated unknown scheme. The authors address the problem of sample selection bias in MMD-GANs and propose an estimate of the MMD between two distributions using weighted maximum mean discrepancy. This paper presents a modification of the objective used to train generative networks with an MMD adversary .
Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation . The authors propose a new algorithm for exploration in Deep RL where they apply Bayesian linear regression with features from the last layer of a DQN network to estimate the Q function for each action. The authors describe how to use Bayesian neural networks with Thompson sampling for efficient exploration in q-learning and propose an approach that outperforms epsilon-greedy exploration approaches.
PolyCNN only needs to learn one seed convolutional filter at each layer. This is an efficient variant of traditional CNN, with on-par performance. Attempts at reducing the number of CNN model parameters by using the polynomial transformation of filters to create blow-up the filter responses. The authors propose a weight sharing architecture for reducing the number of convolutional neural network parameters with seed filters .
In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model as a surrogate to the abnormal distribution. Describes a novel approach to optimising the choice of kernel towards increased testing power and shown to offer improvements over alternatives.
Cluster before you classify; using weak labels to improve classification . Proposes using a clustering based loss function at multiple levels of a deepnet as well as using hierarchical structure of the label space to train better representations. This paper uses hierarchical label information to impose additional losses on intermediate representations in neural network training.
Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft. This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL and an algorithm called ARM which can deal with partial observability better. The paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization and claims that the approach can deal with the partial observable domain better than standard methods.
a deep multi-task learning model adapting tensor ring representation . A variant of tensor ring formulation for multi-task learning by sharing some of the TT cores for learning "common task" while learning individual TT cores for each separate task .
A model for regression that learns conditional distributions of a stochastic process, by incorporating attention into Neural Processes. Proposes to resolve the issue of underfitting in the neural process method by adding an attention mechanism to the deterministic path. An extension to the framework of Neural Processes that adds an attention-based conditioning mechanism, allowing the model to better capture dependencies in the conditioning set. The authors extend neural processes by incorporating self-attention for enriching the features of the context points and cross-attention for producing a query-specific representation. They resolve the underfitting problem of NPs and show ANPs to converge better and faster than NPs.
Solve checkerboard problem in Deconvolutional layer by building dependencies between pixels . This work proposes pixel deconvolutional layers for convolutional neural networks as a way to alleviate the checkerboard effect. A novel technique to generalize deconvolution operations used in standard CNN architectures, which proposes doing sequential prediction of adjacent pixel features, resulting in more spatially smooth outputs for deconvolution layers.
We quantize and prune neural network weights using variational Bayesian inference with a multi-modal, sparsity inducing prior. Proposes to use a mixture of continuous spike propto 1/abs as prior for a Bayesian neural network and demonstrates the good performance with relatively sparsified convnets for minist and cifar-10. This paper presents a variational Bayesian approach for quantising neural network weights to ternary values post-training in a principled way.
We implement a DNN weight pruning approach that achieves the highest pruning rates. This paper focuses on weight pruning for neural network compression, achiving 30x compression rate for AlexNet and VGG for ImageNet. A progressive pruning technique which imposes structural sparsity constraint on the weight parameter and rewrites the optimization as an ADMM framework, achieving higher accurancy than projected gradient descent.
This paper presents a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. Proposes a framework for making predictions on sparse, irregularly sampled time-series data using an interpolation module that models the missing values in using smooth interpolation, non-smooth interpolation, and intensity. Solves the problem of supervised learning with sparse and irregularly sampled multivariate time series using a semi-parametric interpolation network followed by a prediction network.
Permutation-invariant loss function for point set prediction. Proposes a new loss for points registration (aligning two point sets) with preferable permutation invariant property. This paper introduces a novel distance function between point sets, applies two other permutation distances in an end-to-end object detection task, and shows that in two dimensions all local minima of the holographic loss are global minima. Proposes permutation invariant loss functions which depend on the distance of sets.
We introduce a hierarchical model for efficient, end-to-end placement of computational graphs onto hardware devices. Proposes to jointly learn groups of operators to colocate and to place learned groups on devices to distribute operations for deep learning via reinforcement learning. The authors purpose a fully connect network to replace the co-location step in an auto-placement method proposed to accelerate a TensorFlow model's runtime. Proposes a device placement algorithm to place operations of tensorflow on devices.
We propose of method of using group properties to learn a representation of motion without labels and demonstrate the use of this method for representing 2D and 3D motion. Proposes to learn the rigid motion group from a latent representation of image sequences without the need for explicit labels and experimentally demonstrates method on sequences of MINST digits and the KITTI dataset. This paper proposes an approach for learning video motion features in an unsupervised manner, using constraints to optimize the neural network to produce features that can be used to regress odometry.
This paper proposes a novel convolutional layer that operates in a continuous Reproducing Kernel Hilbert Space. Projecting examples into an RK Hilbert space and performing convolution and filtering into that space. This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases .
ImageNet-trained CNNs are biased towards object texture (instead of shape like humans). Overcoming this major difference between human and machine vision yields improved detection performance and previously unseen robustness to image distortions. Using image stylizaton to augment training data for ImageNet-trained CNNs to make resulting networks appear more aligned with human judgements . This paper studies CNNs like AlexNet, VGG, GoogleNet, and ResNet50, shows these models are biased towards texture when trained on ImageNet, and proposes a new ImageNet dataset.
We evaluate the effectiveness of having auxiliary discriminative tasks performed on top of statistics of the posterior distribution learned by variational autoencoders to enforce speaker dependency. Propose an autoencoder model to learn a representation for speaker verification using short-duration analysis windows. A modified version of the variational autoencoder model that tackles the speaker recognition problem in the context of short-duration segments .
Variational inference is biased, let's debias it. Introduces jackknife variational inference, a method for debiasing Monte Carlo objectives such as the importance weighted auto-encoder. The authors analyze the bias and variance of the IWAE bound and derive a jacknife approach to estimate moments as a way to debias IWAE for finite importance weighted samples.
A framework that provides a policy for autonomous lane changing by learning to make high-level tactical decisions with deep reinforcement learning, and maintaining a tight integration with a low-level controller to take low-level actions. Considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting, proposes a new learning strategy Q-masking - coupling a defined low level controller with a high level tactical decision making policy. This paper proposes a deep Q-learning approach to the problem of lane change using "Q-masking," which reduces the action space according to contraints or prior knowledge. Authors propose a method which uses a Q-learning-based high-level policy that is combined with a contextual mask derived from safety-contraints and low-level controllers, which disable certain actions from being selectable at certain states.
Automatic robotic design search with graph neural networks . Proposes an approach for automatic robot design based on Neural graph evolution. The experiments demonstrate that optimizing both controller and hardware is better than optimizing just the controller. The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers to optimize robot structures, combined with their controllers.
We demonstate an autoencoder for graphs. Learning to generate graphs using deep learning methods in "one shot", directly outputting node and edge existence probabilities, and node attribute vectors. A variational auto encoder to generate graphs .
We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties. Propose a new "gate" function for LSTM to enable the values of the gates towards 0 or 1. The paper aims to push LSTM gates to be binary by employing the recent Gumbel-Softmax trick to obtain end-to-end trainable categorical distribution.
Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website . The paper proposes a new neural network based method for recommendation. The authors describe a procedure of building their production recommender system from scratch and integrate time decay of purchases into the learning framework.
Couple the GAN based image restoration framework with another task-specific network to generate realistic image while preserving task-specific features. A novel method of Task-GAN of image coupling that couples GAN and a task-specific network, which alleviates to avoid hallucination or mode collapse. The authors propose to augment GAN-based image restoration with another task-specific branch, such as classification tasks, for further improvement.
An end-to-end trained deep neural network that leverages Gaussian Mixture Modeling to perform density estimation and unsupervised anomaly detection in a low-dimensional space learned by deep autoencoder. The paper presents a joint deep learning framework for dimension reduction-clustering, leads to competitive anomaly detection. A new technique for anomaly detection where the dimension reduction and density estimation steps are jointly optimized.
We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning . This paper proposes a new embedding-based approach for the problem of few-shot learning and an extension to this model to the semi-supervised few-shot learning setting. New method for fully and semi-supervised few-shot classification based on learning a general embedding and then learning a subspace of it for each class .
We investigate contingency-awareness and controllable aspects in exploration and achieve state-of-the-art performance on Montezuma's Revenge without expert demonstrations. This paper investigates the problem of extracting a meaningful state representation to help with exploration when confronted with a sparse reward task by identifying controllable (learned) features of the state . This paper proposes the novel idea of using contingency awareness to aid exploration in sparse-reward reinforcement learning tasks, obtaining state of the art results.
We proposed a supervised algorithm, DNA-GAN, to disentangle multiple attributes of images. This paper investigates the problem of attribute-conditioned image generation using generative adversarial networks, and proposes to generate images from attribute and latent code as high-level representation. This paper proposed a new method to disentangle different attributes of images using a novel DNA structure GAN .
This paper presents a novel latent-variable generative modelling technique that enables the representation of global information into one latent variable and local information into another latent variable. The paper presents a VAE that uses labels to separate the learned representation into an invariant and a covariant part.
We approach to the problem of active learning as a core-set selection problem and show that this approach is especially useful in the batch active learning setting which is crucial when training CNNs. The authors provide an algorithm-agnostic active learning algorithm for multi-class classification . The paper proposes a batch mode active learning algorithm for CNN as a core-set problem which outperforms random sampling and uncertainty sampling. Studies active learning for convolutional neural networks and formulates the active learning problem as core-set selection and presents a novel strategy .
A simple algorithm to improve optimization and handling of long term dependencies in LSTM . The paper introduces a simple stochastic algorithm called h-detach that is specific to LSTM optimization and targeted towards addressing this problem. Proposes a simple modification to the training process of the LSTM to facilitate gradient propogation along cell states, or the "linear temporal path"
Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples. This paper proposes adding an additional label for detecting OOD samples and adversarial examples in CNN models. The paper proposes an additional class that incorporates natural out-distribution images and interpolated images for adversarial and out-distribution samples in CNNs .
In a deep convolutional neural network trained with sufficient level of data augmentation, optimized by SGD, explicit regularizers (weight decay and dropout) might not provide any additional generalization improvement. This paper proposes data augmentation as an alternative to commonly used regularisation techniques, and shows that for a few reference models/tasks that the same generalization performance can be achived using only data augmentation. This paper presents a systematic study of data augmentation in image classification with deep neural networks, suggesting that data augmentation can replicit some common regularizers like weight decay and dropout.
In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing. Reports the design and evaluation of the Gedit interaction techniques. Presents a new set of touch gestures to perform seamless transition between text entry and text editing in mobile devices .
We prove that DNN is a recursively approximated solution to the maximum entropy principle. Presents a derivation which links a DNN to recursive application of maximum entropy model fitting. The paper aims to provide a view of deep learning from the perspective of maximum entropy principle.
We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. Proposes a reinforcement learning framework based on human emotional reaction in the context of autonomous driving. The authors propose to use signals, such as basic autonomic visceral responses that influence decision-making, within the RL framework by augmenting RL reward functions with a model learned directly from human nervous system responses. Proposes to use physiological signals to improve performance of reinforcement learning algorithms and build an intrinsic reward function that is less sparse by measuring heart pulse amplitude .
Are CNNs robust or fragile to label noise? Practically, robust. The authors challenge the CNNs robustness to label noise using ImageNet 1k tree of WordNet. An analysis of convolutional neural network model performance when class dependent and class independent noise is introduced . Demonstrates that CNNs are more robust to class-relevant label noise and argues that real-world noise should be class-relevant .
High-quality audio synthesis with GANs . Proposes an approach that uses GAN framework to generate audio through modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. A strategy to generate audio samples from noise with GANs, with changes to the architecture and representation necessary to generate convincing audio that contains an interpretable latent code. Presents a simple idea for better representing audio data so that convolutional models such as generative adversarial networks can be applied .
Graph Optimization with signal filtering in the vertex domain. The paper investigates learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights uses a projected sub-gradient descent algorithm. Develops a novel scheme for backpropogating on the adjacency matrix of a neural network graph .
This paper describe a 3D authoring tool for providing AR in assembly lines of industry 4.0 . The paper addresses how AR authoring tools support training of assembly line systems and proposes an approach . An AR guidance system for industrial assembly lines that allows for on-site authoring of AR content. Presents a system that allows factory workers to be trained more efficiently using augmented reality system.
We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. This paper uses GANs and multi-task learning to provide a convergence guarantee for primal-dual algorithms on certain min-max problems. Analyses the learning dynamics of GANs by formulating the problem as a primal-dual optimisation problem by assuming a limited class of models .
We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games. Learning to play two-player general-sum games with state with imperfect information . Specifies a trigger strategy (CCC) and corresponding algorithm, demonstrating convergence to efficient outcomes in social dilemmas without need for agents to observe each other's actions.
The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy. The authors propose applying dithered quantization to the stochastic gradients computed through the training process, which improves quantization error and achieves superior results compared to baselines, and propose a nested scheme to reduce communication cost. Authors establish a connection between communication reduction in distributed optimization and dithered quantization and develops two new distributed training algorithms where communication overhead is significantly reduced.
Jointly train an adversarial noise generating network with a classification network to provide better robustness to adversarial attacks. A GAN solution for deep models of classification, faced to white and black box attacks, that produces robust models. The paper proposes a defensive mechanism against adversarial attacks using GANs with generated perturbations used as adversarial examples and a discriminator used to distinguish between them .
Using ensemble methods as a defense to adversarial perturbations against deep neural networks. This paper proposes to use ensembling as an adversarial defense mechanism. Empirally investigated the robustness of different deep neural entworks ensembles to the two types of attacks, FGSM and BIM, on two popular datasets, MNIST and CIFAR10 .
Proposal of the sentence generation method based on fusion between textual information and visual information associated with the textual information . This work describes a deep learning model for dialogue systems that takes advantage of visual information. This paper proposes a novel dataset for grounded dialog and makes a computational observation that it could help to reason about vision even when performing text-based dialog. Proposes to augment traditional text-based sentence generation/dialogue approaches by incorporating visual information by collecting a bunch of data consisting of both text and associated images or video .
we proposed a novel contextual recurrent convolutional network with robust property of visual learning . This paper introduces feedback connection to enhance feature learning through incorporating context information. The paper proposes to add "recurrent" connections inside a convolution network with gating mechanism.
We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty in conventional networks. This paper proposes using batch normalisation at test time to get the predictive uncertainty, and shows Monte Carlo prediction at test time using batch norm is better than dropout. Proposes that the regularization procedure called batch normalization can be understood as performing approximate Bayesian inference, which performs similarly to MC dropout in terms of the estimates of uncertainty that it produces.
We improve gradient dropping (a technique of only exchanging large gradients on distributed training) by incorporating local gradients while doing a parameter update to reduce quality loss and further improve the training time. This paper proposes a 3 modes for combining local and global gradients to better use more computing nodes . Looks at the problem of reducing the communication requirement for implementing the distributed optimiztion techniques, particularly SGD .
Exploration using Distributional RL and truncagted variance. Presents an RL method to manage exploration-explotation trade-offs via UCB techniques. A method to use the distribution learned by Quantile Regression DQN for exploration, in place of the usual epsilon-greedy strategy. Proposes new algorithsms (QUCB and QUCB+) to handle the exploration tradeoff in Multi-Armed Bendits and more generally in Reinforcement Learning .
Motivated by theories of language and communication, we introduce community-based autoencoders, in which multiple encoders and decoders collectively learn structured and reusable representations. The authors tackle the problem of representation learning, aim to build reusable and structured represenation, argue co-adaptation between encoder and decoder in traditional AE yields poor representation, and introduce community based auto-encoders. The paper presents a community based autoencoder framework to address co-adaptation of encoders and decoders and aims at constructing better representations.
We present MetaMimic, an algorithm that takes as input a demonstration dataset and outputs (i) a one-shot high-fidelity imitation policy (ii) an unconditional task policy. The paper looks at the problem of one-shot imitation with high accuracy of imitation, extending DDPGfD to use only state trajectories. This paper proposes an approach for one-shot imitation with high accuracy, and addresses the common problem of exploration in imitation learning. Presents an RL method for learning from video demonstration without access to expert actions .
We present a novel normalization method for deep neural networks that is robust to multi-modalities in intermediate feature distributions. Normalization method that learns multi-modal distribution in the feature space . Proposes a generalization of Batch Normalization under the assumption that the statistics of the unit activations over the batches and over the spatial dimensions is not unimodal .
We proposed a knowledge distillation based method to boost the accuracy of multilingual neural machine translation. A many-to-one multilingual neural machine translation model that first training separate models for each language pair then performs distillation. The paper aims at training a machine translation model by augmenting the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models.
We investigate the various kinds of prior knowledge that help human learning and find that general priors about objects play the most critical role in guiding human gameplay. The authors study by experiment, what aspects of human priors are the important for reinforcement learning in video games. The authors present a study of priors employed by humans in playing video games and demonstrates the existence of a taxonomy of features that affect the ability to complete tasks in the game to varying degrees.
Driven by the need for parallelizable, open-loop hyperparameter optimization methods, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search. Proposes using the k-DPP to select candidate points in hyperparameter searches. The authors propose k-DPP as an open loop method for hyperparameter optimization and provide its empirical study and comparison with other methods. Considers non-sequential and uninformed hyperparameter search using determinantal point processes, which are probability distributions over subsets of a ground set with the property that subsets with more 'diverse' elements haev higher probability .
In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch. Addresses the problem of transfer learning in deep networks and proposes to have a regularization term that penalizes divergence from initialization. Proposes an analysis on different adaptive regularization techniques for deep transfer learning, specifically focusing on the use of an L@-SP condition .
We look at neural networks with block diagonal inner product layers for efficiency. This paper proposes making the inner layers in a neural network be block diagonal, and discusses that block diagonal matrices are more efficient than pruning and block diagonal layers lead to more efficient networks. Replacing fully connected layers with block-diagonal fully connected layers .
We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs. This paper uses spectral regularization to normalize GAN objectives, and the ensuing GAN, called SN-GAN, essentially ensures the Lipschitz property of the discriminator. This paper proposes"spectral normalization", moving a nice step forward in improving the training of GANs.
Transition policies enable agents to compose complex skills by smoothly connecting previously acquired primitive skills. Proposes a scheme for transitioning to favorable strating states for executing given options in continuous domains. This uses two learning processes carried out simultaneously. Presents a method for learning policies for transitioning from one task to another with the goal of completing complex tasks using state proximity estimator to reward for transition policy. Proposes a new training scheme with a learned auxiliary reward function to optimise transition policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option .
We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. The authors analyse GRUs with hidden sizes of one and two as continuous-time dynamical systems, claiming that the expressive power of the hidden state representation can provide prior knowledge on how well a GRU will perform on a given dataset . This paper analyzes GRUs from a dynamical systems perspective, and shows that 2d GRUs can be trained to adopt a variety of fixed points and can approximate line attractors, but cannot mimic a ring attractor. Converts GRU equations into continuous time and uses theory and experiemnts to study 1- and 2-dimensional GRU networks and showcase every variety of dynamical topology available in these systems .
Differentiated inputs cause functional differentiation of the network, and the interaction of loss functions between networks can affect the optimization process. A modification to the original hourglass network for single pose estimation that yields improvements over the original baseline. Authors extend a stacked hourglass network with inception-resnet-A modules and propose a multi-scale approach for human pose estimation in still RGB images.
To train a sentence embedding using technical documents, our approach considers document structure to find broader context and handle out-of-vocabulary words. Presents ideas for improving sentence embedding by drawing from more context. Learning sentence representations with sentences dependencies information . Extends the idea of forming an unsupervised representation of sentences used in the SkipThough approach by using a broader set of evidence for forming the representation of a sentence .
We explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. This paper proposes a method to visualize the loss function of a NN and provides insights on the trainability and generalization of NNs. Investigates the non-convexity of the loss surface and optimization paths.
We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks. This paper proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class. Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines .
We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x. This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words . This paper describes an approach to decode non-autoregressively for neural machine translation with the possibility of more parallel decoding which can result in a significant speed-up. Proposes the introduction of a set of latent variables to represent the fertility of each source word to make the target sentence generation non-autoregressive .
We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples. Proposes a new defense against security attacks on neural networks with the atack model that outputs a security certificate on the algorithm. Derives an upper bound on adversarial perturbation for neural networks with one hidden layer .
we propose a regularizer that improves the classification performance of neural networks . the authors propose to train a model from a point of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. Proposes to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer to maximize mutual information I(Y, \hat{T}) while constraining irrelevant information .
This paper introduces a novel generative modelling framework that avoids latent-variable collapse and clarifies the use of certain ad-hoc factors in training Variational Autoencoders. The paper proposes to resolve the issue about a variational auto-encoder ignoring the latent variables. This paper proposes adding a stochastic autoencoder to the original VAE model to address the problem that the LSTM decoder of a language model might be too strong to ignore the latent variable's information. This paper presents AutoGen, which combines a generative variational autoencoder with a high-fidelity reconstruction model based on autoencoder to better utiliza latent representation .
This paper studies the problem of domain division by segmenting instances drawn from different probabilistic distributions. This paper deals with the problem of novelty recognition in open set learning and generalized zero-shot learning and proposes a possible solution . An approach to domain separation based on bootstrapping to identify similarity cutoff thresholds for known classes, followed by a Kolmogorov-Smirnoff test to refine the bootstrapped in-distribution zones. Proposes to introduce a new domain, the uncertain domain, to better handle the division between seen/unseen domains in open-set and generalized zero-shot learning .
SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss . This paper provides a variational analysis of SGD as a non-equilibrium process. This paper discusses the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation. Develops a theory to study the impact of stocastic gradient noise for SGD, especially for deep neural network models .
Agents can learn to imitate solely visual demonstrations (without actions) at test time after learning from their own experience without any form of supervision at training time. This paper proposes and approach for zero-shot visual learning by learning parametric skill functions. A paper about imitation of a task presented just during inference, where learning is performed in a self-supervised manner and during training the agent explores related but different tasks. Proposes a method for sidestepping the issue of expensive expert demonstration by using the random exploration of an agent to learn generalizable skills which can be applied without specific pretraining .
Paper provides a description of a procedure to enhance word vector space model with an evaluation of Paragram and GloVe models for Similarity Benchmarks. This paper suggests a new algorithm that adjusts GloVe word vectors and then uses a non-Euclidean similarity function between them. The authors present observations on the weaknesses of the existing vector space models and list a 6-step approach for refining existing word vectors .
We propose a  new  quantization method and apply it to quantize RNNs for both compression and acceleration . This paper proposes a multi-bit quantization method for recurrent neural networks. A technique for quantizing neural network weight matrices, and an alternating optimization procedure to estimate the set of k binary vectors and coefficients that best represent the original vector.
We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. In the paper the authors suggest to use MERA tensorization technique for compressing neural networks. A new parameterization of linear maps for neural network use, using a hierarchical factorization of the linear map that reduces the number of parameters while still allowing for relatively complex interactions to be modelled. Studies compressing feed forward layers using low rank tensor decompositions and explore a tree like decomposition .
We present a single shot analysis of a trained neural network to remove redundancy and identify optimal network structure . This paper proposes a set of heuristics for identifying a good neural network architecture, based on PCA of unit activations over the dataset . This paper presents a framework for optimising neural networks architectures through the identification of redundant filters across layers .
We conduct the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels, which represents a step toward understanding the DNN’s vulnerability to side-channel attacks. This paper considers the problem of fingerprinting neural network architectures using cache side channels, and discusses security-through-obscurity defenses. This paper performs cache side-channel attacks to extract attributes of a victim model and infer its architecture, as well as show they can achieve a nearly perfect classification accuracy.
We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks. Considers augmenting the cross-entropy objective with "complement" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth labels. The authors propose a secondary objective for softmax minimization based on evaluating the information gathered from the incorrect classes, leading to a new training approach. Deals with the training of neural networks for classification or sequence generation tasks using across-entropy loss .
Uncertainty estimation in a single forward pass without additional learnable parameters. A new method for computing output uncertainty estimates in DNNs for classification problems that matches state-of-the-art methods for uncertainty estimation and outperforms them in out-of-distribution detection tasks. The authors present inhibited softmax, a modification of the softmax through adding a constant activation which provides a measure for uncertainty.
We made a feature-rich system for deep learning with encrypted inputs, producing encrypted outputs, preserving privacy. A framework for private deep learning model inference using FHE schemes that support fast bootstrapping and thus can reduce computation time. The paper presents a means of evaluating a neural network securely using homomorphic encryption.
We introduce a system called GamePad to explore the application of machine learning methods to theorem proving in the Coq proof assistant. This paper describes a system for applying machine learning to interactive theorem proving, focuses on tasks of tactic prediction and position evaluation, and shows that a neural model outperforms an SVM on both tasks. Proposes that machine learning techniques be used to help build proof in the theorem prover Coq.
In this paper, we studied efficient training of loss-aware weight-quantized  networks with  quantized gradient  in a distributed environment, both theoretically and empirically. This paper studies convergence properties of loss-aware weight quantization with different gradient precisions in the distributed environment, and provides convergence analysis for weight quantization with full-precision, quantized and quantized clipped gradients. The authors proposes an analysis of the effect of simultaneously quantizing the weights and gradients in training a parametrized model in a fully-synchronized distributed environment.
A regularization strategy for improving the performance of sequential learning . A novel, regularization based approach to the sequential learning problem using a fixed size model that adds extra terms to the loss, encouraging representation sparsity and combating catastrophic forgetting. This paper deals with the problem of catastrophic forgetting in lifelong learning by proposing regularized learning strategies .
A synaptic neural network with synapse graph and learning that has the feature of topological conjugation and Bose-Einstein distribution in surprisal space. The authors propose a hybrid neural nework composed of a synapse graph that can be embedded into a standard neural network . Presents a biologically-inspired neural network model based on the excitatory and inhibitory ion channels in the membranes of real cells .
Generalized Graph Embedding Models . A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches. Tackles the task of learning embeddings of multi-relational graphs using a neural network . Proposes a new method, GEN, to compute embeddings of multirelationship graphs, particularly that so-called E-Cells and R-Cells can answer queries of the form (h,r,?),(?r,t), and (h,?,t)
Minimax Curriculum Learning is a machine teaching method involving increasing desirable hardness and scheduled reducing diversity. A curriculum learning approach using a submodular set function that captures the diversity of examples chosen during training. The paper introduces MiniMax Curriculum learning as an approach for adaptively training models by providing it different subsets of data.
Implicit models applied to causality and genetics . The authors propose to use the implicit model to tackle Genome-Wide Association problem. This paper proposes solutions for the problems in genome-wide association studies of confounding due to population structure and the potential presence of non-linear interactions between different parts of the genome, and bridges statistical genetics and ML. Presents a non-linear generative model for GWAS that models population structure where non-linearities are modeled using neural networks as non-linear function approximators and inference is performed using likelihood-free variational inference .
Few-shot learning by exploiting the object-level relation to learn the image-level relation (similarity) This paper deals with the problem of few-shot learning by proposing an embedding-based approach that learns to compare object-level features between support and query set examples . Proposes a few shot learning method that exploits the object-level relation between different images based on neared neighbor search and concatenates feature maps of two input images into one feature map .
Researchers exploring natural language processing techniques applied to source code are not using any form of pre-trained embeddings, we show that they should be. This paper sets to understand whether pretraining word embeddings for programming language code by using NLP-like language models has an impact on extreme code summarization task. This work shows how pre-training word vectors using corpuses of code leads to representations that are more suitable than randomly initialized and trained representations for function/method name prediction .
We solve the Rubik's Cube with pure reinforcement learning . Solution to solving Rubik cube using reinforcement learning (RL) with Monte-Carlo tree search (MCTS) through autodidactic iteration. This work solves Rubik's Cube using an approximate policy iteration method called Autodidactic iteration, overcoming the problem of sparse rewards by creating its own rewards system. Introduces a deep RL algorithm to solve the Rubik's cube that handles the huge state space and very sparse reward of the Rubik's cube .
We describe an end-to-end differentiable model for QA that learns to represent spans of text in the question as denotations in knowledge graph, by learning both neural modules for composition and the syntactic structure of the sentence. This paper presents a model for visual question answering that can learn both parameters and structure predictors for a modular neural network, without supervised structures or assistance from a syntactic parser. Proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words .
We introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks. Proposal to move from ad-hoc code generation in deep learning engines to compiler and languages best practices. This paper presents a compiler framework that allows definition of domain-specific languages for deep learning systems, and defines compilation stages that can take advantage of standard optimizations and specialized optimizations for neural networks. This paper introduces a DLVM to take advantage of the compiler aspects of a tensor compiler .
Attention based architecture for language grounding via reinforcement learning in a new customizable 2D grid environment . The paper tackles the problem of navigation given an instruction and proposes an approach to combine textual and visual information via an attention mechanism . This paper considers the problem of following natural language instructions given a first-person view of an a priori unknown environment, and proposes a neural architecture method. Studies the problem of navigating to a target object in a 2D grid environment by following given natural language description and receiving visual information as raw pixels.
A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models. A fast high performance paraphrasing based data augmentation method and a non-recurrent reading comprehension model using only convolutions and attention. This paper proposes applying CNNs+self-attention modules instead of LSTMs and enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model in order to improve RC performance. This paper presents a reading comprehension model using convolutions and attention and propose to augment additional training data by paraphrasing based on off-the-shelf neural machine translation .
We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts . This paper focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal. The authors develop a novel scheme for representing spherical data from the ground up .
A method for performing automated design on real world objects such as heat sinks and wing airfoils that makes use of neural networks and gradient descent. Neural network (parameterization and prediction) and gradient descent (back propogation) to automatically design for engineering tasks. This paper introduces using a deep network to approximate the behavior of a complex physical system, and then design optimal devices by optimizing this network with respect to its inputs.
We propose a dual version of the logistic adversarial distance for feature alignment and show that it yields more stable gradient step iterations than the min-max objective. The paper deals with fixing GANs at the computational level . This paper studies a dual formulation of an adversarial loss based on an upper-bound of the logistic loss, and turns the standard min max problem of adversarial training into a single minimization problem. Proposes to re-formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression .
state-of-the-art computational performance implementation of binary neural networks . The paper presents a library written in C/CUDA that features all the functionalities required for the forward propagation of BCNNs . This paper builds on Binary-NET and expands it to CNN architectures, provides optimizations that improve the speed of the forward pass, and provides optimized code for Binary CNN.
We propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization. Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'Differentiable Greedy Network' (DGN). Proposes a neural network that aims to select a subset of elements (e.g. selecting k sentences that are mostly related to a claim from a set of retrieved docs)
We introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. The paper proposes using the nested CRP as a clustering model rather than a topic model . Presents a novel hierarchical clustering method over an embedding space where both the embedding space and the heirarchical clustering are simultaneously learnt .
We develop a statistical-geometric unsupervised learning augmentation framework for deep neural networks to make them robust to adversarial attacks. Transfroms traditional deep neural networks into adversarial robust calssifiers using GRNs . Proposes a defense based on class-conditional feature distributions to turn deep neural netwroks into robust classifiers .
Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies. A method to coordinate agent behaviour by using policies that have shared latent structure, a variational policy optimization method to optimize the coordinated policies, and a derivation of the authors' variational, hierarchical update. This paper suggests an algorithmic innovation consisting of hierarchical latent variables for coordinated exploration in multi-agent settings .
We provide many insights into neural network generalization from the theoretically tractable linear case. The authors study a simple model of linear networks towards understanding generalization and transfer learning .
Batch normalisation maintains gradient variance throughout training, thus stabilizing optimization. This paper analyzed the effect of batch normalization on gradient backpropagation in residual networks .
Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks . This paper describes a large-scale experiment on human object/sematic representations and a model of such representations. This paper develops a new representation system for object representations from training on data collected from odd-one-out human judgements of images. A new approach to learn a sparse, positive, interpretable semantic space that maximizes human similarity judgements by training to specifically maximize the prediction of human similarity judgements.
We propose an agent that sits between the user and a black box question-answering system and which learns to reformulate questions to elicit the best possible answers . This paper proposes active question answering via a reinforcement learning approach that learns to rephrase questions in a way to provide the best possible answers. Clearly describes how the researchers designed and actively trained two models for question reformulation and answer selection during question answering episodes .
Learning Priors for Adversarial Autoencoders . Proposes a simple extension of adversarial auto-encoders for conditional image generation. Focuses on adversarial autoencoders and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution .
Generating text using sentence embeddings from Skip-Thought Vectors with the help of Generative Adversarial Networks. Describes application of generative adversarial networks for modeling textual data with the help of ski-thought vectors and experiments with different flavors of GANs for two different datasets.
Introduces an online, unbiased and easily implementable gradient estimate for recurrent models. The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the imitation of truncated backpropagation through time . This paper approaches online training of RNNs in a principled way, and proposes a modification to RTRL and to use forward approach for gradient calculation.
Super-resolving coarse labels into pixel-level labels, applied to aerial imagery and medical scans. A method to super-resolve coarse low-res segmentation labels if the joint distribution of low-res and high-res labels are known.
We propose a method for aligning the latent features learned from different datasets using harmonic correlations. Proposes using feature correspondences to preform manifold alignment between batches of data from the same samples to avoid the collection of noisy measurements.
Evolving the shape of the body in RL controlled agents improves their performance (and help learning) PEOM algorithm that incorporates Shapley value to accelerate the evolution by identifying contribution of each body part .
Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting. An RL algorithm that combines the DQN algorithm with a fear model trained in parallel to predict catastropohic states. The paper studies catastrophic forgetting in RL, by emphasizing tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting.
A novel convolution operator for automatic representation learning inside unit ball . This work is related to the recent spherical CNN and SE(n) equivariant network papers and extends previous ideas to volumetric data in the unit ball. Proposes using volumetric convolutions on convolutions networks in order to learn unit ball and discusses methodology and results of process.
We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages. Develops a curriculum learning method for training an RL agent to navigate a web, based on the idea of decomposing an instruction in to multiple sub-instructions.
Cross Language Text Classification by universal encoding . This paper proposes an approach to cross-lingual text classification through the use of comparable corpora. Learning cross-lingual embeddings and training a classifier using labelled data in the source language to address learning a cross-language text categorizer with no labelled information in the target language .
In this work we propose deep inside-outside recursive auto-encoders(DIORA)  a  fully  unsupervised  method  of  discovering  syntax  while  simultaneously learning representations for discovered constituents. A neural latent tree model trained with an auto-encoding objective that achieves state of the art on unsupervised constituency parsing and captures syntactic structure better than other latent tree models. The paper proposes a model for unsupervised dependency parsing (latent tree induction) that is based on a combination of the inside-outside algorithm with neural modeling (recursive auto-encoders).
We investigate the bias in the short-horizon meta-optimization objective. This paper proposes a simplified model and problem to demonstrate the short-horizon bias of the learning rate meta-optimization. This paper studies the issue of truncated backpropagation for meta-optimization through a number of experiments on a toy problem .
a hierarchical and compositional way to generate captions . This paper presents a more interpretable method for image captioning.
We develop a new topological complexity measure for deep neural networks and demonstrate that it captures their salient properties. This paper proposes the notion of neural persistence, a topological measure to assign scores to fully-connected layers in a neural network. Paper proposes to analyze the complexity of a neural network using its zero-th persistent homology.
Looking at decision boundaries around an input gives you more information than a fixed small neighborhood . The authors present a novel attack for generating adversarial examples where they attack classifiers created by randomly classifying L2 small perturbations . A new approach to generate adversarial attacks to a neural network, and a method to defend a neural network from those attacks.
We train a neural network to output approximately optimal weights as a function of hyperparameters. Hyper-networks for hyper-parameter optimization in neural networks.
Covariance matrix estimation of financial assets with Gaussian Process Latent Variable Models . Illustrates how the Gaussian Process Latent Variable Model (GP-LVM) can replace classical linear factor models for the estimation of covariance matrices in portfolio optimization problems. This paper uses standard GPLVMs to model the covariance structure and a latent space representation of S&P500 financial time series, to optimize portfolios and predict missing values. This paper proposes to use a GPLVM to model financial returns .
We introduce meta-adversarial learning, a new technique to regularize GANs, and propose a training method by explicitly controlling the discriminator's output distribution. The paper proposes variance regularizing adversarial learning for training GANs to ensure that the gradient for the generator does not vanish .
A deep reinforcement learning agent with parametric noise added to its weights can be used to aid efficient exploration. This paper introduces NoisyNets, neural networks whose parameters are perturbed by a parametric noise function, that obtain substantial performance improvement over baseline deep reinforcement learning algorithms. New exploration method for deep RL by injecting noise into deep networks' weights, with the noise taking various forms .
"Active Neural Localizer", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning. This paper formulates the problem of localisation on a known map using a belief network as an RL problem where the agent's goal is to minimise the number of steps to localise itself. This is a clear and interesting paper that builds a parameterized network to select actions for a robot in a simulated environment .
We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference. Distills knowledge from intermediary hidden states and attention weights to improve non-autoregressive neural machine translation. Proposes to leverage well trained autoregressive model to inform the hidden states and the word alignment of non-autoregressive Neural Machine Translation models.
Using mophological operation (dilation and erosion) we have defined a class of network which can approximate any continious function. This paper proposes to replace standard RELU/tanh units with a combination of dilation and erosion operations, observing that the new operator creates more hyper-planes and has more expressive power. The authors introduce Morph-Net, a single layer neural network where the mapping is performed using morphological dilation and erosion.
This work advances DNN compression beyond the weights to the activations by integrating the activation pruning with the weight pruning. An integral model compression method that handles both weight and activation pruning, leading to more efficient network computation and effective reduction of the number of multiply-and-accumulate. This article presents a novel approach to reduce the computation cost of deep neural networks by integrating activation pruning along with weight pruning and show that common techniques of exclusive weight pruning  increases the number of non-zero activations after ReLU.
We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling . Introducting an importance sampling distribution and using samples from distribution to compute importance-weighted estimate of the gradient . This paper proposes to use important sampling to optimize VAE with discrete latent variables.
A new distributed asynchronous SGD algorithm that achieves state-of-the-art accuracy on existing architectures without any additional tuning or overhead. Proposes an improvement to existing ASGD approaches at mid-size scaling using momentum with SGD for asynchronous training across a distributed worker pool. This paper addresses the gradient staleness vs parallel performance problem in distributed deep learning training, and proposes an approach to estimate future model parameters at the slaves to reduce communication latency effects.
We propose a new learning algorithm of deep neural networks, which unlocks the layer-wise dependency of backpropagation. An alternative training paradigm for DNIs in which the auxiliary module is trained to approximate directly the final output of the original model, offering side benefits. Describes a method of training neural networks without update locking.
Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly. Proposes stochastic determination methods for truncation points in backpropagation through time. A new approximation to backpropagation through time to overcome the computational and memory loads that arise when having to learn from long sequences.
non-targeted and targeted attack on GCN by adding fake nodes . The authors propose a new adversarial technique to add "fake" nodes to fool a GCN-based classifier .
Transfer learning for sequence via learning to align cell-level information across domains. The paper proposed to use RNN/LSTM with collocation alignment as a representation learning method for transfer learning/domain adaptation in NLP.
We formulate model uncertainty in Reinforcement Learning as a continuous Bayes-Adaptive Markov Decision Process and present a method for practical and scalable Bayesian policy optimization. Using a Bayesian approach, there is a better trade-off between exploration and exploitation in RL .
We argue that GAN benchmarks must require a large sample from the model to penalize memorization and investigate whether neural network divergences have this property. Authors propose criterion for evaluating the quality of samples produced by a Generative Adversarial Network.
open domain dialogue generation with dialogue acts . The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues . The paper describes a technique to incorporate dialog acts into neural conversational agents .
Our hypothesis is that given two domains, the lowest complexity mapping that has a low discrepancy approximates the target mapping. The paper addresses the problem of learning mappings between different domains without any supervision, stating three conjectures. Demonstrates that in unsupervised learning on unaligned data it is possible to learn the between domains mapping using GAN only without a reconstruction loss.
We refine the over-approximation results from incomplete verifiers using MILP solvers to prove more robustness properties than state-of-the-art. Introduces a verifier that obtains improvement on precision of incomplete verifiers and scalability of the complete verifiers using over-parameterization, mixed integer linear programming and linear programming relaxation. A mixed strategy to obtain better precision on robustness verifications of feed-forward neural networks with piecewise linear activation functions, achieving better precision than incomplete verifiers and more scalability than complete verifiers.
Are HMMs a special case of RNNs? We investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization and provide new insights. This paper explores if HMMs are a special case of RNNs using language modeling and POS tagging .
We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network. This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network in order to disentangle the latent representation .
The proposed scheme mimics the classification process mediated by a series of one component picking. A method to increase accuracy of deep-nets on multi-class classification tasks seemingly by a reduction of multi-class to binary classification. A novel classification procedure of discerning, maxmum response, and multiple check to improve accuracy of mediocre networks and enhance feedforward networks.
Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve. This paper shows that wider RNNs improve convergence speed when applied to NLP problems, and by extension the effect of increasing the widths in deep neural networks on the convergence of optimization . This paper characterizes the impact of over-parametrization in the number of iterations it takes an algorithm to converge, and presents further empirical observations on the effects of over-parametrization in neural network training.
Multi-headed Pointer Networks for jointly learning to localize and repair Variable Misuse bugs . Proposes a LSTM based model with pointers to break the problem of VarMisuse down into multiple steps. This paper presents an LSTM-based model for bug detection and repair of the VarMisuse bug, and demonstrates significant improvements compared to prior approaches on several datasets.
Human-like Clustering with CNNs . The paper validates the idea that deep convolutional neural networks could learn to cluster input data better than other clustering methods by noting their ability to interpret the context of every input point due to a large field of view. This work combines deep learning for feature representation with the task of human-like unsupervised grouping.
We develop two linear-complexity algorithms for model-agnostic model interpretation based on the Shapley value, in the settings where the contribution of features to the target is well-approximated by a graph-structured factorization. The paper proposes two approximations to the Shapley value used for generating feature scores for interpretability. This paper proposes two methods for instance-wise feature importance scoring using Shapely values, and provides two efficient methods of computing approximate Shapely values when there is a known structure relating the features.
Local codes have been found in feed-forward neural networks . A method for determining to what degree individual neurons in a hidden layer of an MLP encode a localist code, which is studied for different input representations. Studies the development of localist representations in the hidden layers of feed-forward neural networks.
Extending relational modeling to support multimodal data using neural encoders. This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information, and presents a model able to encode all sorts of information when scoring triples. The paper is about incorporating information from different modalities into link prediction approaches .
Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with improved performance and significantly reduced cost than traditional methods. The authors propose a procedure to generate an ensemble of sparse structured models . A new framework for training ensemble neural networks that uses SG-MCMC methods within deep learning, and then increases computational efficiency by group sparsity+pruning. This paper explores the use of FNN and LSTMs to make bayesian model averaging more computationally feasible and improve average model performance.
Novel framework for meta-learning that unifies and extends a broad class of existing few-shot learning methods. Achieves strong performance on few-shot learning benchmarks without requiring iterative test-time inference. This work tackles few-shot learning from a probabilistic inference viewpoint, achieving state-of-the-art despite simpler setup than many competitors .
Defining a partially mutual exclusive softmax loss for postive data and implementing a cooperative based sampling scheme . This paper presents Cooperative Importance Sampling towards resolving the problem of the mutually exclusive assumption of traditional softmax being biased when negative samples are not explicitly defined . This paper proposes PMES methods to relax the exclusive outcome assumption in softmax loss, demonstrating empirical merit in improving word2vec type of embedding models.
Teacher-Student framework for efficient video classification using fewer frames . The paper proposes an idea to distill from a full video classification model a small model that only receives smaller number of frames. The authors present a teacher-student network to solve video classification problem, proposing serial and parallel training algorithms aimed at reducing computational costs.
A unified statistical view of the broad class of deep generative models . The paper develops a framework interpreting GAN algorithms as performing a form of variational inference on a generative model reconstructing an indicator variable of whether a sample is from the true of generative data distributions.
a method combining rule list learning and prototype learning . Presents a new interpretable prediction framework, which combines rule based learning, prototype learning, and NNs, that is particularly applicable to longitudinal data. This paper aims at tackling the lack of interpretability of deep learning models, and propose Prototype lEArning via Rule Lists (PEARL), which combines rule learning and prototype learning to achieve more accurate classification and makes the task of interpretability simpler.
This paper proposes a new Generative Adversarial Network that is more stable, more efficient, and produces better images than those of status-quo . This paper combines Fisher-GAN and Deli-GAN . This paper combines Deli-GAN, which has a mixture prior distribution in latent space, and Fisher GAN, which uses Fisher IPM instead of JSD as an objective.
We introduce a modular multi-sensor network architecture with an attentional mechanism that enables dynamic sensor selection on real-world noisy data from CHiME-3. A generic neural architecture able to learn the attention that must be payed to different input channels depending on the relative quality of each sensor with respect to the others. Considers the use of attention for sensor or channel selection with results on TIDIGITS and GRID showing a benefit of attention over concatentation of features.
To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate data representations, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud. This paper proposes a technique to privatize data by learning a feature representation that is difficult to use for image reconstruction, but helpful for image classification.
Conditional recurrent GANs for real-valued medical sequences generation, showing novel evaluation approaches and an empirical privacy analysis. Proposes to use synthetic data generated by GANs as a replacement for personally identifiable data in training ML models for privacy-sensitive applications . The authors propose a novel recurrent GAN architecture that generates continuous domain sequences, and evaluate it on several synthetic tasks and an ICU timeseries data task. Proposes to use RGANs and RCGANs to generate synthetic sequences of actual data.
Our studies and empirical models provide valuable new information for designers who want to understand and control how emphasis effects will be perceived by users . This paper considers which visual highlighting is perceived faster in data visualization and how different highlighting methods compare to each other . Two studies on the efficacy of emphasis effects, one assessing levels of useful differences, and one more applied using actual different visualizations for a more ecologically valid investigation.
A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog. Introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN).
We show that splitting a neural network into parallel branches improves performance and that proper coupling of the branches improves performance even further. The work proposes a reconfiguration of the existing state-of-the-art CNN model using a new branching architecture, with better performance. This paper shows parameter-saving benefits of coupled ensembling. Presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores.
Combine noise injection, gradual quantization and activation clamping learning to achieve state-of-the-art 3,4 and 5 bit quantization . Proposes to inject noise during training and clamp parameter values in a layer as well as activation output in neural network quantization. A method for quantization of deep neural networks for classification and regression, using noise injection, clamping with learned maximum activations, and gradual block quantization to perform on-par or better than state-of-the-art methods.
We propose Leap, a framework that transfers knowledge across learning processes by  minimizing the expected distance the training process travels on a task's loss surface. The article proposes a novel meta-learning objective aimed at outperforming state-of-the-art approaches when dealing with collections of tasks that exhibit substantial between-task diversity .
An alternative to transfer learning that learns faster, requires much less parameters (3-13 %), usually achieves better results and precisely preserves performance on old tasks. Controller modules for increment learning on image classification datasets .
We present a general technique toward 8-bit low precision inference of convolutional neural networks. This paper designs a system to automatically quantize the CNN pretrained models .
We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks. This paper replaces the dot-product similarity used in attention mechanisms with the negative hyperbolic distance, and applies it to the existing Transformer model, graph attention networks, and Relation Networks . The authors propose a novel approach to improve relational-attention by changing the matching and aggregation functions to use hyperbolic geometric.
This paper demonstrates how H-infinity control theory can help better design robust deep policies for robot motor taks . Proposes to incorporate elements of robust control into guided policy research in order to devise a method that is resilient to perturbations and model mismatch. The paper presents a method for evaluating the sensitivity and robustness of deep RL policies, and proposes a dynamic game approach for learning robust policies.
Analysis of vulnerability of classifiers to universal perturbations and relation to the curvature of the decision boundary. The paper provides an interesting analysis linking the geometry of classifier decision boundaries to small universal adversarial perturbations. This paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data points. The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks .
We propose a meta-learning method for interactively correcting policies with natural language. This paper provides a meta learning framework that shows how to learn new tasks in an interactive setup. Each task is learned through a reinforcement learning setup, and then the task is being updated by observing new instructions. This paper teaches agents to complete tasks via natural language instructions in an iterative process.
We investigate the modularity of deep generative models. The paper provides a way to investigate the modular structure of the deep generative model, with the key concept to distribute over channels of generator architectures.
We introduce Seq2SQL, which translates questions to SQL queries using rewards from online query execution, and WikiSQL, a SQL table/question/query dataset orders of magnitude larger than existing datasets. A new semantic parsing dataset which focuses on generating SQL from natural language using a reinforcement-learning based model .
Noise modeling at the input during discriminative training improves adversarial robustness. Propose PCA based evaluation metric for adversarial robustness . This paper proposes, ExL, an adversarial training method using multiplicate noise that is shown to be helpful in defending against blackbox attacks on three datasets. This paper includes multiplicative noise N in training data to achieve adversarial robustness, when training on both model parameters theta and on the noise itself.
A method to answer "why not class B?" for explaining deep networks . The paper proposes an approach to provide contrastive visual explanations for deep neural networks.
We analyze the invertibility of deep neural networks by studying preimages of ReLU-layers and the stability of the inverse. This paper studies the volume of preimage of a ReLU network’s activation at a certain layer, and it builds on the piecewise linearity of a ReLU network’s forward function. This paper presents an analysis of the inverse invariance of ReLU networks and provides upper bounds on singular values of a train network.
Adversarial training of ensembles provides robustness to adversarial examples beyond that observed in adversarially trained models and independently-trained ensembles thereof. Proposes to train an ensemble of models jointly, where at each time step, a set of examples that are adversarial for the ensemble itself is incorporated in the learning.
routing networks: a new kind of neural network which learns to adaptively route its input for multi-task learning . The paper suggests to use a modular network with a controller which makes decisions, at each time step, regarding the next nodule to apply. The paper presents a novel formulation for learning the optimal architecture of a neural network in a multi-task learning framework by using multi-agent reinforcement learning to find a policy, and shows improvement over hard-coded architectures with shared layers.
We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating. The authors introduce a gradient-based approach to minimize an objective function with an L0 sparse penalty to help learn sparse neural networks .
We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets . The authors propose two extensions of GCNs, by removing intermediate non-linearities from the GCN computation and adding an attention mechanism in the aggregation layer. The paper proposes a semi supervised learning algorithm for graph node classification with is inspired from Graph Neural Networks.
A framework for training autoencoder-based generative models, with non-adversarial losses and unrestricted neural network architectures. This paper uses autoencoders to do distribution matching in high dimensional space.
Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures. Proposes a dimensionality reduction method that embeds data into a product manifold of spherical, Euclidean, and hyperbolic manifolds. The algorithm is based on matching the geodesic distances on the product manifold to graph distances.
We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example. The paper presents a branch-and-bound approach to learn good programs where an LSTM is used to predict which branches in the search tree should lead to good programs . Proposes system that synthesizes programs from a single example that generalize better than prior state-of-the-art .
We explore the intersection of VAEs and sparse coding. This paper proposes an extension of VAEs with sparse priors and posteriors to learn sparse interpretable representations.
Phasing out skip-connections in a principled manner avoids degradation in deep feed-forward networks. The authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections . The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets without skip connections.
Compression of Deep neural networks deployed on embedded device. The authors present an l-1 regularized SVRG based training algorithm that is able to force many weights of the network to be 0. This work reduces memory requirements.
A predictive coding based learning algorithm for building deep neural network models of the brain . The paper considers learning of a generative neural network using a predictive coding setup .
Object instance recognition with adversarial autoencoders was performed with a novel 'mental image' target that is canonical representation of the input image. The paper proposes a method to learn features for object recognition that is invariant to various transformations of the object, most notably object pose. This paper investigated the task of few shot recognition via a generated “mental image” as intermediate representation given the input image.
Combine temporal logic with hierarchical reinforcement learning for skill composition . The paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula. Proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition.
We propose a quantization scheme for weights and activations of deep neural networks. This reduces the memory footprint substantially and accelerates inference. CNN model compression aand inference acceleration using quantization.
When a robot is deployed in an environment that humans have been acting in, the state of the environment is already optimized for what humans want, and we can use this to infer human preferences. The authors propose to augment the explicitly stated reward function of an RL agent with auxiliary rewards/costs inferred from the initial state and a model of the state dynamics . This work proposes a way to infer the implicit information in the initial state using IRL and combine the inferred reward with a specified reward.
Systematic categorization of regularization methods for deep learning, revealing their similarities. Attempts to build a taxonomy for regularization techniques employed in deep learning.
We prove the exponential efficiency of recurrent-type neural networks over shallow networks. The authors compare the complexity of tensor train networks with networks structured by CP decomposition .
A novel probabilistic treatment for GAN with theoretical guarantee. This paper proposes a Bayesian GAN that has theoretical guarantees of convergence to the real distribution and put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions.
Defending against adversarial perturbations of neural networks from manifold assumption . The manuscript proposes two objective functions based on the manifold assumption as defense mechanisms against adversarial examples. Defending against adversarial attacks based on the manifold assumption of natural data .
single shot neural architecture search via direct sparse optimization . Presents an architecture search method where connections are removed with sparse regularization. This paper proposes Direct Sparse Optimization, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost. This paper proposes a neural architecture search method based on a direct sparse optimization .
Obtains state-of-the-art accuracy for quantized, shallow nets by leveraging distillation. Proposes small and low-cost models by combining distillation and quantization for vision and neural machine translation experiments . This paper presents a framework of using the teacher model to help the compression for the deep learning model in the context of model compression.
improve NMT with latent trees . This paper describes a method to induce source-side dependency structures in service to neural machine translation.
Learn by working backwards from a single demonstration, even an inefficient one, and progressively have the agent do more of the solving itself. This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. The paper presents a strategy for solving sparse reward tasks with RL by sampling initial states from demonstrations.
External memory for online reinforcement learning based on estimating gradients over a novel reservoir sampling technique. The paper proposes a modified approach to RL, where an additional "episodic memory" is kept by the agent and use a "query network" that based on the current state.
We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation. The goal of this paper is to analyze the effectiveness and generalizability of deep learning by presenting a theoretical analysis of bias-variance decomposition for hierarchical models, specifically Boltzmann Machines . The paper arrives at the main conclusion that it is possible to reduce both the bias and the variance in a hierarchical model.
Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation. This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip.
We present a new pruning method and sparse matrix format to enable high index compression ratio and parallel index decoding process. The authors use Viterbi encoding to dramatically compress the sparse matrix index of a pruned network, reducing one of the main memory overheads and speeding up inference in the parallel setting.
A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills. This paper aims to learn hierarchical policies by using a recursive policy structure regulated by a stochastic temporal grammar . This paper proposes an approach to learning hierarchical policies in a lifelong learning context by stacking policies and then using an explicit "switch" policy.
We propose to use explicit vector algebraic formulae projection as an alternative way to visualize embedding spaces specifically tailored for goal-oriented analysis tasks and it outperforms t-SNE in our user study. Analysis of embedding psaces in a non-parametric (example-based_ way .
We propose a principled approach that endows classifiers with the ability to resist larger variations between training and testing data in an intelligent and efficient manner. Using introspective learning to handle data variations at test time . This paper suggests the use of learned transformation networks, embedded within introspective networks to improve classification performance with synthesized examples.
Input discretization leads to robustness against adversarial examples . The authors present an in-depth study of discretizing / quantizing the input as a defense against adversarial examples .
we proved dimension-independent bounds for low-precision training algorithms . This paper discusses conditions under which the convergence of training models with low-precision weights do not rely on model dimension.
Modifications to MAML and RL2 that should allow for better exploration. The paper proposes a trick of extending objective functions to drive exploration in meta-RL on top of two recent meta-RL algorithms .
A probabilistic neural symbolic model with a latent program space, for more interpretable question answering . This paper proposes a discrete, structured latent variable model for visual question answering that involves compositional generalization and reasoning with significant gain in performance and capability.
We use formal verification to assess the effectiveness of techniques for finding adversarial examples or for defending against adversarial examples. This paper proposes a method for computing adversarial examples with minimum distance to the original inputs. The authors propose to employ provably minimal-distance examples as a tool to evaluate the robustness of a trained network. The paper describes a method for generating adversarial examples that have minimal distance to the training example used to generate them .
Paragraph retriever and machine reader interacts with each other via reinforcement learning to yield large improvements on open domain datasets . The paper introduces a new framework of bi-directional interaction between document retriever and reader for open-domain question answering with idea of 'reader state' from reader to retriever. The paper proposes a multi-document extractive machine reading model composed of 3 distinct parts and an algorithm.
Presents new architecture which leverages information globalization power of u-nets in a deeper networks and performs well across tasks without any bells and whistles. A network architecture for semantic image segmentation, based on composing a stack of basic U-Net architectures, that reduces the number of parameters and improves results. This proposes a stacked U-Net architecture for image segmentation.
We propose a neural network that is able to generate topic-specific questions. Presents a neural network-based approach to generate topic-specific questions with the motivation that topical questions are more meaningful in practical applications. Proposes a topic-based generation method using an LSTM to extract topics using a two-stage encoding technique .
We implement an adversarial domain adaptation network to stabilize a fixed Brain-Machine Interface against gradual changes in the recorded neural signals. Describes a novel approach for implanted brain-machine interface in order to address calibration problem and covariate shift. The authors define a BMI that uses an autoencoder and then address the problem of data drift in BMI.
Analysing and understanding how neural network agents learn to understand simple grounded language . The authors connect psychological experimental methods to understanding how the black box of deep learning methods solves problems. This paper presents an analysis of the agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information .
Learning object parts, hierarchical structure, and dynamics by watching how they move . Proposes an unsupervised learning model that learns to disentangle objects into parts, predict hierarchical structure for the parts, and based on the disentangled parts and the hierarchy, predict motion.
We build an understanding of resource-efficient techniques on Super-Resolution . The paper proposes a detailed empirical evaluation of the trade-offs achieved by various convolutional neural networks on the super resolution problem. This paper proposed to improve the system resource efficiency for super resolution networks.
We investigate ReLU networks in the Fourier domain and demonstrate peculiar behaviour. Fourier analysis of ReLU network, finding that they are biased towards learning low frequency . This paper has theoretical and empirical contributions on topic of Fourier coefficients of neural networks .
The paper proposes using probability distributions instead of points for instance embeddings tasks such as recognition and verification. Paper proposes an alternative to current point embedding and a technique to train them. The paper proposes a model using uncertain-embeddings to extend deep learning to Bayesian applications .
We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance. This paper proposes new layer architectures of neural networks using a low-rank representation of tensors . This paper incorporates tensor decomposition and tensor regression into CNN by using a new tensor regression layer.
We use bilingual dictionaries for data augmentation for neural machine translation . This paper investigates using bilingual dictionaries to create synthetic sources for target-side monolingual data to improve over NMT models trained with small amounts of parallel data.
We propose a novel model of curiosity based on episodic memory and the ideas of reachability which allows us to overcome the known "couch-potato" issues of prior work. Proposes to give exploration bonuses in RL algorithms by giving larger bonuses to observations that are father away in environment steps. The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems and considers many experiments on complex 3D environments .
We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. The paper proposes a new model to use deep models for detecting logical entailment as a product of continuous functions over possible worlds. Proposes a new model designed for machine learning with predicting logical entailment.
The paper is about a new energy-efficient methodology for Incremental learning . Proposes procedure for incremental learning as transfer learning. The paper presents a method to train deep convolutional neural networks in an incremental fashion, in which data are available in small batches over a period of time. Presents an approach to class-incremental learning using deep networks by proposing three different learning strategies in the final/best approach.
use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency . Proposes accelerating RNN by applying the method from Blelloch. The authors propose a parallel algorithm for Linear Surrogate RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM.
Natural language GAN for filling in the blank . This paper proposes to generate text using GANs. Generating text samples using GAN and a mechanism to fill in missing words conditional on the surrounding text .
Comparison of psychophysical and CNN-encoded  texture representations in a one-class neural network novelty detection application. This paper focuses on novelty detection and shows that psychophysical representations can outperform VGG-encoder features in some part of this task . This paper considers detecting anomalies in textures and proposes original loss function. Proposes training two anomaly detectors from three different models to detect perceptual anomalies in visual textures.
We propose a new type of regularization approach that encourages non-overlapness in representation learning, for the sake of improving interpretability and reducing overfitting. The paper introduces a matrix regularizer to simultaneously induce both sparsity and approximate orthogonality. The paper studies a regularization method to promote sparsity and reduce the overlap among the supports of the weight vectors in the learned representations to enhance interpretability and avoid overfitting . The paper proposed a new regularization approach that simultaneously encourages the weight vectors (W) to be sparse and orthogonal to each other.
Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight. Paper links recurrent network deisgn and its effect on how the network reacts to time transformations, and uses this to develop a simple bias initialization scheme.
We propose and verify the effectiveness of learning to teach, a new framework to automatically guide machine learning process. This paper focuses on "machine teaching" and proposes leveraging reinforcement learning by defining the reward as how fast the learner learns and using policy gradient to update the teacher parameters . The authors define a deep learning model composed of four components: a student model, a teacher model, a loss function, and a data set. Suggests a "learning to teach" framework, corresponding to choices over the data presented to the learner.
A differentiable loss for logic constraints for training and querying neural networks. A framework for turning queries over parameters and input, ouput pairs to neural networks into differentiable loss functions and an associated declarative language for specifying these queries . This paper tackles the problem of combining logical approaches with neural networks by translating a logical formula into a non-negative loss function for a neural network.
Genetic algorithms based approach for optimizing deep neural network policies . The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together. This paper proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks.
A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses. Proposes ProxQuant method to train neural networks with quantized weights. Proposes solving binary nets and its variants using proximal gradient descent.
"Bad" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before . In networks with a single hidden layer, the volume of suboptimal local minima exponentially decreases in comparison to global minima. This paper aims to answer why standard SGD based algorithms on neural network converge to 'good' solutions.
We propose an attention-invariant attack method to generate more transferable adversarial examples for black-box attacks, which can fool state-of-the-art defenses with a high success rate. The paper proposes a new way of overcoming state of the art defences against adversarial attacks on CNN. This paper suggests that "attention shift" is a key property behind failure of adversarial attacks to transfer and propose an attention-invariant attack method .
How to Training 100,000 classes on a single GPU . Proposes an efficient hashing method MACH for softmax approximation in the context of large output space, which saves both memory and computation. A method for classification scheme for problems involving a large number of classes in a multi-class setting demonstrated on ODP and Imagenet-21K datasets . The paper presents a hashing based scheme for reducing memory and computation time for K-way classification when K is large .
We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. Suggests a new approach to performing gradient descent for blackbox optimization or training discrete latent variable models.
We propose a support size estimator of GANs's learned distribution to show they indeed suffer from mode collapse, and we prove that encoder-decoder GANs do not avoid the issue as well. The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally. This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated sample, with experiment results interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models. The paper uses birthday paradox to show that some GAN architectures generate distributions with fairly low support.
We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space. Suggests a method for creation of semantical adversary examples. Proposes a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation .
We extend the K-FAC method to RNNs by developing a new family of Fisher approximations. The authors extends the K-FAC method to RNNs and presents 3 ways of approximating F, showing optimization results on 3 datasets, which outperforms ADAM in both number of updates and computation time. Proposes to extend the Kronecker-factor Appropriate Curvature optimization method to the setting of recurrent neural networks. The authors present a second-order method that is specifically designed for RNNs .
The generative model for kernels of convolutional neural networks, that acts as a prior distribution while training on new datasets. A method for modeling convolutional neural networks using a Bayes method. Proposes the 'deep weight prior': the idea is to elicit a prior on an auxilary dataset and then use that prior over the CNN filters to jump start inference for a data set of interest. This paper explores learning informative priors for convolutional neural network models with similar problem domains by using autoencoders to obtain an expressive prior on the filtered weights of the trained networks.
We applied deep learning techniques to hyperspectral image segmentation and iterative feature sampling. Proposes a greedy scheme to select a subset of highly correlated spectral features in a classification task. The paper explores the use of neural networks for classification and segmentation of hypersepctral imaging (HSI) of cells. Classifying cells and implementing cell segmentation based on deep learning techniques with reduction of input features .
We created a new dataset for data interpretation over plots and also propose a baseline for the same. The authors propose a pipeline to solve the DIP problem involving learning from datasets containing triplets of the form {plot, question, answer} . Proposes an algorithm that can interpret data shown in scientific plots.
Improving Predictive State Recurrent Neural Networks via Orthogonal Random Features . Proposes improving the performances of Predicitve State Recurrent Neural Networks by considering Orthogonal Random Features. The paper tackles the problem of training predictive state recurrent neural networks and makes two contributions.
We propose Fidelity-weighted Learning, a semi-supervised teacher-student approach for training neural networks using weakly-labeled data. This paper suggests an approach for learning with weak supervision by using a clean and a noisy dataset and assuming a teacher and student networks . The paper attemps to train deep neural network models with few labelled training samples. The authors propose an approach for training deep learning models for situations where there is not enough reliable annotated data.
We proposed two new approaches,  the incremental sliced inverse regression and incremental overlapping sliced inverse regression, to implement supervised dimension reduction in an online learning manner. Studies sufficient dimension reduction problem and proposes an incremental sliced inverse regression algorithm. This paper proposes an online learning algorithm for supervised dimension reduction, called incremental sliced inverse regression .
We propose a learning model enabling DNN to learn with only 2 bit/weight, which is especially useful for on-device learning . Proposes a method to discretisize a NN incrementally to improve memory and performance.
Learning transport operators on manifolds forms a valuable representation for doing tasks like transfer learning. Uses a dictionary learning framework to learn manifold transport operators on augmented USPS digits. The paper considers the framework of manifold transport operator learning of Culpepper and Olshausen (2009), and interprets it as obtaining a MAP estimate under a probabilistic generative model.
We present a neural variational model for learning language-guided compositional visual concepts. Proposes a novel neural net architecture that learns object concepts by combining a beta-VAE and SCAN. This paper introduces a VAE-based model for translating between images and text, with their latent representation well-suited to applying symbolic operations, giving them a more expressive language for sampling images from text. This paper proposes a new model called SCAN (Symbol-Concept Association Network) for hierarchical concept learning and allows for generalization to new concepts composed from existing concepts using logical operators.
Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses. This paper proposed the combination of topic model and seq2seq conversational model . Proposes a conversational model with topical information by combining seq2seq model with neural topic models and shows the proposed model outperforms some the baseline model seq2seq and other latent variable model variant of seq2seq. The paper addresses the issue of enduring topicality in conversation models and proposes a model which is a combination of a neural topic model and a seq2seq-based dialog system.
The graph analysis problem is transformed into a point cloud analysis problem. Proposes a deep GNN network for graph classification problems using their adaptive graph pooling layer. The authors propose a method for learning representations for graphs .
We propose to generate adversarial example based on generative adversarial networks in a semi-whitebox and black-box settings. Describes AdvGAN, a conditional GAN plus adversarial loss, and evaluates AdvGAN on semi-white box and black box setting, reporting state-of-art results. This paper proposes a way of generating adversarial examples that fool classification systems and wins MadryLab's mnist challenge.
This paper demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set. This paper presents a deep autoencoder model for rating prediction that outperforms other state-of-the-art approahces on the Netflix prize dataset. Proposes to use a deep AE to do rating prediction tasks in recommender systems. The authors present a model for more accurate Netflix recommendations demonstrating that a deep autoencoder can out-perform more complex RNN-based models that have temporal information.
We introduce the Universal Transformer, a self-attentive parallel-in-time recurrent sequence model that outperforms Transformers and LSTMs on a wide range of sequence-to-sequence tasks, including machine translation. Proposes a new model UT, based on the Transformer model, with added recurrence and dynamic halting of the recurrence. This paper extends Transformer by recursively applying a multi-head self-attention block, rather than stacking multiple blocks in the vanilla Transformer.
The paper develops an interpretable continual learning framework where explanations of the finished tasks are used to enhance the attention of the learner during the future tasks, and where an explanation metric is proposed too. The authors propose a framework for continual learning based on explanations for performed classifications of previously learned tasks . This paper proposes an extension to the continual learning framework using existing variational continual learning as the base method with weight of evidence.
Mixed precision training pipeline using 16-bit integers on general purpose HW;  SOTA accuracy for ImageNet-class CNNs; Best reported accuracy for ImageNet-1K classification task with any reduced precision training; . This paper shows that a careful implementation of mixed-precision dynamic fixed point computation can achieve state of the art accuracy using a reduced precision deep learning model with a 16 bit integer representation . Proposes a "dynamic fixed point" scheme that shares the exponent part for a tensor and develops procedures to do NN computing with this format and demonstrates this for limited precision training.
A letter-based ConvNet acoustic model leads to a simple and competitive speech recognition pipeline. This paper applies gated convolutional neural networks to speech recognition, using the training criterion ASG.
A noval GAN framework that utilizes transformation-invariant features to learn rich representations and strong generators. Proposes a modified GAN objective consisting of a classic GAN term and an invariant encoding term. This paper presents the IVE-GAN, a model that introduces en encoder to the Generative Adversarial Network framework.
We propose a method for learning latent dependency structure in variational autoencoders. Uses a matrix of binary random variables to capture dependencies between latent variables in a hierarchical deep generative model. This paper presents a VAE approach in which a dependency structure on the latent variable is learned during training. The authors propose to augment the latent space of a VAE with an auto-regressive structure, to improve the expressiveness of both the inference network and the latent prior .
We introduce a scale-invariant neural network architecture for changepoint detection in multivariate time series. The paper leverages the concept of wavelet transform within a deep architecture to solve change point detection. This paper proposes a pyramid based neural net and applies it to 1D signals with underlying processes occurring at different time scales where the task is change point detection .
RL finds better heuristics for automated reasoning algorithms. Aims to learn a heuristic for a backtracking search algorithm utilizing Reinforcement learning and proposes a model that makes use of Graphical Neural Networks to produce literal and clauses embedding, and use them to predict the quality of each literal to decide the probability of each action. The paper proposes an approach to automatically learning variable selection heuristics for QBF using deep learning .
Assess whether or not your GAN is actually doing something other than memorizing the training data. Aims to provide a quality measure/test for GANs and proposes to evaluate the current approximation of a distribution learnt by a GAN by using Wasserstein distance between two distributions made of a sum of Diracs as a baseline performance. This paper proposed a procedure for assessing the performance of GANs by re-considering the key of observation, using the procedure to test and improve the current GANs .
We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet. Proposes a reinforcement learning based approach for finding non-linearity by searching through combinations from a set of unary and binary operators. This paper utilizes reinforcement learning to search the combination of a set of unary and binary functions resulting in a new activation function . The author uses reinforcement learning to find new potential activation functions from a rich set of possible candidates.
A bottom-up algorithm that expands CNNs starting with one feature per layer to architectures with sufficient representational capacity. Proposes to dynamically adjust the feature map depth of a fully convolutional neural network, formulating a measure of self-resemblance and boosting performance. Introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity. Aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network.
We train a feedforward network without backprop by using an energy-based model to provide local targets . This paper aims at quickening the iterative inference procedure in energy-based models trained with Equilibrium Propagation (EP), by proposing to train a feedforward network to predict a fixed point of the "equilibrating network". Training a separate network to initialize recurrent networks trained using equilibrium propagation .
Learn representations for images that factor out a single attribute. This paper builds on Conditional VAE GANs to allow attribute manipulation in the synthesis process. This paper proposes a generative model to learn the representation which can separate the identity of an object from an attribute, and extends the autoencoder adversarial by adding an auxiliary network.
We present a model for consistent 3D reconstruction and jumpy video prediction e.g. producing image frames multiple time-steps in the future without generating intermediate frames. This paper proposes a general method for indexed data modeling by encoding index information together with observation into a neural network, and then decode the observation condition on the target index. Proposes using a VAE that encodes input video in a permuation invariant way to predict future frames of a video.
Analyzing the popular Adam optimizer . The paper trys to improve Adam based on variance adaption with momentum by proposing two algorithms . This paper analyzes the scale-invariance and the particular shape of the learning rate used in Adam, arguing that Adam's update is a combination of a sign-update and a variance-based learning rate. The paper splits ADAM algorithm into two components: stochastic direction in sign of gradient and adaptive stepwise with relative variance, and two algorithms are proposed to test each of them.
We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The authors connect dropout parameters to a bound of the Rademacher complexity of the network . Relates complexity of networks' learnability to dropout rates in backpropagation.
Optimized gated deep learning architectures for sensor fusion is proposed. The authors improve upon several limitations of the baseline negated architecture by proposing a coarser-grained gated fusion architecture and a two-stage gated fusion architecture . Proposes two gated deep learning architectures for sensor fusion and by having the grouped features, demonstrates improved performance, especially in the presence of random sensor noise and failures.
Batch normalization causes exploding gradients in vanilla feedforward networks. Develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights. Provides a dynamic perspective on deep neural network using the evolution of the covariance matrix along with the layers.
We train a graph network to predict boolean satisfiability and show that it learns to search for solutions, and that the solutions it finds can be decoded from its activations. The paper describes a general neural network architecture for predicting satisfiability . This paper presents the NeuroSAT architecture which uses a deep message passing neural net for predicting the satisfiability of CNF instances .
A neural sequence model that learns to forecast on a directed graph. The paper proposes the Diffusion Convolutional Recurrent Neural Network architecture for the spatiotemporal traffic forecasting problem . Proposes to build a traffic forecasting model using a diffusion process for convolutional recurrent neural networks to address saptio-temporal autocorrelation.
We train neural networks to be uncertain on noisy inputs to avoid overconfident predictions outside of the training distribution. Presents an approach to obtain uncertainty estimates for neural network predictions that has good performance when quantifying predictive uncertainty at points that are outside of the training distribution. The paper considers the problem of uncertainty estimation of neural networks and proposes to use Bayesian approach with noice contrastive prior .
This study highlights a key difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias. Seeks to establish via a series of well-designed experiments that CNNs trained for image classification don’t encode shape-bias like human vision. This paper highlights the fact that CNNs will not necessarily learn to recognize objects based on their shape and shows they will overfeat to noise based features.
We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views. This paper presents a GAN-based method for image generation that attempts to separate latent variables describing image content from those describing properties of view. This paper proposes a GAN architecture that aims at decomposing the underlying distribution of a particular class into "content" and "view". Proposes a new generative model based on the Generative Adversarial Network (GAN) that disentangles the content and the view of objects without view supervision and extends GMV into a conditional generative model that takes an input image and generates different views of the object in the input image.
A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed. Proposes a method of compressing network by means of weight ternarization. The paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values. The paper extends the loss-aware weight binarization scheme to terarization and arbitrary m-bit quantization and demonstrate its promising performance.
We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step. The paper presents a new policy gradient technique for learning options, where a single sample can be used to update all options. Proposes an off-policy method for learning options in complex continuous problems.
Unsupervised feature selection through capturing the local linear structure of the data . Proposes locally linear unsupervised feature selection. The paper proposes the LLUFS method for feature selection.
Using a simple language-driven navigation task, we study the compositional capabilities of modern seq2seq recurrent networks. This paper focuses on the zero-shot learning compositional capabilities of modern sequence-to-sequence RNNs and  exposes the short-comings of current seq2seq RNN architectures. The paper analyzes the composition abilities of RNNs, specifically, the generalization ability of RNNs on random subset of SCAN commands, on longer SCAN commands, and of composition over primitive commands. The authors introduce a new dataset that facilitates the analysis of a Seq2Seq learning case .
We tackle the problem of similarity learning for structured objects with applications in particular in computer security, and propose a new model graph matching networks that excels on this task. Authors introduce a Graph Matching Network for retrival and matching of graph structured objects. The authors attack the problem of graph matching by proposing an extension of graph embedding networks . The authors present two methods for learning a similarity score between pairs of graphs and show the beneficiality of introducing idesa from graph matching to graph neural networks.
We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning. Modifications to the skip-thought framework for learning sentence embeddings. This paper presents a new RNN encoder–CNN decoder hybrid design for use in pretraining, which does not require an autoregressive decoder when pretraining encoders. The authors extend Skip-thought by decoding only one target sentence using a CNN decoder.
A statistical approach to compute sample likelihoods in Generative Adversarial Networks . Show that WGAN with entropic regularization maximizes a lower bound on the likelihood of the observed data distribution. Authors claim it is possible to leverage the upper bound from an entropy regularized optimal transport to come up with a measure of 'sample likelihood'.
We introduce geomstats, an efficient Python package for Riemannian modelization and optimization over manifolds compatible with both numpy and tensorflow . The paper introduces the software package geomstats, which provides simple use of Riemannian manifolds and metrics within machine learning models . Proposes a Python package for optimization and applications on Reimannian manifolds and highlights the differences between Geomstats package and other packages. Introduces a geometric toolbox, Geomstats, for machine learning on Reimannian manifolds.
We construct dynamic sparse graph via dimension-reduction search to reduce compute and memory cost in both DNN training and inference. The authors propose to use dynamic sparse computation graph for reducing the computation memory and time cost in deep neural network (DNN). This paper proposes a method to speed up training and inference of deep neural networks using dynamic pruning of the compute graph.
We develop a practical extension of Information-Directed Sampling for Reinforcement Learning, which accounts for parametric uncertainty and heteroscedasticity in the return distribution for exploration. The authors propose a way of extending Information-Directed Sampling to reinforcement learning by combining two types of uncertainty to obtain a simple exploration strategy based on IDS. This paper investigates sophistical exploration approaches for reinforcement learning built on Information Direct Sampling and on Distributional Reinforcement Learning .
using graph neural network to model structural information of the agents to improve policy and transferability . A method for representing and learning structured policy for continuous control tasks using Graph Neural Networks . The submission proposes incorporation of additional structure into reinforcement learning problems, particularly the structure of the agent's morphology . Propose an application of Graph Neural Networks to learning policies for controlling "centipede" robots of different lengths.
This paper presents a hierarchical reinforcement learning framework based on deterministic option policies and mutual information maximization. Proposes an HRL algorithm that attempts to learn options that maximize their mutual information with the state-action density under the optimal policy. This paper proposes an HRL system in which the mutal information of the latent variable and the state-action pairs is approximately maximized. Proposes a criterion that aims to maximize the mutual information between options and state-action pairs and show empirically that the learned options decompose the state-action space but not the state space.
We introduce and validate hierarchical local interpretations, the first technique to automatically search for and display important interactions for individual predictions made by LSTMs and CNNs. A novel approach to explain neural network predictions by learning hierarchical representations of groups of input features and their contribution to the final prediction . Extends an existing feature interpretation method for LSTMs to more generic DNNs and introduces a hierarchical clustering of the input features and the contributions of each cluster to the final prediction. This paper proposes a hierarchical extension of contextual decomposition.
We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints. Proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. This paper introduces an approach to compressing neural networks by looking at the correlation of filter responses in each layer via two strategies. This paper proposes a compression method based on spectral analysis .
Multiple diverse query reformulation agents trained with reinforcement learning to improve search engines. Parellelization of ensemble method in reinforement learning for query reformulation, speeding up training and improving the diversity of learnt freformulations . The authors propose to train multiple distinct agents, each over a different subset of the training set. The authors propose an ensemble approach for query reformulation .
We introduce a network embedding method that accounts for prior information about the network, yielding superior empirical performance. The paper proposed to use a prior distribution to constraint the network embedding, for the formulation this paper used very restricted Gaussian distributions. Proposes learning unsupervised node embeddings by considering the structural properties of networks.
We analyze convergence of Adam-type algorithms and provide mild sufficient conditions to guarantee their convergence, we also show  violating the conditions can makes an algorithm diverge. Presents a convergence analysis in the non-convex setting for a family of optimization algorithms. This paper investigates the convergence condition of Adam-type optimizers in the unconstrained non-convex optimization problems.
Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function. The paper proposes using the absolute value activation function in an autoencoder architecture with an additional supervised learning term in the objective function . This paper introduces a reversible network with absolute value used as the activation function .
We propose a Transformer based relation extraction model that uses pre-trained language representations instead of explicit linguistic features. Presents a transformer-based relation extraction model that leverages pre-training on unlabeled text with a language modeling objective. This article describes a novel application of Transformer networks for relation extraction. The paper presents a Transformer based architecture for relaxation extraction, evaluating on two datasets.
We propose a simple and efficent method for architecture search for convolutional neural networks. Proposes a neural architecture search method that achieves close to state-of-the-art accuracy on CIFAR10 and takes much less computational resources. Presents a method to search neural network architectures at the same time of training which dramatically saves training time and architecture searching time. Proposes variant of neural architecture search using network morphisms to define a search space using CNN architectures completing the CIFAR image classification task .
Using GANs to generate graphs via random walks. The authors proposed a generative model of random walks on graphs that allows for model-agnostic learning, controllable fitting, ensemble graph generation . Proposes a WGAN formulation for generating graphs based on random walks using node embeddings and an LSTM architecture for modeling.
We propose to solve a problem of simultaneous classification and novelty detection within the GAN framework. Proposes a GAN to unify classification and novelty detection. The paper presents a method for novelty detection based on a multi-class GAN which is trained to output images generated from a mixture of the nominal and novel distributions. The paper proposes a GAN for novelty detection using a mixture generator with feature matching loss .
Speaker verificaiton performance can be significantly improved by adapting the model to in-domain data using Generative Adversarial Networks. Furthermore, the adaptation can be performed in an unsupervised way. Propose a number of GAN variants on the task of speaker recognition in the domain mismatched condition.
Minimising the synergistic mutual information within the latents and the data for the task of disentanglement using the VAE framework. Proposes a new objective function for learning dientangled representations in a variational framework by minimizing the synergy of the information provided. The authors aim at training a VAE that has disentangled latent representations in a "synergistically" maximal way. This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables.
Accelerating SGD by arranging examples differently . The paper presents a method for improving the convergence rate of Stochastic Gradient Descent for learning embeddings by grouping similar training samples together. Proposes a non-uniform sampling strategy to construct minibatches in SGD for the task of learning embeddings for object associations.
Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue . This paper proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations with sizable gains over the baselines. Proposes combining external pretrained word embeddings and pretrained word embeddings on training data by keeping them as two views. Proposes method to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to conversational datasets and applies new variants of LSTM-based model to the task of response-selection in dialogue modeling.
We analyze problems when training learned optimizers, address those problems via variational optimization using two complementary gradient estimators, and train optimizers that are 5x faster in wall-clock time than baseline optimizers (e.g. Adam). This paper uses un-rolled optimization to learn neural networks for optimization. This paper tackles the problem of learning an optimizer, specifically the authors focus on obtaining cleaner gradients from the unrolled training procedure. Presents a method for "learning an optimizer" by using a Variational Optimization for the "outer" optimizer loss and proposes the idea to combine both the reparametrized gradient and the score-function estimator for the Variational Objective and weights them using a product of Gaussians formula for the mean.
A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds. The paper proposes an algorithm to restrict the staleness in asynchronous SGD and provides theoretical analysis . Proposes a hybrid-algorithm to eliminate the gradient delay of asynchronous methods.
We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. Proposes a regularization scheme to protect quantized neural networks from adversarial attacks using a Lipschitz constant filitering of the inner layers' inpout-output.
A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures. Proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. Proposes a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps.
A fast second-order solver for deep learning that works on ImageNet-scale problems with no hyper-parameter tuning . Choosing direction by using a single step of gradient descent "towards Newton step" from an original estimate, and then taking this direction instead of original gradient . A new approximate second-order optimization method with low computational cost that replaces the computation of the Hessian matrix with a single gradient step and a warm start strategy.
Attention based model trained with REINFORCE with greedy rollout baseline to learn heuristics with competitive results on TSP and other routing problems . Presents an attention-based approach to learning a policy for solving TSP and other routing-type combinatorial optimzation problems. This paper trys to learn heuristics for solving combinatorial optimisation problems .
An algorithm for optimizing regularization hyper-parameters during training . The paper proposes a way to re-initialize y at each update of lambda and a clipping procedure of y to maintain the stability of the dynamical system. Proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method. Proposes an extension to an existing method to optimize regularization hyperparameters.
Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable. This paper describes a comprehensive validation of LSTM-based word and character language models, leading to a significant result in language modeling and a milestone in deep learning.
We show how using skip connections can make speech enhancement models more interpretable, as it makes them use similar mechanisms that have been explored in the DSP literature. The authors propose incorporating Residual, Highway and Masking blocks inside a fully convolutional pipeline in order to understand how iterative inference of the output and the masking is performed in a speech enhancement task . The authors interpret highway, residual and masking connections. The authors generate their own noisy speech by artificially adding noise from a well established noise data-set to a less know clean speech data-set.
A method for eliminating gradient variance and automatically tuning priors for effective training of bayesian neural networks . Proposes a new approach to perform deterministic variational inference for feed-forward BNN with specific nonlinear activation functions by approximating layerwise moments. The paper considers a purely deterministic approach to learning variational posterior approximations for Bayesian neural networks.
A formal method's approach to skill composition in reinforcement learning tasks . The paper combines RL and constraints expressed by logical formulas by setting up an automation from scTLTL formulas. Proposes a method that helps to construct policy from learned subtasks on the topic of combining RL tasks with linear temporal logic formulas.
Deriving a general formulation of a multi-modal VAE from the joint marginal log-likelihood. Proposes a multi-modal VAE with a variational bound derived from the chain rule. This paper proposes an objective, M^2VAE, for multi-modal VAEs, which is supposed to learn a more meaningful latent space representation.
We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights. The paper proposes a version of IWAE-style training that uses SMC instead of classical importance sampling. This work proposes auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carto objective based on SMC.
We propose an architecture for learning value functions which allows the use of any linear policy evaluation algorithm in tandem with nonlinear feature learning. The paper proposes a two-timescale framework for learning the value function and a state representation altogether with nonlinear approximators. This paper proposes Two-Timescale Networks (TTNs) and prove the convergence of this method using methods from two time-scale stochastic approximation. This paper presents a Two-Timescale Network (TTN) that enables linear methods to be used to learn values.
We propose simple, but effective, low-rank matrix factorization (MF) algorithms to speed up in running time, save memory, and improve the performance of LSTMs. Proposes to accelerate LSTM by using MF as the post-processing compression strategy and conducts extensive experiements to show the performance.
A forensic metric to determine if a given image is a copy (with possible manipulation) of another image from a given dataset. Introduces the siamese network to identify duplicate and copied/modified images, which can be used to improve surveillance of the published and in-peer-review literature. The paper presents an application of deep convolutional networks for the task of duplicate image detection . This work addresses the problem of finding duplicate/near duplicate images from biomedical publications and proposes a standard CNN and loss functions and apply it to this field.
Stable GAN training in high dimensions by using an array of discriminators, each with a low dimensional view of generated samples . The paper proposes to stabilize GAN training by using an ensemble of discriminators, each working on a random projection of the input data, to provide the training signal for the generator model. The paper proposes a GAN training method for improving the training stability. The paper proposes a new approach to GAN training, which provides stable gradients to train the generator.
we show a geometric method to perfectly encode categroy tree information into pre-trained word-embeddings. The paper proposes N-ball embedding for taxonomic data where an N-ball is a pair of a centroid vector and the radius from the center. The paper presents a method for tweaking existing vector embeddings of categorical objects (such as words), to convert them to ball embeddings that follow hierarchies. Focuses on adjusting the pretrained word embeddings so that they respect the hypernymy/hyponymy relationship by appropriate n-ball encapsulation.
We propose Convolutional CRFs a fast, powerful and trainable alternative to Fully Connected CRFs. The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel and show that inference is more efficient and training is easier. Proposes to perform message passing on a truncated Gaussian kernel CRF using a defined kernel and parallelized message passing on GPU.
We propose a model agnostic approach to validation of Q&A system robustness and demonstrate results on state-of-the-art Q&A models. Addresses the problem of robustness to adversarial information in question answering. Improving robustness of machine comprehension/question answering.
multi generator to capture Pdata, solve the competition and one-beat-all problem . Proposes parallel GANs to avoid mode collapse in GANs through a combination of multiple weak generators.
Weakly-supervised image segmentation using compositional structure of images and generative models. This paper creates a layered representation in order to better learn segmentation from unlabeled images. This paper proposes a GAN-based generative model that decomposes images into multiple layers, where the objective of the GAN is to distinguish real images from images formed by combining the layers. This paper proposes a neural network architecture around the idea of layered scene composition .
We present a geometric framework for proving robustness guarantees and highlight the importance of codimension in adversarial examples. This paper gives a theoretical analysis of adversarial examples, showing that  there exists a tradeoff between robustness in different norms, adversarial training is sample inefficient, and the nearest neighbor classifier can be robust under certain conditions.
CharNMT is brittle . This paper investigates the impact of character-level noise on 4 different neural machine translation systems . This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural. This paper investigates the impact of noisy input on Machine Translation and tests ways to make NMT models more robust .
We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet. The paper explains and generalizes approaches for learning neural nets with hard activation. This paper examines the problem of optimizing deep networks of hard-threshold units. The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it with a collection of heuristics/approximations.
Using a novel, controlled, visual-relation challenge, we show that same-different tasks critically strain the capacity of CNNs; we argue that visual relations can be better solved using attention-mnemonic strategies. Demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data. This paper explores how current CNN's and Relational Networks fail to recognize visual relations in images.
We propose AD-VAT, where the tracker and the target object, viewed as two learnable agents, are opponents and can mutually enhance during training. This work aims to address the visual active tracking problem with a training mechanism in which the tracker and target serve as mutual opponents . This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. Proposes a novel reward function - "partial zero sum", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.
We presented a method to jointly learn a Hierarchical Word Embedding (HWE) using a corpus and a taxonomy for identifying the hypernymy relations between words. The paper presents a method to jointly learn word embeddings using co-occurrence statistics as well as incorporating hierarchal information from semantic networks. This paper proposed a joint learning method of hypernym from both raw text and supervised taxonomy data. This paper proposes adding a measure of "distributional inclusion" difference to the GloVE objective for the purpose of representing hypernym relations.
integration of self-organization and supervised learning in a hierarchical neural network . The paper discusses learning in a neural network with three layers, where the middle layer is topographically organized and investigates interplay between unsupervised and hierarchical supervised learning in biological context. A supervised variant of Kohonen's self-organizing map (SOM), but where the linear output layer is replaced with squared error by a softmax layer with cross-entropy. Proposes a model using hidden neurons with self-organising activation function, whose outputs feed to classifier with softmax output function.
precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization . Investigates the problem of neural network quantization by employing an end-to-end precision highway to reduce the accumulated quantization error and enable ultra-low precision in deep neural networks. This paper studies methods to improve the performance of quantized neural networks . This paper proposes to keep a high activation/gradient flow in two kinds of networks structures, ResNet and LSTM.
An algorithm for training neural networks efficiently on temporally redundant data. The paper describes a neural coding scheme for spike based learning in deep neural networks . This paper presents a method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data. This paper applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network, combining the three components in a way not seen previously.
Information bottleneck behaves in surprising ways whenever the output is a deterministic function of the input. Argues that most real classification problems show such a deterministic relation between the class labels and the inputs X and explores several issues that result from such pathologies. Explores issues that arise when applying information bottlenext concepts to deterministic supervised learning models . The authors clarify several counter-intuitive behaviors of the information bottleneck method for supervised learning of a deterministic rule.
We prove that idealised Bayesian neural networks can have no adversarial examples, and give empirical evidence with real-world BNNs. The paper studies the adversarial robustness of Bayesian classifiers and state two conditions that they show are provably sufficient for "idealised models" on "idealised datasets" to not have adversarial examples . Paper posit a class of discriminative BAyesian classifiers that do not have any adversarial examples.
We introduce the first instance of adversarial attacks that reprogram the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. The authors present a novel adversarial attack scheme where a neural net is repurposed to accomplish a different task than the one it was originally trained on . This paper proposed "adversarial reprogramming" of well-trained and fixed neural networks and show that adversarial reprogramming is less effective on untrained networks. The paper extends the idea of 'adversarial attacks' in supervised learning of NNs to a full repurposing of the solution of a trained net.
We develop a new scheme to predict the generalization gap in deep networks with high accuracy. Authors suggest using a geometric margin and layer-wise margin distribution for predicting generalization gap. Empirically shows an interesting connection between the proposed margin statistics and the generalization gap, which can be used to provide some prescriptive insights towards understanding generalization in deep neural nets.
We propose an algorithm for provably recovering parameters (convolutional and output weights) of a convolutional network with overlapping patches. This paper studies the theoretical learning of one-hidden-layer convolutional neural nets, resulting in a learning algorithm and provable guarantees using the algorithm. This paper gives a new algorithm for learning a two layer neural network which involves a single convolutional filter and a weight vector for different locations.
A new regularization term can improve your training of wasserstein gans . The paper proposes a regularization scheme for Wasserstein GAN based on relaxation of the constraints on the Lipschitz constant of 1. The article deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric.
We propose the Wasserstein proximal method for training GANs. Proposes a new GAN procedure that takes into account points generated in the previous iteration and updates the generator to be carried out l times. Considers natural gradient learning in GAN learning, where the Riemannian structure induced by the Wasserstein-2 distance is employed. The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN and the authors propose the Wasserstein proximal operator as a regularization.
This paper introduces an elimination based heuristic function for sequential decision making, suitable for guiding AND/OR search algorithms for solving influence diagrams. generalizes minibuckets inference heuristic to influence diagrams.
Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers. The authors focus on the problem of uncertainty propagation DNN . This paper revisits the feed-forward propagation of mean and variance in neurons, by addressing the problem of propagating uncertainty through max-pooling layers and softmax.
A discriminator that is not easily fooled by adversarial example makes GAN training more robust and leads to a smoother objective. This paper proposes a new way to stabilize the training process of GAN by regularizing the Discriminator to be robust to adversarial examples. The paper proposes a systematic way of training GANs with robustness regularization terms, allowing for smoother training of GANs. Presents idea that making a discriminator robust to adversarial perturbations the GAN objective can be made smooth which results in better results both visually and in terms of FID.
We model the activation function of each neuron as a Gaussian Process and learn it alongside the weight with Variational Inference. Propose placing Gaussian process priors on the functional form of each activation function in the neural net to learn the form of activation functions.
We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The paper proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This paper proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.
StarHopper is a novel touch screen interface for efficient and flexible object-centric camera drone navigation . The authors outline a new drone control interface StarHopper that they have developed, combining automated and manual piloting into a new hybrid navigation interface and gets rid of the assumption that the target object is already in the drone’s FOV by using an additional overhead camera. This paper presents StarHopper, a system for semi-automatic drone navigation in the context of remote inspection. Introduces StarHopper, an application that uses computer vision techniques with touch input to support drone piloting with an object-centric approach.
A self-attention network for RNN/CNN-free sequence encoding with small memory consumption, highly parallelizable computation and state-of-the-art performance on several NLP tasks . Proposes applyting self-attention at two levels to limit the memory requirement in attention-based models with a negligible impact on speed. This paper introduces bi-directional block self-attention model as a general-purpose encoder for various sequence modeling tasks in NLP .
A new state-of-the-art model for multi-evidence question answering using coarse-grain fine-grain hierarchical attention. Proposes a method for multi-hop QA based on two separate modules (coarse-grained and fine-grained modules). This paper proposes an interesting coarse-grain fine-grain coattention network architecture to address multi-evidence question answering . Focuses on multi-choice QA and proposes a coarse-to-fine scoring framework.
We propose new methodology for unbalanced optimal transport using generative adversarial networks. The authors consider the unbalanced optimal transport problem between two measures with different total mass using a stochastic min-max algorithm and local scaling . The authors propose an approach to estimate unbalanced optimal transport between sampled measures that scales well in the dimension and in the number of samples. The paper introduces a static formulation for unbalanced optimal transport by learning simultaneously a transport map T and scaling factor xi.
We propose a new saliency map extraction method which results in extracting higher quality maps. Proposes a classifier-agnostic method for saliency map extraction. This paper introduces a new saliency map extractor that seems to improve state-of-the-art results. The authors argue that when an extracted saliency map is directly dependent on a model, then it might not be useful for a different classifier, and suggests a scheme to approximate the solution.
We propose a novel method to incorporate the set of instance attributes for image-to-image translation. This paper proposes a method -- InstaGAN -- which builds on CycleGAN by taking into account instance information in the form of per-instance segmentation masks, with results that compare favorably to CycleGAN and other baselines. Proposes to add instance-aware segmentation masks for the problem of unpaired image-to-image translation.
The parameter-function map of deep networks is hugely biased; this can explain why they generalize. We use PAC-Bayes and Gaussian processes to obtain nonvacuous bounds. The paper studies the generalization capabilities of deep neural networks, with the help of the PAC-Bayesian learning theory and empirically backed intuitions. This paper proposes an explaination of the generalization behaviors of large over-parameterized neural networks by claiming the parameter-function map in neural networks are biased towards "simple" functions and generalization behavior will be good if the target concept is also "simple".
We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization . The paper presents a combination of evolutionary computation and variational EM for models with binary latent variables represented via a particle-based approximation . The paper makes an attempt to tightly integrate expectation-maximization training algorithms with evolutionary algorithms.
We propose an efficient recurrent network model for forward prediction on time-varying distributions. This paper proposes a method for creating neural nets that maps historical distributions onto distributions and applies the method to several distribution prediction tasks. Proposes a Reccurent Distribution Regression Network which uses a recurrent architecture upon a previous model Distribution Regression Network. This paper is on regressing over probability distributions by studying time varying distributions in a recurrent neural network setting .
A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task. This paper proposes a new method for classifying nodes of a graph, which can be used in semi-supervised scenarios and on a completely new graph. The paper introduces a neural network architecture to operate on graph-structured data named Graph Attention Networks. Provides a fair and almost comprehensive discussion of the state of art approaches to learning vector representations for the nodes of a graph.
A novel reinforcement learning based approach to compress deep neural networks with knowledge distillation . This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process . Introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model.
We prove that the mode collapse in conditional GANs is largely attributed to a mismatch between reconstruction loss and GAN loss and introduce a set of novel loss functions as alternatives for reconstruction loss. The paper proposes a modification to the traditional conditional GAN objective in order to promote diverse, multimodal generation of images. This paper proposes an alternative to L1/L2 errors that are used to augment adversarial losses when training conditional GANs.
We use causal inference to characterise the architecture of generative models . This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN. This work exploits the causality principle to quantify how the weights of successive layers adapt to each other.
An approach to learning a shared embedding space between visually distinct games. A new approach for learning underlying structure of visually distinct games combining convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning and adversarial approach to force the embedding representation to be independent of the visual representation of games . Introduces a method to learn a policy on visually distinct games by adapting deep reinforcement learning. This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics .
We study the state equation of a recurrent neural network. We show that SGD can efficiently learn the unknown dynamics from few input/output observations under proper assumptions. The paper studies discrete-time dynamical systems with a non-linear state equation, proving that running SGD on a fixed-length trajectory gives logarithmic convergence. This work considers the problem of learning a non-linear dynamical system in which the output equals the state. This paper studies the ability of SGD to learn dynamics of a linear system and non-linear activation.
A new knowledge distill method for transfer learning . The work introduces a knowledge distillation method using the proposed neuron manifold concept. Proposes a knowledge distilling method in which neural manifold is taken as the transferred knowledge.
We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. The paper proposes an idea of combining different size models together into one shared net, greatly improving performance for detection . This paper trains a single network executable at different widths.
Similarity network to learn a non-metric visual similarity estimation between a pair of images . The authors propose learning similarity measure for visual similarity and obtain by this an improvement in very well-known datasets of Oxford and Paris for image retrival. The paper argues that it is more suitable to use non-metric distances instead of metric distances.
A new cyclic adversarial learning augmented with auxiliary task model which improves domain adaptation performance in low resource supervised and unsupervised situations . Proposes an extension of cycle-consistent adversatial adaptation methods in order to tackle domain adaptation where limited supervised target data is available. This paper introduces a domain adaptation approach based on the idea of Cyclic GAN and proposes two different algorithms.
We develop a method for learning structural signatures in networks based on the diffusion of spectral graph wavelets. Using spectral graph wavelet diffusion patterns of a node's local meighbothood to embed the node in a low-dimensional space . The paper derived a way to compare nodes in graph based on wavelet analysis of graph laplacian.
A mixed reality driving simulator using stereo cameras and passthrough VR evaluated in a user study with 24 participants. Proposes a complicated system for driving simulation. This paper presents a mixed reality driving simulator setup to enhance the sensation of presence . Proposes a mixed reality driving simulator that incorporates traffic generation and claims an enhanced "presence" due to an MR system.
Quadrature rules for kernel approximation. The paper proposes improving the kernel approximation of random features by using quadrature rules like stochastic spherical-radial rules. The authors propose a novel version of the random feature map approach to approximately solve large-scale kernel problems. This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature, a new way to apply quadrature rules to improve kernel approximation.
How to build neural-speakers/listeners that learn fine-grained characteristics of 3D objects, from referential language. The authors provide a study on learning to refer to 3D objects, collecting a dataset of referential expressions and training several models by experimenting with a number of architectural choices .
We present a framework for learning object-centric representations suitable for planning in tasks that require an understanding of physics. The paper presents a platform for predicting images of objects interacting with each other under the effect of gravitational forces. The paper presents a method that learns to reproduce 'block towers' from a given image. Proposes a method which learns to reason on physical interaction of different objects with no supervison of object properties.
We provide efficiently checkable necessary and sufficient conditions for global optimality in deep linear neural networks, with some initial extensions to nonlinear settings. The paper gives conditions for the global optimality of the loss function of deep linear neural networks . The paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks. Studies some theoretical properties of deep linear networks.
Using recurrent auto-encoder model to extract multidimensional time series features . This writeup describes an application of recurrent autoencoder to analyze of multidimensional time series . The paper describes a sequence to sequence auto-encoder model which is used to learn sequence representations, showing that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements. Proposes a strategy inspired by the recurrent auto-encoder model such that clustering multidimensional time series data can be performed based on context vectors.
We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations. Proposes a graph-to-graph translation model for molecule optimization inspired by matched molecular pair analysis. Extension of JT-VAE into the graph to graph translation scenario by adding the latent variable to capture multi-modality and an adversarial regularization in the latent space . Proposes a quite complex system, involving many different choices and components, for obtaining chemical compouds with improved properties starting from a given corpora.
We learn a fast neural solver for PDEs that has convergence guarantees. Develops a method to accelerate the finite difference method in solving PDEs and proposes a revised framework for fixed point iteration after discretization. The authors propose a linear method for speeding up PDE solvers.
We perform functional variational inference on the stochastic processes defined by Bayesian neural networks. Fitting of variational Bayesian Neural Network approximations in functional form and considering matching to a stochastic process prior implicitly via samples. Presents a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors features in the literature. Presents a new variational inference algorithm for Bayesian neural network models where the prior is specified functionally rather than via a prior over weights.
We embed words in the hyperbolic space and make the connection  with the Gaussian word embeddings. This paper adapts the Glove word embedding to a hyperbolic space given by the Poincare half-plane model . This paper proposes an approach to implement a GLOVE-based hyperbolic word embedding model, which is optimized via the Riemannian Optimization methods.
Memory Networks do not learn multi-hop reasoning unless we supervise them. Claims multi-hop reasoning is not easy to learn directly and requires direct supervision and doing well on WikiHop doesn't necessarily mean the model is actually learning to hop. The paper proposes to investigate the well-known problem of memory network learning and more precisely the difficulty of the attention learning supervision with such models. This paper argues that memory network fails to learn reasonable multi-hop reasoning.
We introduce generative networks that do not require to be learned with a discriminator or an encoder; they are obtained by inverting a special embedding operator defined by a wavelet Scattering transform. Introduces scattering transforms as image generative models in the context of Generative Adversarial Networks and suggest why they could be seen as Gaussianization transforms with controlled information loss and invertibility. The paper proposes a generative model for images that does no require to learn a discriminator (as in GAN’s) or learned embedding.
We propose a new model for neural speed reading that utilizes the inherent punctuation structure of a text to define effective jumping and skipping behavior. The paper proposes a Structural-Jump-LSTM model to speed up machine reading with two agents instead of one . Proposes a novel model for neural speed reading in which the new reader has the ability to skip a word or sequence of words. The paper proposes a fast-reading method using skip and jump actions, showing that the proposed method is as accurate as LSTM but uses much less computation.
We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation. The paper proposes improving the performance of recommendation systems through reinforcement learning by using an Imagination Reconstruction Network. The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks.
Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness . This paper presents an adaptation of the algorithmic robustness of Xu&Mannor'12 and presents learning bounds and an experimental showing correlation between empirical ensemble robustness and generalization error. Proposes a study of the generalization ability of deep learning algorithms using an extension of notion of stability called ensemble robustness and gives bounds on generalization error of a randomized algorithm in terms of stability parameter and provides empirical study attempting to connect theory with practice. The paper studied the generalization ability of learning algorithms from the robustness viewpoint in a deep learning context .
Few-shot learning PixelCNN . The paper proposes on using density estimation when the availability of training data is low by using a meta-learning model. This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning . The paper focuses on few shot learning with autoregressive density estimation and improves PixelCNN with neural attention and meta learning techniques.
We show that SGD learns two-layer over-parameterized neural networks with Leaky ReLU activations that provably generalize on linearly separable data. The paper studies overparameterised models being able to learn well-generalising solutions by using a 1-hidden layer network with fixed output layer. This paper shows that on linearly seperabel data, SGD on an overparameterized network can still lean a classifier that provably generalizes.
Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus . Proposes regularizing standard RL losses with the negative conditional mutual information for policy search in a multi-goal RL setting. This paper proposes the concept of decision state and proposes a KL divergence regularization to learn the structure of the tasks to use this information to encourage the policy to visit the decision states. The paper proposes a method of regularising goal-conditioned policies with a mutual information term.
We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems. The authors propose an approach that combines random search with the surrogate gradient information and give a discussion on variance-bias trade-off as well as a discussion on hyperparameter optimization. The paper proposes a method to improve random search by building a subspace of the previous k surrogate gradients. This paper attempts accelerating the OpenAI type evolution by introducing a non-isotrophic distribution with a covariance matrix in the form I + UU^t and external information such as a surrogate gradient to determine U .
A GAN using graph convolution operations with dynamically computed graphs from hidden features . The paper proposes a version of GANs specifically designed for generating point clouds with the core contribution of the work the upsampling operation. This paper proposes graph-convolutional GANs for irregular 3D point clouds that learn domain and features at the same time.
We introduce a fast and easy-to-implement algorithm that is robust to dataset noise. The paper aims to remove potential examples with label noise by discarding the ones with large losses in the training procedure.
Gradient-based attacks on binarized neural networks are not effective due to the non-differentiability of such networks; Our IPROP algorithm solves this problem using integer optimization . Proposes a new target propagation style algorithm to generate strong adversarial attacks on binarized neural networks. This paper proposed a new attack algorithm based on MILP on binary neural networks. This paper presents an algorithm to find adversarial attacks to binary neural networks which iteratively finds desired representations layer by layer from the top to the input and is more efficient than solving the full mixed integer linear programming (MILP) solver.
Decoding the last token in the context using the predicted next token distribution acts as a regularizer and improves language modeling. The authors introduce the idea of past decoding for the purpose of regularization for improved perplexity on Penn Treebank . Proposes an additional loss term to use when training an LSTM LM and shows that by adding this loss term they can achieve SOTA perplexity on a number of LM benchmarks. Suggests a new regularization technique which can be added on top of those used in AWD-LSTM of Merity et al. (2017) with little overhead.
Propose to observe implicit orders in datasets in a generative model viewpoint. The authors deal with the problem of implicit ordering in a dataset and the challenge of recovering it and propose to learn a distance-metric-free model that assumes a Markov chain as the generative mechanism of the data . The paper proposes “Generative Markov Networks” - a deep-learning-based approach to modeling sequences and discovering order in datasets. Proposes learning the order of an unordered data sample by learning a Markov chain.
Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model . Presents a seq2Tree model to translate a problem statement in natural language to the corresponding functional program in DSL, which has shown an improvement over the seq2seq baseline approach. This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples. The paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network.
This paper studies the discrimination and generalization properties of GANs when the discriminator set is a restricted function class like neural networks. Balances capacities of generator and discriminator classes in GANs by guaranteeing that induced IPMs are metrics and not pseudo metrics . This paper provides a mathematical analysis of the role of the size of the adversary/discriminator set in GANs .
All you need to train deep residual networks is a good initialization; normalization layers are not necessary. A method is presented for initialization and normalization of deep residual networks. This is based on observations of forward and backward explosion in such networks. The method performance is on par with the best results obtained by other networks with more explicit normalization. The authors propose a novel way to initialize residual networks, which is motivated by the need to avoid exploding/vanishing gradients. Proposes a new initialization method used to train very deep RedNets without using batch-norm.
This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN. Describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value. This paper considers the problem of improving sequence generation by learning better metrics, specifically the exposure bias problem .
Decompose the task of learning a generative model into learning disentangled latent factors for subsets of the data and then learning the joint over those latent factors. Locally Disentangled Factors for hierarchical latent variable generative model, which can be seen as a hierarchical variant of Adversarially Learned Inference . The paper investigates the potential of hierarchical latent variable models for generating images and image sequences and proposes to train several ALI models stacked on top of each other to create a hierarchical representation of the data. The paper aims to learn the hierarchies for training GAN in a hierarchical optimization schedule directly instead of being designed by a human .
We propose a joint model to incorporate visual knowledge in sentence representations . The paper proposes a method to use videos paired with captions to improve sentence embeddings . This submission proposes a model for sentence learning sentence representations that are grounded, based on associated video data. Proposes a method for improving text-based sentence embeddings through a joint multimodal framework.
