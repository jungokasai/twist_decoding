We devise adaptive loss scaling to improve mixed precision training that surpass the state-of-the-art results. Proposal for an adaptive loss scaling method during backpropagation for mix precision training where scale rate is decided automatically to reduce the underflow. The authors propose a method to train models in FP16 precision that adopts a more elaborate way to minimize underflow in every layer simultaneously and automatically.
We present a novel approach for learning to predict sets with unknown permutation and cardinality using feed-forward deep neural networks. A formulation to learn the distribution over unobservable permutation variables based on deep networks for the set prediction problem.
We compare object recognition performance on images that are downsampled uniformly and with three different foveation schemes.
We develop methods to train deep neural models that are both robust to adversarial perturbations and whose robustness is significantly easier to verify. The paper presents several ways to regularize plain ReLU networks to opimize the adversarial robustness, provable adversarial robustness, and the verification speed. This paper proposes methods to train robust neural networks that can be verified faster, using pruning methods to encourage weight sparsity and regularization to encourage ReLU stability.
Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. This paper addresses vulnerability to adversarial perturbations in BatchNorm, and proposes an alternative called RobustNorm, using min-max rescaling instead of normalization. This paper investigates the reason behind the vulnerability of BatchNorm and proposes Robust Normalization, a normalization method that achieves significantly better results under a variety of attack methods.
Our variational-recurrent imputation network (V-RIN) takes into account the correlated features, temporal dynamics, and further utilizes the uncertainty to alleviate the risk of biased missing values estimates. A missing data imputation network to incorporate correlation, temporal relationships, and data uncertainty for the problem of data sparsity in EHRs, which yields higher AUC on mortality rate classification tasks. The paper presented a method that combines VAE and uncertainty aware GRU for sequential missing data imputation and outcome prediction.
An adaptive method for fixed-point quantization of neural networks based on theoretical analysis rather than heuristics. Proposes a method for quantizing neural networks that allow weights to be quantized with different precision depending on their importance, taking into account the loss. The paper proposes a technique for quantizing the weights of a neural network with bit-depth/precision varying on a per-parameter basis.
Based on fuzzy set theory, we propose a model that given only the sizes of symmetric differences between pairs of multisets, learns representations of such multisets and their elements. This paper proposes a new task of set learning, predicting the size of the symmetric difference between multisets, and gives a method to solve the task based on fuzzy set theory.
This paper proposes a deep learning aided method to elicit credible samples from self-interested agents. The authors propose a sample elicitation framework for the problem of eliciting credible samples from agents for complex distributions, suggest that deep neural frameworks can be applied in this framework, and connect sample elicitation and f-GAN. This paper studies the sample elicitation problem, proposing a deep learning approach that relies on the dual expression of the f-divergence which writes as a maximum over a set of functions t.
Graph to Sequence Learning with Attention-Based Neural Networks . A graph2seq architecture that combines a graph encoder mixing GGNN and GCN components with an attentional sequence encoder, and that shows improvements over baselines. This work proposes an end-to-end graph encoder to sequence decoder models with an attention mechanism in between.
A zero-shot segmentation framework for 3D object part segmentation. Model the segmentation as a decision-making process and solve as a contextual bandit problem. A method for segmenting 3D point clouds of objects into component parts, focused on generalizing part groupings to novel object categories unseen during training, that shows strong performance relative to baselines. This paper proposes a method for part segmentation in object point clouds.
A new perspective on how to collect the correlation between nodes based on diffusion properties. A new diffusion operation for graph neural networks that does not require eigenvalue calculation and can propagate exponentially faster compared to traditional graph neural networks. The paper proposes to cope with the speed of diffusion problem by introducing ballistic walk.
We propose a weak supervision training pipeline based on the data programming framework for ranking tasks, in which we train a BERT-base ranking model and establish the new SOTA. The authors propose a combination of BERT and the weak supervision framework to tackle the problem of passage ranking, obtaining results better than the fully supervised state-of-the-art.
In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process. This paper analyzes the loss of neural networks in the Fourier domain and finds that DNNs tend to learn low-frequency components before high-frequency ones. The paper studies the training process of NNs through Fourier analysis, concluding that NNs learn low frequency components before high frequency components.
We propose a multi-resolution, hierarchically coupled encoder-decoder for graph-to-graph translation. A hierarchical graph-to-graph translation model to generate molecular graphs using chemical substructures as building blocks that is fully autoregressive and learns coherent multi-resolution representations, outperforming previous models. The authors present a hierarchical graph-to-graph translation method for generating novel organic molecules.
We utilize attention to restrict equivariant neural networks to the set or co-occurring transformations in data. This paper combines attention with group equivariance, specifically looking at the p4m group of rotations, translations, and flips, and derives a form of self-attention that doesn't destroy the equivariance property. The authors propose a self-attention mechanism for rotation-equivariant neural nets that improves classification performance over regular rotation-equivariant nets.
We train a GAN to generate and recover full-atom protein backbones , and we show that in select cases we can recover the generated proteins after sequence design and ab initio forward-folding. A generative model for protein backbone which uses a GAN, autoencoder-like network, and refinement process, and a set of qualitative evaluations suggesting positive results. This paper presents an end-to-end approach for generating protein backbones using generative adversarial networks.
Meta Learning for Few Shot learning assumes that training tasks and test tasks are drawn from the same distribution. What do you do if they are not? Meta Learning with task-level Domain Adaptation! This paper proposes a model combining unsupervised adversarial domain adaptation with prototypical networks that performs better than few-shot learning baselines on few-shot learning tasks with domain shift. The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup, demonstrating performance improvements in several experiments.
Divide, Conquer, and Combine is a new inference scheme that can be performed on the probabilistic programs with stochastic support, i.e. the very existence of variables is stochastic.
A community preserving node embedding algorithm that results in more effective detection of communities with a clustering on the embedded space .
An autoregressive deep learning model for generating diverse point clouds. An approach for generating 3D shapes as point clouds which considers the lexicographic ordering of points according to coordinates and trains a model to predict points in order. The paper introduces a generative model for point clouds using a pixel RNN-like auto-regressive model and an attention model to handle longer-range interactions.
Describes a series of explainability techniques applied to a simple neural network controller used for navigation. This paper provides insights and explanations for the problem of providing explanations for a multilayer perceptron used as an inverse controller for rover movement, and ideas on how to explain a black-box model.
We propose a self-monitoring agent for the Vision-and-Language Navigation task. A method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module, and performs well on standard benchmarks. This paper describes a model for vision-and-language navigation with a panoramic visual attention and an auxillary progress monitoring loss, giving state-of-the-art results.
event discovery to represent the history for the agent in RL . The authors study the problem of RL under partially observed settings, and propose a solution that uses a FFNN but provides a history representation, outperforming PPO. This paper proposes a new way to represent past history as input to an RL agent, showing to perform better than PPO and an RNN variant of PPO.
We show that autoregressive models can generate high fidelity images. An architecture utilizing decoder, size-upscaling decoder, and depth-upscaling decoder components to tackle the problem of learning long-range dependencies in images in order to obtain high fidelity images. This paper addresses the problem of generation to high fidelity images, successfully showing convincing Imagenet samples with 128x128 resolution for a likelihood density model.
A deep hierarchical state-space model in which the state transitions of correlated objects are coordinated by graph neural networks. A hierarchical latent variable model of sequential dynamic processes of multiple objects when each object exhibits significant stochasticity. The paper presents a relational state-space model that simulates the joint state transitions of correlated objects which are hierarchically coordinated in a graph structure.
We introduce a new inductive bias that integrates tree structures in recurrent neural networks. This paper proposes ON-LSTM, a new RNN unit that integrates the latent tree structure into recurrent models and that has good results on language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.
Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies. The authors show that elimination singularities and overlap singularities impede learning in deep neural networks, and demonstrate that skip connections can reduce the prevalence of these singularities, speeding up learning. Paper examines the use of skip connections in deep networks as a way of alleviating singularities in the Hessian matrix during training.
Learning state representations which capture factors necessary for control . An approach to representation learning in the context of reinforcement learning that distinguishes two stages functionally in terms of the actions that are needed to reach them. The paper presents a method to learn representations where proximity in euclidean distance represents states that are achieved by similar policies.
We study the behavior of a CNN as it masters new tasks while preserving mastery for previously learned tasks .
Morty refits pretrained word embeddings to either: (a) improve overall embedding performance (for Multi-task settings) or improve Single-task performance, while requiring only minimal effort.
Selectively augmenting difficult to classify points results in efficient training. The authors study the problem of identifying subsampling strategies for data augmentation and propose strategies based on model influence and loss as well as empirical benchmarking of the proposed methods. The authors propose to use influence or loss-based methods to select a subset of points to use in augmenting data sets for training models where the loss is additive over data points.
A deep generative model for organic molecules that first generates reactant building blocks before combining these using a reaction predictor. A molecular generative model that generates molecules via a two-step process that provides synthesis routes of the generated molecules, allowing users to examine the synthetic accessibility of generated compounds.
Improve the robustness and energy efficiency of a deep neural network using the hidden representations. This paper aims to reduce the misclassifications of deep neural networks in an energy efficient way by adding Relevant feature based Auxiliary Cells after one or more hidden layers to decide whether to end classification early.
Understanding the structure of knowledge graph representation using insight from word embeddings. This paper attempts to understand the latent structure underlying knowledge graph embedding methods, and demonstrates that a model's ability to represent a relation type depends on the model architecture's limitations with respect to relation conditions. This paper proposes a detailed study on the explainability of link prediction (LP) models by utilizing a recent interpretation of word embeddings to provide a better understanding of LPs' model performance.
A novel self-attention mechanism for multivariate, geo-tagged time series imputation. This paper proposes the problem of applying the transformer network to spatiotemporal data in a compuationally efficient way, and investigates ways of implementing 3D attention. This paper empirically studies the effectiveness of transformer models for time series data imputation across dimensions of the input.
We designed and tested a REDNET (ResNet Encoder-Decoder) with 8 skip connections to remove noise from documents, including blurring and watermarks, resulting in a high performance deep network for document image cleanup.
We identify a family of defense techniques and show that both deterministic lossy compression and randomized perturbations to the input lead to similar gains in robustness. This paper discusses ways of destabilizing a given adversarial attack, what makes adversarial images non-robust, and if it's possible for attackers to use a universal model of perturbations to make their adversarial examples robust against such perturbations. The paper studies the robustness of adversarial attacks to transformations of their input.
We provide a method to benchmark optimizers that is cognizant to the hyperparameter tuning process. Introduction of a novel metric to capture the tunability of an optimizer, and a comprehensive empirical comparison of deep learning optimizers under different amounts of hyper-parameter tuning. This paper introduces a simple measure of tunability that allows to compare optimizers under resource constraints, finding that tuning Adam optimizers' learning rate is easiest to find well-performing hyperparameter configurations.
We introduce a semi-supervised deep neural network to approximate the solution of the phase problem in electron microscopy .
Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features. This paper extends SGNS with an architectural change from a bag-of-words model to a feedforward model, and contributes a new form of regularization by tying a subset of layers between different associated networks. A method to use non-linear combination of context vectors for learning vector representation of words, where the main idea is to replace each word embedding by a neural network.
Using a 10s window of fMRI signals, our GCN model identified 21 different task conditions from HCP dataset with a test accuracy of 89%.
Efficiently inducing low-rank deep neural networks via SVD training with sparse singular values and orthogonal singular vectors. This paper introduces an approach to network compression by encouraging the weight matrix in each layer to have a low rank and explicitly factorizing the weight matrices into an SVD-like factorization for treatment as new parameters. Proposal to parametrize each layer of a deep neural network, before training, with a low-rank matrix decomposition, accordingly replace convolutions with two consecutive convolutions, and then train the decomposed method.
We propose a few-shot learning model that is tailored specifically for regression tasks . This paper proposes a novel shot-learning method for small sample regression problems. A method that learns a regression model with a few samples and outperforms other methods.
We present a novel approach for detecting out-of-distribution pixels in semantic segmentation. This paper addresses out-of-distribution detection for helping the segmentation process, and proposes an approach of training a binary classifier that distinguishes image patches from a known set of classes from those of an unknown. This paper aims to detect out-of-distribution pixels for semantic segmentation, and this work utilizes data from other domains to detect undetermined classes to model uncertainty better.
Accurate, Fast and Automated Kernel-Wise Neural Network Quantization with Mixed Precision using Hierarchical Deep Reinforcement Learning . A method for quantizing neural network weights and activations that uses deep reinforcement learning to select bitwidth for individual kernels in a layer and that achieves better performance, or latency, than prior approaches. This paper proposes to automatically search quantization schemes for each kernel in the neural network, using hierarchial RL to guide the search.
Gaggle, an interactive visual analytic system to help users interactively navigate model space for classification and ranking tasks. A new visual analytic system which aims to enable non-expert users to interactively navigate a model space by using a demonstration-based approach. A visual analytics system that helps novice analysts navigate model space in performing classification and ranking tasks.
We propose a novel attention networks with the hybird encoder to solve the text representation issue of Chinese text classification, especially the language phenomena about pronunciations such as the polyphone and the homophone. This paper proposes an attention-based model consisting of the word encoder and Pinyin encoder for the Chinese text classification task, and extends the architecture for the Pinyin character encoder. Proposal for an attention network where both word and pinyin are considered for Chinese representation, with improved results shown in several datasets for text classification.
multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention. A new sampling-based approach for inference in latent variable models that applies to multi-modal imitation learning and works better than deterministic neural networks and stochastic neural networks for a real visual robotics task. This paper shows how to learn several modalities using imitation learning from visual data using stochastic Neural Networks, and a method for learning from demonstrations where several modalities of the same task are given.
A novel approach to construct hierarchical explanations for text classification by detecting feature interactions. A novel method for providing explanations for predicitions made by text classifiers that outperforms baselines on word level importance scores, and a new metric, cohesion loss, to evaluate span-level importance. An interpretation method based on feature interactions and feature importance score as compared to independent feature contributions.
We make convolutional layers run faster by dynamically boosting and suppressing channels in feature computation. A feature boosting and suppression method for dynamic channel pruning that predicts the importance of each channel and then uses an affine function to amplify/suppress channel importance. Proposal for a channel pruning method for dynamically selecting channels during testing.
Neural networks can be pre-defined to have sparse connectivity without performance degradation. This paper examines sparse connection patterns in upper layers of convolutional image classification networks, and introduces heuristics for distributing connections among windows/groups and a measure called scatter to construct connectivity masks. Proposal to reduce the number of parameters learned by a deep network by setting up sparse connection weights in classiication layers, and introduction of a concept of "scatter."
We provide a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness of deep learning models. This paper presents an evaluation of different kinds of classification models under various adversarial attack methods. A large-scale empirical study comparing different adversarial attack and defense techniques, and use of accuracy vs. perturbation budget and accuracy vs. attack strength curves to evaluate attacks and defenses.
We propose a modification to traditional Artificial Neural Networks motivated by the biology of neurons to enable the shape of the activation function to be context dependent. A method to scale the activations of a layer of neurons in an ANN depending on inputs to that layer that reports improvements above the baselines. Introduction of an architectural change for basic neurons in a neural network, and the idea to multiply neuron linear combination output by a modulator prior to feeding it into the activation function.
We identify the forgetting problem in fine-tuning of pre-trained NLG models, and propose the mix-review strategy to address it. This paper analyzes the forgetting problem in the pretraining-finetuning framework from the perspective of context sensitivity and knowledge transfer, and proposes a fine-tuning strategy which outperforms the weight decay method. Study of the forgetting problem in the pretrain-finetune framework, specifically in dialogue response generation tasks, and proposal of a mix-review strategy to alleviate the forgetting issue.
Improved modeling of complex systems uses hybrid neural/domain model composition, new decorrelation loss functions and extrapolative test sets . This paper conducts experiments to compare the extrapolative predictions of various hybrid models which compose physical models, neural networks and stochastic models, and tackles the challenge of unmodeled dynamics being a bottleneck. This paper presents approaches for combining neural network with non-NN models to predict behavior of complex physical systems.
We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition. This paper discusses zero shot generalization into new environments, and proposes an approach with results on Grid-World, Super Mario Bros, and 3D Robotics. A method aiming to learn task-agnostic priors for zero-shot generalization, with the idea to employ a modeling approach on top of the model-based RL framework.
Analyze the underlying mechanisms of variance collapse of SVGD in high dimensions.
Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior . The authors suggest that statistical mechanics ideas will help to understand generalization properties of deep neural networks, and give an approach that provides strong qualitative descriptions of empirical results regarding deep neural networks and learning algorithms. A set of ideas related to theoretical understanding generalization properties of multilayer neural networks, and a qualitative analogy between behaviours in deep learning and results from quantitative statistical physics analysis of single and two-layer neural networks.
We present doubly sparse softmax, the sparse mixture of sparse of sparse experts, to improve the efficiency for softmax inference through exploiting the two-level overlapping hierarchy. This paper proposes a fast approximation to the softmax computation when the number of classes is very large. This paper proposes a sparse mixture of sparse experts that learns a two-level class hierarchy for efficient softmax inference.
We explore using passively collected eye-tracking data to reduce the amount of labeled data needed during training. A method to use gaze information to reduce the sample complexity of a model and the needed labeling effort to get a target performance, with improved results in middle-sized samples and harder tasks. A method to incorporate gaze signals into standard CNNs for image classification, adding a loss function term based in the difference between the model's Class Activation Map and the map constructed from eye tracking information.
We apply loss correction to graph neural networks to train a more robust to noise model. This paper introduces loss correction for Graph Neural Networks to deal with symmetric graph label noise, focused on a graph classification task. This paper proposes the use of a noise correction loss in the context of graph neural networks to deal with noisy labels.
We use graph co-attention in a paired graph training system for graph classification and regression. This paper injects a multi-head co-attention mechanism in GCN that allows one drug to attend to another drug during drug side effect prediction. A method to extend graph-based learning with a co-attentional layer, which outperforms other previous ones on a pairwise graph classification task.
Image captioning as a conditional GAN training with novel architectures, also study two discrete GAN training methods. An improved GAN model for image captioning that proposes a context-aware LSTM captioner, introduces a stronger co-attentive discriminator with better performance, and uses SCST for GAN training.
Exploit curvature to make MCMC methods converge faster than state of the art.
Keras for infinite neural networks.
We propose a novel loss function that achieves state-of-the-art results in out-of-distribution detection with Outlier Exposure both on image and text classiﬁcation tasks. This paper tackles the problems of out-of-distribution detection and model calibration by adapting the loss function of the Outlier Exposure technique, with results demonstrating increased performance over OE on vision and text benchmarks and improved model calibration. Proposal for a new loss function to train the network with Outlier Exposure which leads to better OOD detection compared to simple loss functions using KL divergence.
Task agnostic pre-training can shape RNN's attractor landscape, and form diverse inductive bias for different navigation tasks . This paper studies the internal representations of recurrent neural networks trained on navigation tasks, and finds that RNNs pre-trained to use path integration contain 2D continuous attractors while RNNs pre-trained for landmark memory contain discrete attractors. This paper explores how pre-training recurrent networks on different navigational objectives confers different benefits for solving downstream tasks, and shows how different pretraining manifests as different dynamical structures in the networks after pre-training.
Neural Network Verification for Temporal Properties and Sequence Generation Models . This paper extends interval bound propagation to recurrent computation and auto-regressive models, introduces and extends Signal Temporal Logic for specifying temporal contraints, and provides proof that STL with bound propagation can ensure neural models conform to temporal specification. A way to train time-series regressors verifiably with respect to a set of rules defined by signal temporal logic, and work in deriving bound propagation rules for the STL language.
We propose a universal neural network solution to derive effective NN architectures for tabular data automatically. A new Neural Network training procedure, designed for tabular data, that seeks to leverage feature clusters extracted from GBDTs. Proposal for a hybrid machine learning algorithm using Gradient Boosted Decision Trees and Deep Neural Networks, with intended research direction on tabular data.
Probabilistic Rule Learning system using Lifted Inference . A model for probabilistic rule learning to automate the completion of probabilistic databases that uses AMIE+ and lifted inference to help computational efficiency.
We propose the first non-autoregressive neural model for Dialogue State Tracking (DST), achieving the SOTA accuracy (49.04%) on MultiWOZ2.1 benchmark, and reducing inference latency by an order of magnitude. A new model for the DST task that reduces inference time complexity with a non-autoregressive decoder, obtains competitive DST accuracy, and shows improvements over other baselines. Proposal for a model that is capable of tracking dialogue states in a non-recursive fashion.
A novel network architecture to perform Deep 3D Zoom or close-ups. A method for creating a "zoomed image" for a given input image,and a novel back re-projection reconstruction loss that allows the network to learn underlying 3D structure and maintain a natural appearance. An algorithm for synthesizing 3D-zoom behavior when the camera is moving forward, a network structure incorporating disparity estimation in a GANs framework to synthesize novel views, and a proposed new computer vision task.
A quantitative refinement of the universal approximation theorem via an algebraic approach. The authors derive the universal approximation property proofs algebraically and assert that the results are general to other kinds of neural networks and similar learners. A new proof of Leshno's version of the universal approximation property for neural networks, and new insights into the universal approximation property.
Modular framework for document classification and data aggregation technique for making the framework robust to various distortion, and noise and focus only on the important words. The authors consider training a RNN-based text classification where there is a resource restriction on test-time prediction, and provide an approach using a masking mechanism to reduce words/phrases/sentences used in prediction followed by a classifier to handle those components.
Allowing partial channel connection in super-networks to regularize and accelerate differentiable architecture search . An extension of the neural architecture search method DARTS that addresses its shortcoming of immense memory cost by using a random subset of channels and a method to normalize edges. This paper proposes to improve DARTS in terms of training efficiency, from large memory and computing overheads, and proposes a partially-connected DARTS with partial channel connection and edge normalization.
Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. This paper studies a multiagent dialog task in which the learning agent aims to generate natural language actions that elicit a particular action from the other agent, and shows RL-agents can achieve higher task completion levels than imitation learning baselines. This paper explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game and observes that the RL approaches outperform supervised learning models.
A new partially policy-agnostic method for infinite-horizon off-policy policy evalution with multiple known or unknown behavior policies. An estimated mixture policy which takes ideas from off-policy policy evaluation infinite horizon estimators and regression importance sampling for importance weight, and extends them to many policies and unknown policies. An algorithm to solve infinite horizon off policy evaluation with multiple behavior policies by estimating a mixed policy under regression, and theoretical proof that an estimated policy ratio can reduce variance.
We introduce a more efficient neural architecture for amortized inference, which combines continuous and conditional normalizing flows using a principled choice of sparsity structure.
We show that ENAS with ES-optimization in RL is highly scalable, and use it to compactify neural network policies by weight sharing. The authors construct reinforcement learning policies with very few parameters by compressing a feed-forward neural network, forcing it to share weights, and using a reinforcement learning method to learn the mapping of shared weights. This paper combines ideas from ENAS and ES methods for optimisation, and introduces the chromatic network architecture, which partitions weights of the RL network into tied sub-groups.
We introduce Deep SAD, a deep method for general semi-supervised anomaly detection that especially takes advantage of labeled anomalies. A new method to find anomaly data, when some labeled anomalies are given, that applies information theory-derived loss based on normal data usuallly having lower entropy than abnormal data. Proposal for an abnormal detection framework under settings where unlabeled data, labeled positive data, and labeled negative data are available, and proposal to approach semi-supervised AD from an information theoretic perspective.
This paper analyzes training dynamics and critical points of training deep ReLU network via SGD in the teacher-student setting. Study of over-parametrization in student-teacher multilayer ReLU networks, a theoretical part about SGD critical points for the teacher-student setting, and a heuristic and empirical part on dynamics of the SDG algorithm as a function of teacher networks.
Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets. The authors study the convergence of gradient descent in training deep linear residual networks, and establish a global convergence of GD/SGD and linear convergence rates of SG/SGD. Study of convergence properties of GD and SGD on deep linear resnets, and proof that under certain conditions on the input and output transformations and with zero initialization, GD and SGD converges to global minima.
We analyze the training process for Deep Networks and show that they start from rapidly learning shallow classifiable examples and slowly generalize to harder data points.
Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation. A method for learning deep latent-variable MRF with an optimization objective that utilizes Bethe free energy, that also solves the underlying constraints of Bethe free energy optimizations. An objective for learning latent variable MRFs based on Bethe free energy and amortized inference, different from optimizing the standard ELBO.
A general framework for explanation generation using Logic. This paper researches explanation generation from a KR point of view and conducts experiments measuring explanation size and runtime on random formulas and formulas from a Blocksworld instance. This paper provides a perspective on explanations between two knowledge bases, and runs parallel to work on model reconciliation in planning literature.
Deep and narrow neural networks will converge to erroneous mean or median states of the target function depending on the loss with high probability. This paper studies failure modes of deep and narrow networks, focusing on as small as possible models for which the undesired behavior occurs. This paper shows that the training of deep ReLU neural networks will converge to a constant classifier with high probability over random initialization if hidden layer widths are too small.
We propose MMA training to directly maximize input space margin in order to improve adversarial robustness primarily by removing the requirement of specifying a fixed distortion bound. An adaptive margin-based adversarial training approach to train robust DNNs, by maximizing the shortest margin of inputs to the decision boundary, that makes adversarial training with large perturbation possible. A method for robust learning against adversarial attacks where the input space margin is directly maximized and a softmax variant of the max-margin is introduced.
We propose a method for anomaly detection with GANs by searching the generator's latent space for good sample representations. The authors propose using GAN for anomaly detection, a gradient-descent based method to iteratively update latent representations, and a novel parameter update to the generators. A GAN based approach to doing anomaly detection for image data where the generator's latent space is explored to find a representation for a test image.
The transient behavior of gradient-based MCMC and variational inference algorithms is more similar than one might think, calling into question the claim that variational inference is faster than MCMC.
A Composition-based Graph Convolutional framework for multi-relational graphs. The authors develop GCN on multi-relational graphs and propose CompGCN, which leverages insights from knowledge graph embeddings and learns node and relation representations to alleviate the problem of over-parameterization. This paper introduces a GCN framework for multi-relational graphs and generalizes several existing approaches to Knowledge Graph embedding into one framework.
We fully quantize the Transformer to 8-bit and improve translation quality compared to the full precision model. An 8-bit quantization method to quantize the machine translation model Transformer, proposing to use uniform min-max quantization during inference and bucketing weigts before quantization to reduce quantization error. A method for reducing the required memory space by a quantization technique, focused on reducing it for Transformer architecture.
Latent Embedding Optimization (LEO) is a novel gradient-based meta-learner with state-of-the-art performance on the challenging 5-way 1-shot and 5-shot miniImageNet and tieredImageNet classification tasks. A new meta-learning framework that learns data-dependent latent space, performs fast adaptation in the latent space, is effective for few-shot learning, has task-dependent initialization for adaptation, and works well for multimodal task distribution. This paper proposes a latent embedding optimization method for meta-learning, and claims the contribution is to decouple optimization-based meta-learning techniques from high-dimensional space of model parameters.
Relational inductive biases improve out-of-distribution generalization capacities in model-free reinforcement learning agents . A shared relational network architecture for parameterizing the actor and critic network, focused on distributed advantage actor-critic algorithms, that enhances model-free deep reinforcement techniques with relational knowledge about the environment so agents can learn interpretable state representations. A quantitative and qualitative analysis and evaluation of the self-attention mechanism combined with relation network in the context of model-free RL.
We propose a simple generative model for unsupervised image translation and saliency detection.
We define a concept of layerwise model-parallel deep neural networks, for which layers operate in parallel, and provide a toolbox to design, train, evaluate, and on-line interact with these networks. A GPU-accelerated toolbox for parallel neuron updating, written in Theano, that supports different update orders in recurrent networks and networks with connections that skip layers. A new toolbox for deep neural networks learning and evaluation, and proposal for a paradigm switch from layerwise-sequential networks to layer-wise parallel networks.
An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability . The authors formulate training NNs as finding an optimal controller for a discrete dynamical system, allowing them to use method of successive approximations to train a NN in a way to be more robust to adversarial attacks. This paper uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness.
We propose a simple yet effective reweighting scheme for GCNs, theoretically supported by the mean field theory. A method, known as DrGCN, for reweighting the different dimensions of the node representations in graph convolutional networks by reducing variance between dimensions.
Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark. A sequential latent variable model for knowledge selection in dialogue generation that extends the posterior attention model to the latent knowledge selection problem and achieves higher performances than previous state-of-the-art models. A novel architecture for selecting knowledge-grounded multi-turn dialogue that yields state of the art on relevant benchmarks datasets, and scores higher in human evaluations.
We propose a meta-learning method which efficiently amortizes hierarchical variational inference across training episodes. An adaptation to MAML-type models that accounts for posterior uncertainty in task specific latent variables by employing variational inference for task-specific parameters in a hierarchical Bayesian view of MAML. The authors consider meta-learning to learn a prior over neural network weights, done via amortized variational inference.
Representation/knowledge distillation by maximizing mutual information between teacher and student . This paper combines a contrastive objective measuring the mutual information between the representations learned by teacher and student networks for model distillation, and proposes a model with improvement over existing alternatives on distillation tasks.
Networks that learn with feedback connections and local plasticity rules can be optimized for using meta learning.
CNNs with biologically-inspired lateral connections learned in an unsupervised manner are more robust to noisy inputs.
This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.
We study the certified robustness for top-k predictions via randomized smoothing under Gaussian noise and derive a tight robustness bound in L_2 norm. This paper extends work on deducing a certified radius using randomized smoothing, and shows the radius at which a smoothed classifier under Gaussian perturbations is certified for the top k predictions. This paper builds upon the random smoothing technique for top-1 prediction, and aims to provide certification on top-k predictions.
We present structured priors for unsupervised learning of disentangled representations in VAEs that significantly mitigate the trade-off between disentanglement and reconstruction loss. A general framework to use the family of L^p-nested distributions as the prior for the code vector of VAE, demonstrating a higher MIG. The authors point out issues in current VAE approaches and provide a new perspective on the tradeoff between reconstruction and orthogonalization for VAE, beta-VAE, and beta-TCVAE.
We generalize residual blocks to tandem blocks, which use arbitrary linear maps instead of shortcuts, and improve performance over ResNets. This paper performs an analysis of shortcut connections in ResNet-like architectures, and proposes to substitute the identity shortcuts with an alternative convolutional one referred to as tandem block. This paper investigates the effect of replacing identity skip connections with trainable convolutional skip connections in ResNet and finds that performance improves.
We present a new framework for adapting Adam-typed methods, namely AdamT, to include the trend information when updating the parameters with the adaptive step size and gradients. A new type of Adam variant that uses Holt's linear method to compute the smoothed first order and second order momentum instead of using exponential weighted average.
A method to explain a classifier, by generating visual perturbation of an image by exaggerating  or diminishing the semantic features that the classifier associates with a target label. A model that when given a query input to a black-box, aims to explain the outcome by providing plausible and progressive variations to the query that can result in a change to the output. A method for explaining the output of black box classification of images, that generates gradual perturbation of outputs in response to gradually perturbed input queries.
We present an influence-directed approach to constructing explanations for the behavior of deep convolutional networks, and show how it can be used to answer a broad set of questions that could not be addressed by prior work. A way to measure influence that satisfies certain axioms, and a notion of influence that can be used to identify what input part is most influential for the output of a neuron in a deep neural network. This paper proposes to measure the influence of single neurons with regard to a quantity of interest represented by another neuron.
We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible. A technique for exploiting prior knowledge to learn embedding representations for new words with minimal data.
A memory architecture that support inferential reasoning. This paper proposes changes to the End2End Memory Network architecture, introduces a new Paired Associative Inference task that most existing models struggle to solve, and shows that their proposed architecture solves the task better. A new task (paired associate inference) drawn from cognitive psychology, and proposal for a new memory architecture with features that allow for better performance on the paired associate task.
Depthwise separable convolutions improve neural machine translation: the more separable the better. This paper proposes to use depthwise separable convolution layers in a fully convolutional neural machine translation model, and introduces a new super-separable convolution layer which further reduces computational cost.
Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence. This paper proposes a useful expression of the class of f-divergences, investigates theoretical properties of popular f-divergences from newly developed tools, and investigates GANs with the non-saturating training scheme.
We introduce a novel text representation method which enables image classifiers to be applied to text classification problems, and apply the method to inventor name disambiguation. A method to map a pair of textual information into a 2D RGB image that can be fed to 2D convoutional neural networks (image classifiers). The authors consider the problem of names disambiguisation for patent names inventors and propose to build an image page representation of the two name strings to compare and to apply an image classifier.
We proposed "Difference-Seeking Generative Adversarial Network" (DSGAN) model to learn the target distribution which is hard to collect training data. This paper presents DS-GAN, which aims to learn the difference between any two distributions whose samples are difficult or impossible to collect, and shows its effectiveness on semi-supervised learning and adversarial training tasks. This paper considers the problem of learning a GAN to capture a target distribution with only very few training samples from that distribution available.
A general method that improves the image translation performance of GAN framework by using an attention embedded discriminator . A feedback mechanism in the GAN framework which improves the quality of generated images in image-to-image translation, and whose discriminator outputs a map indicating where the generator should focus to make its results more convincing. Proposal for a GAN with an attention-based discriminator for I2I translation which provides the probability of real/fake and an attention map which reflects salience for image generation.
We propose a new dataset to investigate the entailment problem under semi-structured table as premise . This paper proposes a new dataset for table-based fact verification and introduces methods for the task. The authors propose the problem of fact verification with semi-structured data sources such as tables, create a new dataset, and evaluate baseline models with variations.
We develop a deep graph matching architecture which refines initial correspondences in order to reach neighborhood consensus. A framework for answering graph matching questions consisting of local node embeddings with a message passing refinement step. A two-stage GNN-based architecture to establish correspondences between two graphs that performs well on real-world tasks of image matching and knowledge graph entity alignment.
This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. This paper investigates the approximation properties of a family of neural networks designed to address multi-instance learning problems, and shows that results for standard one layer architectures extend to these models. This paper generalizes the universal approximation theorem to real functions on the space of measures.
A new framework for context-dependent and context-free explanations of predictions . The authors extend the linear local attribution method LIME for interpreting black box models, and propose a method to discern between context-dependent and context-free interactions. A method that can provide hierarchical explanations for a model, including both context-dependent and context-free explanations by a local interpretation algorithm.
Finetuning after quantization matches or exceeds full-precision state-of-the-art networks at both 8- and 4-bit quantization. This paper proposes to improve the performance of low-precision models by doing quantization on pre-trained models, using large batches size, and using proper learning rate annealing with longer training time. A method for low bit quantization to enable inference on efficient hardware that achieves full accuracy on ResNet50 with 4-bit weights and activations, based on observations that fine-tuning at low precision introduces noise in the gradient.
Two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which directly quantify how strongly information encoded in neural activation patterns corresponds to information represented by symbolic structures.
This paper introduces a framework for data-efficient representation learning by adaptive sampling in latent space. A method for sequential and adaptive selection of training examples to be presented to the training algorithm, where selection happens in the latent space based on choosing samples in the direction of the gradient of the loss. A method to efficiently select hard samples during neural network training, achieved via a variational auto-encoder that encodes samples into a latent space.
An adversarial training-based method for disentangling two complementary sets of variations in a dataset where only one of them is labelled, tested on style vs. content in anime illustrations. An image generation method combining conditional GANs and conditional VAEs that generates high fidelity anime images with various styles from various artists. Proposal for a method to learn disentangled style (artist) and content representations in anime.
We introduce a smoothness regularization for convolutional kernels of CNN that can help improve adversarial robustness and lead to perceptually-aligned gradients . This paper proposes a new regularization scheme that encourages convolutional kernels to be smoother, arguing that reducing neural network reliance on high-frequency components helps robustness against adversarial examples. The authors propose a method for learning smoother convolutional kernels, specifically, a regularizer penalizing large changes between consecutive pixels of the kernel with the intuition of penalizing the use of high-frequency input components.
We investigate the large-sample behaviors of the Q-value estimates and proposed an efficient exploration strategy that relies on estimating the relative discrepancies among the Q estimates.
We train word embeddings based on entailment instead of similarity, successfully predicting lexical entailment. The paper presents a word embedding algorithm for lexical entailment which follows the work of Henderson and Popa (ACL, 2016).
Unsupervised learning for reinforcement learning using an automatic curriculum of self-play . A new formulation for exploring the environment in an unsupervised way to aid a specific task later, where one agent proposes increasingly difficult tasks and the learning agent tries to accomplish them. A self-play model where one agent learns to propose tasks that are easy for them but difficult for an opponent, creating a moving target of self-play objectives and learning curriculum.
Exploiting rich strucural details in graph-structued data via adaptive "strucutral fingerprints'' A graph structure based methodology to augment the attention mechanism of graph neural networks, with the main idea to explore interactions between different types of nodes of the local neighborhood of a root node. This paper extends the idea of self-attention in graph NNs, which is typically based on feature similarity between nodes, to include structural similarity.
We propose a scalable Bayesian Reinforcement Learning algorithm that learns a Bayesian correction over an ensemble of clairvoyant experts to solve problems with complex latent rewards and dynamics. This paper considers Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs) by making decisions with experts. In this paper, the authors motivate and propose a learning algorithm, called Bayesian Residual Policy Optimization (BRPO), for Bayesian reinforcement learning problems.
We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks. This work considers optimizing a two-layer over-parameterized ReLU network with the squared loss and given a data set with arbituary labels. This paper studies one hidden layer neural networks with square loss, where they show that in over-parameterized setting, random initialization and gradient descent gets to zero loss.
To analyze inverse problems with Invertible Neural Networks . The author proposes to use invertible networks to solve ambiguous inverse problems and suggest to not only train the forward model, but also the inverse model with an MMD critic. The research paper proposes an invertible network with observations for posterior probability of complex input distributions with a theoretical valid bidirectional training scheme.
Performance metrics are incomplete specifications; the ends don't always justify the means. The authors show how meta-learning reveals the hidden incentives for distributional shift and propose an approach based on swapping learners between environments to reduce self introduced distributional shift. The paper generalizes the inherent incentive for the learner to win by making the task easier in meta-learning to a larger class of problems.
We propose an anomaly-detection approach that combines modeling the foreground class via multiple local densities with adversarial training. The paper proposes a technique to make generative models more robust by making them consistent with the local density.
We propose a GAN variant which learns to generate point clouds. Different studies have been explores, including tighter Wasserstein distance estimate,  conditional generation, generalization to unseen point clouds and image to point cloud. This paper proposes using GAN to generate 3D point cloud and introduces a sandwiching objective, averaging the upper and lower bound of Wasserstein distance between distributions. This paper proposes a new generative model for unordered data, with a particular application to point clouds, which includes an inference method and a novel objective function.
The paper presents a novel approach for attentional mechanisms that can benefit a range of tasks such as machine translation and image captioning. This paper extends the current attention models from word level to the combination of adjacent words, by applying the models to items made from merged adjacent words.
We identify a phenomenon, neural brainwashing, and introduce a statistically-justified weight plasticity loss to overcome this. This paper discusses the phenomena of “neural brainwashing”, which refers to that the performance of one model is affected via another model sharing model parameters.
This paper introduces Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative evaluation of representation learning. This paper discusses the problem of evaluating and diagnosing the represenatations learnt using a generative model. Authors present a set of criteria to categorize MNISt digists and a set of interesting perturbations to modify MNIST dataset.
structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control . The paper introduces visual abstractions that are used for reinforcement learning, where an algorithm learns to "control" each abstraction as well as select the options to achieve the overall task.
A new policy gradient algorithm designed to approach black-box combinatorial optimization problems. The algorithm relies only on function evaluations, and returns locally optimal solutions with high probability. The paper proposes an approach to construct surrogate objectives for the application of policy gradient methods to combinatorial optimization with the goal of reducing the need of hyper-parameter tuning. The paper propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution.
Fast, calibrated uncertainty estimation for neural networks without sampling . This paper proposes a novel approach to estimate the confidence of predictions in a regression setting, opening the door to online applications with fully integrated uncertainty estimates. This paper proposed deep evidential regression, a method for training neural networks to not only estimate the output but also the associated evidence in support of that output.
We propose a new algorithm that quickly finds winning tickets in neural networks. This paper proposes a novel objective function that can be used to jointly optimize a classification objective while encouraging sparsification in a network that performs with high accuracy. This work propose a new iterative pruning methods named Continuous Sparsification, which continuously prunes the current weight until it reaches the target ratio.
Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule . This work presents a technique for tuning the learning rate for Neural Network training when under a fixed number of epochs. This paper analyzed which learning rate schedule should be used when the number of iteration is limited using an introduced concept of BAS (Budget-Aware Schedule).
We conduct exploration using intrinsic rewards that are based on a weighted distance of nearest neighbors in representational space. This paper proposes a method for efficient exploration in tabular MDPs as well as a simple control environment, using deterministic encoders to learn a low dimensional representation of the environment dynamics. This paper proposes a method of sample-efficient exploration for RL agent using a combination of model-based and model-free approaches with a novelty metric.
Robustness performance of PGD trained models are sensitive to semantics-preserving transformation of image datasets, which implies the trickiness of evaluation of robust learning algorithms in practice.
We propose ranking policy gradient that learns the optimal rank of actions to maximize return. We propose a general off-policy learning framework with the properties of optimality preserving, variance reduction, and sample-efficiency. This paper proposes to reparameterize the policy using a form of ranking to convert the RL problem into a supervised learning problem. This paper presents a new view on policy gradient methods from the perspective of ranking.
Combining classification and image retrieval in a neural network architecture, we obtain an improvement for both tasks. This paper proposes a unified embedding for image classification and instance retrieval to enhance the performance for both tasks. The paper proposes to jointy train a deep neural net for image classification, instance, and copy recognition.
We investigate mapping the hyponymy relation of wordnet to feature vectors . This paper studies how hyponymy between words can be mapped to feature representations. This paper explores the notion of hyponymy in word vector representations and describes a method of organizing WordNet relations into a tree structure to define hyponymy.
We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing. This paper proposes to bring together multiple inductive biases that hope to correct for inconsistencies in sequence decoding and proposes to optimize for the parameters of a pre-defined combination of various sub-objectives. This paper combines RNN language model with several discriminatively trained models to improve the language generation. This paper proposes to improve RNN language model generation using augmented objectives inspired by Grice's maxims of communication.
Scalable and low communication load balancing solution for heterogeneous-server multi-dispatcher systems with strong theoretical guarantees and promising empirical results.
A quantitative measure to predict the performances of deep neural network models. The paper proposes a novel quantity that counts the number of path in the neural network which is predictive of the performance of neural networks with the same number of parameters. The paper presents a method for counting paths in deep neural networks that arguably can be used to measure the performance of the network.
This paper presents a rigorous study of why practically used learning rate schedules (for a given computational budget) offer significant advantages even though these schemes are not advocated by the classical theory of Stochastic Approximation. This paper presents a theoretical study of different learning rate schedules that resulted in statistical minimax lower bounds for both polynomial and constant-and-cut schemes. The paper studies the effect of learning-rate choices for stochastic optimization, focusing on least-mean-squares with decaying stepsizes .
We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems. Proposes methods, which can be seen as modifications of Value Iteration Networks (VIN), with some improvements aimed at improving sample efficiency and generalization to large environment sizes. The paper presents an extension of the original value iteration networks (VIN) by considering a state-dependent transition function.
learning better domain embeddings via lifelong learning and meta-learning . Presents a lifelong learning method for learning word embeddings. This paper proposes an approach to learn embeddings in new domains and significantly beats the baseline on an aspect extraction task.
we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance. This paper proposes a regularization-based pruning method to incrementally assign different regularization factors to different weight groups based on their relative importance.
Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.
We show that oversubscription planning tasks can be solved using A* and introduce novel bound-sensitive heuristics for oversubscription planning tasks. Presents an approach to solve oversubscription planning (OSP) tasks optimally by using a translation to classical planning with multiple cost functions. The paper proposes modifications to admissible heuristics to make them better informed in a multi-criteria setting where.
We develop meta-learning methods for adversarially robust few-shot learning. This paper presents a method that enhances the robustness of few-shot learning by introducing adversarial query data attack in the inner-task fine-tuning phase of a meta-learning algorithm. The authors of this paper propose a novel approach for training a robust few-shot model.
We find that pooling alone does not determine deformation stability in CNNs and that filter smoothness plays an important role in determining stability.
We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets. This paper proposed "self-ensemble label filtering" for learning with noisy labels where the label noise is instance-independent, which yield more accurate identification of inconsistent predictions. This paper proposes an algorithm for learning from data with noisy labels which alternates between updating the model and removing samples that look like they have noisy labels.
We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks. The authors propose to replace dense layers with sparsely-connected linear layers and an approach to finding the best topology by measuring how well the sparse layers approximate random weights of their dense counterparts. The paper proposes a sparse cascade architecture that is a multiplication of several sparse matrices and a specific connectivity pattern that outperforms other provided considerations.
We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks. This paper extends Neural Architecture Search to the multi-task learning problem where a task conditioned model search controller is learned to handle multiple tasks simultaneously. In this paper, authors summarize their work on building a framework, called Multitask Neural Model Search controller, for automated neural network construction across multiple tasks simultaneously.
We model non-linear visual processes as autoregressive noise via generative deep learning. Proposes a new method that models non-linear visual process with a deep version of a linear process (Markov process). This paper proposes a new deep generative model for sequences, particularly image sequences and video, which uses a linear structure in part of the model.
This paper proposes a new feed-forward network, call PDE-Net, to learn PDEs from data. The paper expores the use of deep learning machinery for the purpose of identifying dynamical systems specified by PDEs. The paper proposes a neural network based algorithm for learning from data that arises from dynamical systems with governing equations that can be written as partial differential equations. This paper addresses complex dynamical systems modelling through nonparametric Partial Differential Equations using neural architectures, with the most important idea of the papier (PDE-net) to learn both differential operators and the function that governs the PDE.
We give a fast normalising-flow like sampling procedure for discrete latent variable models. This paper uses an autoregressive filtering variational approximation for parameter estimation in discrete dynamical systems by using fixed point iterations. The authors posit a general autoregressive posterior family for discrete variables or their continuous relaxations. This paper has two main contributions: it extends normalizing flows to discrete settings and presents an approximate fixed-point update rule for autoregressive time-series that can exploit GPU parallelism.
We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge. The authors propose the N-Gram machine to answer questions over long documents. This paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations which can be queried efficiently.
This paper proposes a meta-learning objective based on speed of adaptation to transfer distributions to discover a modular decomposition and causal variables. The paper shows that a model with the correct underlying structure will adapt faster to a causal intervention than a model with the incorrect structure. In this work, the authors proposed a general and systematic framework of meta-transfer objective incorporating the causal structure learning under unknown interventions.
Another perspective on catastrophic forgetting . This paper introduces a framework for combatting catastrophic forgetting based upon changing the loss term to minimise changes in classifier likelihood, obtained via a Taylor series approximation. This paper tries to solve the continual learning prolem by focusing on regularization approaches, and it proposes a L_1 strategy to mitigate the problem.
We propose an approach to construct realistic 3D facial morphable models (3DMM) that allows an intuitive facial attribute editing workflow by selecting the best sets of eigenvectors and anthropometric measurements. Proposes a piecewise morphable model for human face meshes and also proposes a mapping between anthropometric measurements of the face and the parameters of the model in order to synthesize and edit faces with desired attributes. This paper describes a method of part-based morphable facial model allowing for localized user control.
Two Algorithms outperformed eight others on a EEG-based BCI experiment .
We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk. The authors describe a variant of the negotation game with the consideration of a secondary communication channel for cheap talk, finding that the secondary channel improves negotation outcomes. This paper explores how agents can learn to communicate to solve a negotiation task and find that prosocial agents are able to learn to ground symbols using RL, but self-interested agents are not. Examines problems of how agents can use communication to maximise their rewards in a simple negotiation game.
We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner, the first to learn label propagation for transductive few-shot learning and produced effective empirical results. This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner. Studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.
We describe the use of an automated scheduling system for observation policy design and to schedule operations of NASA's ECOSTRESS mission. This paper presents an adaptation of an automated scheduling system, CLASP, to target an EO experiment (ECOSTRESS) on the ISS.
Hybird storage and representation of learned knowledge may be a reason for adversarial examples.
New Experiments and Theory for Adam Based Q-Learning . This paper provides a convergence result for traditional Q-learning with linear function approximation when using an Adam-like update. This paper describes a method to improve the AltQ algorithm by using a combination of an Adam optimizer and regularly restarting the internal parameters of the Adam optimizer.
A new capsule network that converges faster on our healthcare benchmark experiments. Presents a variant of capsule networks that instead of using EM routing employs a linear subspace spanned by the dominant eigenvector on the weighted votes matrix from the previous capsule. The paper proposes an improved routing method, which employs tools of eigendecomposition to find capsule activation and pose.
We propose a method of distributed fine-tuning of language models on user devices without collection of private data . This paper deals with improving language models on mobile equipments based on small portion of text that the user has inputted by employing a linearly interpolated objectives between user specific text and general English.
This paper utilizes the analysis of Lipschitz loss on a bounded hypothesis space to derive new ERM-type algorithms with strong performance guarantees that can be applied to the non-conjugate sparse GP model.
We propose a regularization method for neural network and a noise analysis method . This paper proposes a new regularization method to mitigate the overfitting issue of deep neural networks by rotating features with a random rotation matrix to reduce co-adaptation. This paper proposes a novel regularization method for training neural networks, which adds noise neurons in an inter-dependent fashion.
A probabilistic framework for multi-agent reinforcement learning . This paper proposes a new algorithm named Multi-Agent Soft Actor-Critic (MA-SAC) based on the off-policy maximum-entropy actor critic algorithm Soft Actor-Critic (SAC)
We provide a continuous relaxation to the sorting operator, enabling end-to-end, gradient-based stochastic optimization. The paper considers how to sort a number of items without explicitly necessarily learning their actual meanings or values and proposes a method to perform the optimization via a continuous relaxation. This work builds on a sum(top k) identity to derive a pathwise differentiable sampler of 'unimodal row stochastic' matrices. Introduces a continuous relaxation of the sorting operator in order to construct an end-to-end gradient-based optimization and introduces a stochastic extension of its method using Placket-Luce distributions and Monte Carlo.
We perform efficient and flexible transfer learning in the framework of Bayesian optimization through meta-learned neural acquisition functions. The authors present MetaBO which uses reinforcement learning to meta-learn the acquisition function for Bayesian Optimization, showing increasing sample efficiency on new tasks. The authors propose a meta-learning based alternative to standard acquisition functions (AFs), whereby a pretrained neural network outputs acquisition values as a function of hand-chosen features.
Deterministic deep neural networks do not discard information, but they do cluster their inputs. This paper provides a principled way to examine the compression phrase in deep neural networks by providing an theoretical sounding entropy estimator to estimate mutual information.
We propose regularization objectives for multi-agent RL algorithms that foster coordination on cooperative tasks. This paper proposes two methods of biasing agents towards learning coordinated behaviours and evaluates both rigorously across multi-agent domains of suitable complexity. This paper proposes two methods building upon MADDPG to encourage collaboration amongst decentralized MARL agents.
We present a model that learns robust joint representations by performing hierarchical cyclic translations between multiple modalities. This paper presents the Multimodal Cyclic Translation Network (MCTN) and evaluates it for multimodal sentiment analysis.
Understanding the neural network Hessian eigenvalues under the data generating distribution. This paper analyzes the spectrum of the Hessian matrix of large neural networks, with an analysis of max/min eigenvalues and visualization of spectra using a Lanczos quadrature approach. This paper uses the random matrix theory to study the spectrum distribution of the empirical Hessian and true Hessian for deep learning, and proposes an efficient spectrum visualization methods.
One simple trick to improve sequence models: Compose them with a graph model . This paper presents a structural summarization model with a graph-based encoder extended from RNN. This work combines Graph Neural Networks with a sequential approach to abstractive summarization, effective across all datasets in comparison to external baselines.
A sparse classifier based on a discriminative Gaussian mixture model, which can also be embedded into a neural network. The paper presents a Gaussian mixture model trained via gradient descent arguments which allows for inducing sparsity and reducing the trainable model layer parameters. This paper proposes a classifier, called SDGM, based on discriminative Gaussian mixture and its sparse parameter estimation.
Initialize weights using off-the-shelf Grassmannian codebooks, get  faster training and better accuracy .
An unsupervised domain adaptation approach which adapts at both the pixel and feature levels . This paper proposes a domain adaptation approach by extending the CycleGAN with task specific loss functions and loss imposed over both pixels and features. This paper proposes the use of CycleGANs for Domain Adaptation . This paper makes a novel extension to the previous work on CycleGAN by coupling it with adversarial adaptation approaches, including a new feature and semantic loss in the overall objective of the CycleGAN, with clear benefits.
Amharic Light Stemmer is designed for improving performance of  Amharic Sentiment Classification. This paper studies the stemming for morphologically rich languages with a light stemmer that only removes affixes to the extent that the original semantic information in the word is kept. This paper proposes a technique for Amharic light stemming using a cascade of transformations that standardize the form, remove suffixes, prefixes, and infixes.
We investigated if simple deep networks possess grid cell-like artificial neurons while memory retrieval in the learned concept space.
Pros and cons of saccade-based computer vision under a predictive coding perspective . Presents a computational framework for the active vision problem and explains how the control policy can be learned to reduce the entropy of the posterior belief.
We study theoretically the consistency the Laplacian spectrum and use it as whole-graph embeddding . This paper forcuses on the laplacian spectrum of a graph as means to generate a representation to be used to compare graphs and classify them. This work proposed to use Graph Laplacian spectrum to learn graph representation.
FGSM-based adversarial training, with randomization, works just as well as PGD-based adversarial training: we can use this to train a robust classifier in 6 minutes on CIFAR10, and 12 hours on ImageNet, on a single machine. This paper revisits Random+FGSM method to train robust models against strong PGD evasion attacks faster than previous methods. The main claim of this paper is that a simple strategy of randomization plus fast gradient sign method (FGSM) adversarial training yields robust neural networks.
We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training. The paper proposes a scale-invariant regularizer (DeepHoyer) inspired by the Hoyer measure to enforce sparsity in neural networks.
We show that extra unlabeled data is not required for self-supervised auxiliary tasks to be useful for time series classification, and present new and effective auxiliary tasks. This paper proposes a self-supervised method for learning from time series data in healthcare settings via designing auxilliary tasks based on data's internal structure to create more labeled auxilliary training tasks. This paper propose an approach for self-supervised learning on time series.
Eigenvalues of Conjugate (aka NNGP) and Neural Tangent Kernel can be computed in closed form over the Boolean cube and reveal the effects of hyperparameters on neural network inductive bias, training, and generalization. This paper gives a spectral analysis on neural networks' conjugate kernel and neural tangent kernel on boolean cube to resolve why deep networks are biased towards simple functions.
All functional brain parcellations are wrong, but some are useful .
Imitation from pixels, with sparse or no reward, using off-policy RL and a tiny adversarially-learned reward function. The paper proposes to use a "minimal adversary" in generative adversarial imitation learning under high-dimensional visual spaces. This paper aims at solving the problem of estimating sparse rewards in a high-dimensional input setting.
We show strategies to easily identify fake samples generated with the Generative Adversarial Network framework. Show that fake samples created with common generative adversarial network (GAN) implementations are easily identified using various statistical techniques. The paper proposes statistics to identify fake data generated using GANs based on simple marginal statistics or formal specifications automatically generated from real data.
We present an analytical framework to determine accumulation bit-width requirements in all three deep learning training GEMMs and verify the validity and tightness of our method via benchmarking experiments. The authors propose an analytical method to predict the number of mantissa bits needed for partial summations for convolutional and fully connected layers . The authors conduct a thorough analysis of the numeric precision required for the accumulation operations in neural network training and show the theoretical impact of reducing number of bits in the floating point accumulator.
A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations. Proposes a novel feature transfer network that optimizes domain adversarial loss and domain separation loss.
We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems . This paper proposes Prox-SGD, a theoretical framework for stochastic optimization algorithms shown to converge asymptotically to stationarity for smooth non-convvex loss + convex constraint/regularizer. The paper proposes a new gradient-based stochastic optimization algorithm with gradient averaging by adapting theory for proximal algorithms to the non-convex setting.
We give a bound for NNs on the output error in case of random weight failures using a Taylor expansion in the continuous limit where nearby neurons are similar . This paper considers the problem of dropping neurons from a neural network, showing that if the goal is to become robust to randomly dropped neurons during evaluation, then it is sufficient to just train with dropout. This contribution studies the impact of deletions of random neurons on prediction accuracy of trained architecture, with the application to failure analysis and the specific context of neuromorphic hardware.
We explore and study the synergies between sound and action. This paper explores the connections between action and sound by building a sound-action-vision dataset with a tilt-bot. This paper studies the role of audio in object and action perception, as well as how auditory information can help learning forward and inverse dynamics models.
We propose Hierarchical Complement Objective Training, a novel training paradigm to effectively leverage category hierarchy in the labeling space on both image classification and semantic segmentation. A method that regularizes the entropy of the posterior distribution over classes which can be useful for image classsification and segmentation tasks .
Our paper identifies the issue of existing weight sharing approach in neural architecture search and propose a practical method, achieving strong results. Author identifies an issue with NAS called posterior fading and introduces Posterior Convergent NAS to mitigate this effect .
We propose a novel two-phase training approach based on "early stopping" for robust training on noisy labels. Paper proposes to study how early stopping in optimization helps find confident examples . This paper proposes a two-phase training method for learning with label noise.
We introduce IC3Net, a single network which can be used to train agents in cooperative, competitive and mixed scenarios. We also show that agents can learn when to communicate using our model. Author proposes a new architecture for multi-agent reinforcement learning that uses several LSTM controllers with tied weights that transmit a continuous vector to each other . The authors propose an interesting gating scheme allowing agents to communicate in an multi-agent RL setting.
We present the first neural abstractive summarization model capable of customization of generated summaries.
We propose a software framework based on ideas of the Learning-Compression algorithm , that allows one to compress any neural network by different compression mechanisms (pruning, quantization, low-rank, etc.). This paper presents the design of a software library that makes it easier for the user to compress their networks by hiding away the details of the compression methods.
This paper proposes a method of end-to-end multi-modal generation of human face from speech based on a self-supervised learning framework. This paper presents a multi-modal learning framework that links the inference stage and generation stage for seeking the possibility of generating the human face from voice solely. This work aims to build one conditional face image generation framework from the audio signal.
A top-down approach how to recursively represent propositional formulae by neural networks is presented. This paper provides a new neural-net model of logical formulae that gathers information about a given formula by traversing its parse tree top-down. The paper pursues the path of a tree-structured network isomorphic to the parse tree of a propositional-calculus formula, but by passing information top-down rather than bottom-up.
Ape-X DQfD = Distributed (many actors + one learner + prioritized replay) DQN with demonstrations optimizing the unclipped 0.999-discounted return on Atari. The paper proposes three extensions (Bellman update, temporal consistency loss, and expert demonstration) to DQN to improve the learning performance on Atari games, achieving outperformance over the state-of-the-art results for Atari games. This paper proposes a transformed Bellman operator that aims to solve sensitivity to unclipped reward, robustness to the value of the discount factor, and the exploration problem.
Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering. The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models to regularize the embedding space instead of the parameter space. The paper argues for encoding external knowledge in the linguistic embedding layer of a multimodal neural network, as a set of hard constraints.
Solving inverse problems by using smooth approximations of the forward algorithms to train the inverse models.
A deep learning method for weakly-supervised pointwise localization that learns using image-level label only. It relies on conditional entropy to localize relevant and irrelevant regions aiming to minimize false positive regions. This work explores the problem of WSL using a novel design of regularization terms and a recursive erasing algorithm. This paper presents a new weakly supervised approach for learning object segmentation with image-level class labels.
Acquire states from high frequency region for search-control in Dyna. The authors propose to do sampling in the high-frequency domain to increase the sample efficiency . This paper proposes a new way to select states from which do do transitions in dyna algorithm.
We introduce a data-driven Distributed Source Coding framework based on Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC). The paper proposed a distributed recurrent auto-encoder for image compression that uses a ConvLSTM to learn binary codes that are constructed progressively from residuals of previously encoded information . The authors propose a method to train image compression models on multiple sources, with a separate encoder on each source, and a shared decoder.
Gates do all the heavy lifting in LSTMs by computing element-wise weighted sums, and removing the internal simple RNN does not degrade model performance. This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate . This paper presents an analysis of LSTMS, showing that they have a form where the memory cell contents at each step is a weighted combination of the “content update” values computed at each time step and offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context. The paper proposes a new insight to LSTM in which the core is an element-wise weighted sum and argues that LSTM is redundant by keeping only input and forget gates to compute the weights .
By analyzing more than 300 papers in recent machine learning conferences, we found that Machine Learning for Health (ML4H) applications lag behind other machine learning fields in terms of reproducibility metrics. This paper conducts a quantitative and qualitative review of the state of the reproducibility for ML healthcare applications and proposes reccomendations to make research more reproducible.
We train many small networks each for a specific operation, these are then combined to perform complex operations . This paper proposes to use neural networks to evaluate the mathematical expressions by designing 8 small building blocks for 8 fundamental operations, e.g., addition, subtraction, etc and then designing multi-digit multiplication and division using these small blocks. The paper proposes a method to design a NN based mathematical expression evaluation engine.
Improving the quality and stability of GANs using a relativistic discriminator; IPM GANs (such as WGAN-GP) are a special case. The paper proposes a “relativistic discriminator”, whic helps in some settings, although a bit sensitive to hyperparameters, architectures, and datasets. In this work, the authors considers a variation of GAN by simultaneously decreasing the probability that real data is real for the generator.
A state-value function-based version of MPO that achieves good results in a wide range of tasks in discrete and continuous control. This paper presents an algorithm for on-policy reinforcement learning that can handle both continuous/discrete control, single/multi-task learning and use both low dimensional states and pixels. The paper proposes an online variant of MPO, V-MPO, which learns the V-function and updates the non-parametric distribution towards the advantages.
We propose neural execution engines (NEEs), which leverage a learned mask and supervised execution traces to mimic the functionality of subroutines and demonstrate strong generalization. This paper investigates a problem of building a program execution engine with neural networks and proposes a transformer-based model to learn basic subroutines and applies them in several standard algorithms. This paper deals with the problem of designing neural network architectures that can learn and implement general programs.
Bayesian changepoint detection enables meta-learning directly from time series data. The paper considers the meta-learning in the task un-segmented setting and apply Bayesian online change point detection with meta-learning. This paper pushes meta-learning towards task-unsegmented settings, where the MOCA framework adopts a Bayesian changepoint estimation scheme for task change detection.
A deep learning based approach for zero delay fricative phoneme detection . This paper apples supervised deep learning methods to detect exact duration of a fricative phoneme in order to improve practical frequency lowering algorithm.
An online and linear-time attention mechanism that performs soft attention over adaptively-located chunks of the input sequence. This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention. The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position.
Develop new techniques that rely on patch reordering to enable detailed analysis of data-set relationship to training and generalization performances.
We produce reinforcement learning agents that generalize well to a wide range of environments using a novel regularization technique. The paper introduces the high variance policies challenge in domain randomization for reinforcement learning and mainly focuses on the problem of visual randomization, where the different randomized domains differ only in state space and the underlying rewards and dynamics are the same. To improve the generalization ability of deep RL agents across the tasks with different visual patterns, this paper proposed a simple regularization technique for domain randomization.
We explore the intersection of network neurosciences and deep learning.
This paper presents a system for unsupervised, high-precision knowledge base construction using a probabilistic program to define a process of converting knowledge base facts into unstructured text. Overview about existing knowledge base that is constructed with a probabilistic model, with the knowledge base construction approach evaluated against other knowledge base approaches YAGO2, NELL, Knowledge Vault, and DeepDive. This paper uses a probabilistic program describing the process by which facts describing entities can be realised in text and large number of web pages, to learn to perform fact extraction about people using a single seed fact.
New Signal Extraction Method in the Fourier Domain .
Improve the scalability of graph neural networks on imitation learning and prediction of swarm motion . The paper proposes a new time series model for learning a sequence of graphs. This work considers sequence prediction problems in a multi-agent system.
We propose a differentiable product quantization framework that can reduce the size of embedding layer in an end-to-end training at no performance cost. This paper works on methods for compressing embedding layers for low memory inference, where compressed embeddings are learned together with the task-specific models in a differentiable end-to-end fashion.
We introduce a simple and novel modal regression algorithm which is easy to scale to large problems. The paper proposes an implicit function approach to learning the modes of multimodal regression. The present work proposes a parametric approach to estimate the conditional mode using the Implicit Function Theorem for multi-modal distributions.
Sample efficient meta-RL by combining variational inference of probabilistic task variables with off-policy RL . This paper proposes using off-policy RL during the meta-training time to greatly improve sample efficiency of Meta-RL methods.
This paper focuses on identifying high quality web sources for industrial knowledge base augmentation pipeline.
We investigate the merits of employing neural networks in the match prediction problem where one seeks to estimate the likelihood of a group of M items preferred over another, based on partial group comparison data. This paper proposes a deep neural network solution to the set ranking problem and designs a architecture for this task inspired by previous manually designed algorithms. This paper provides a technique to solve the match prediction problem using a deep learning architecture.
A novel approach to maintain orthogonal recurrent weight matrices in a RNN. Introduces a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. Novel parametrization of RNNs allows representing orthogonal weight matrices relatively easily.
We use a single model to solve a great variety of natural language analysis tasks by formulating them in a unified span-relation format. This paper generalizes a wide range of natural language processing tasks as a single span-based framework and proposes a general architecture to solve all these problems. This work presents a unified formulation of various phrase and token level NLP tasks.
We present a variational lower bound for GP models that can be optimised without computing expensive matrix operations like inverses, while providing the same guarantees as existing variational approximations.
Variational Autoencoders with latent spaces modeled as products of constant curvature Riemannian manifolds improve on image reconstruction over single-manifold variants. This paper introduces a general formulation of the notion of a VAE with a latent space composed by a curved manifold. This paper is about developing VAEs in non-Euclidean spaces.
We introduce a black box algorithm for repeated optimization of compounds using a translation framework. The authors frame molecule optimization as a sequence-to-sequence problem, and extend existing methods for improving molecules, showing that it is beneficial for optimizing logP but not QED. The paper builds on existing translation models developed for molecular optimization, making an iterative use of sequence to sequence or graph to graph translation models.
Proposing the first watermarking framework for multi-bit signature embedding and extraction using the outputs of the DNN. Proposes a method for multi-bit watermarking of neural networks in a black-box setting and demonstrate that the predictions of existing models can carry a multi-bit string that can later be used to verify ownership. The paper proposes an approach for model watermarking where the watermark is a bit string embedded in the model as part of a fine-tuning procedure .
Don't know how to optimize? Then just learn to optimize! This paper proposes a way to train image classification models to be resistant to L-infinity perturbation attacks. This paper proposes using the learning-to-learn framework to learn an attacker.
A simple and easy to train method for multimodal prediction in time series. This paper introduces a times-series prediction model that that learns a deterministic mapping and trains another net to predict future frames given the input and residual error from the first network. The paper proposes a model for prediction under uncertainty where they separate out deterministic component prediction and uncertain component prediction.
This paper introduces and motivates simple_rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity.
This paper deals with stability of simple gradient penalty $\mu$-WGAN optimization by introducing a concept of measure valued differentiation. WGAN with a squared zero centered gradient penalty term w.r.t. to a general measure is studied. Characterizes the convergence of gradient penalized Wasserstein GAN.
State-of-the-art training method for binary and ternary weight networks based on alternating optimization of randomly relaxed weight partitions . The paper proposes a new training scheme of optimizing a ternary neural network. Authors propose RPR, a way to randomly partition and quantize weights and train the remaining parameters followed by relaxation in alternate cycles to train quantized models.
We replace some gradients paths in hierarchical RNN's by an auxiliary loss. We show that this can reduce the memory cost while preserving performance. The paper introduces a hierarchical RNN architecture that could be trained more memory efficiently. The proposed paper suggests to decouple the different layers of hierarchy in RNN using auxiliary losses.
Neural networks that do a good job of classification project points into more spherical shapes before compressing them into fewer dimensions.
We propose an novel learning method for deep sound recognition named BC learning. Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes to increase discriminitive power of the final learned network. Proposes a method to improve the performance of a generic learning method by generating "in between class" training samples and presents the basic intuition and necessity of the proposed technique.
We propose a method that infers the time-varying data quality level for spatiotemporal forecasting without explicitly assigned labels. Introduces a new definition of data quality that relies on the notion of local variation defined in (Zhou and Scholkopf) and extends it to multiple heterogenous data sources. This work proposed a new way to evaluate the quality of different data sources with the time-vary graph model, with the quality level used as a regularization term in the objective function .
We propose 3D shape programs, a structured, compositional shape representation. Our model learns to infer and execute shape programs to explain 3D shapes. An approach to infer shape programs given 3D models, with architecture consisting of a recurrent network that encodes a 3D shape and outputs instructions, and a second module that renders the program to 3D. This paper introduces a high-level semantic description for 3D shapes, given by the ShapeProgram.
We show that conventional regularization methods (e.g., $L_2$, dropout), which have been largely ignored in RL methods, can be very effective in policy optimization. The authors study a set of existing direct policy optimization methods in the field of reinforcement learning and provide a detailed investigation on the effect of regulations on the performance and behavior of agents following these methods. This paper provides a study on the effect of regularization on performance in training environments in policy optimization methods in multiple continuous control tasks.
We present a question-answering dataset, FigureQA, as a first step towards developing models that can intuitively recognize patterns from visual representations of data. This paper introduces a dataset of templated question answering on figures, involving reasoning about figure elements. The paper introduces a new visual reasoning dataset called Figure-QA which consists of 140K figure images and 1.55M QA pairs, which can help in developing models that can extract useful information from visual representations of data.
This position paper analyzes different types of self explanation that can arise in planning and related systems. Discusses different aspects of explanations, particularly in the context of sequential decision making.
The first deep learning approach to MFSR to solve registration, fusion, up-sampling in an end-to-end manner. This paper proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. This paper proposes a framework including recursive fusion to co-registration loss to solve the problem of super-resolution results and high-resolution labels not being pixel aligned.
For distributed training over high-latency networks, use gossip-based approximate distributed averaging instead of exact distribute averaging like AllReduce. The authors propose using gossip algorithms as a general method of computing approximate average over a set of workers approximately . The paper proves the convergence of SGP for nonconvex smooth functions and shows the SGP can achieve a significant speed-up in the low-latency environment without sacrificing too much predictive performance.
This paper develops an adversarial learning framework for neural conversation models with persona . This paper proposes an extension to hredGAN to simultaneously learn a set of attribute embeddings that represent the persona of each speaker and generate persona-based responses .
Bio-inspired artificial neural networks, consisting of neurons positioned in a two-dimensional space, are capable of forming independent groups for performing different tasks.
Discrete transformer which uses hard attention to ensure that each step only depends on a fixed context. This paper presents modifications to the standard transformer architecture with the goal of improving interpretability while retaining performance in NLP tasks. This paper proposes three Discrete Transformers: a discrete and stochastic Gumbel-softmax based attention module, a two-stream syntactic and semantic transformer, and sparsity regularization.
We show empirical evidence that predictive coding models yield representations more correlated to brain data than supervised image recognition models.
A generic framework for handling transfer and multi-task learning using pairs of autoencoders with task-specific and shared weights. Proposes a generic framework for end-to-end transfer learning / domain adaptation with deep neural networks. This paper proposes a model for allowing deep neural network architectures to share parameters across different datasets, and applies it to transfer learning. The paper focuses on learning common features from multiple domains data and ends up with a general architecture for multi-task, semi-supervised and transfer learning .
We propose a framework to combine decision trees and neural networks, and show on image classification tasks that it enjoys the complementary benefits of the two approaches, while addressing the limitations of prior work. The authors proposed a new model, Adaptive Neural Trees, by combining the representation learning and gradient optimization of neural networks with architecture learning of decision trees . This paper proposes the Adaptive Neural Trees approach to combine the two learning paradigms of deep neural nets and decision trees .
Translating portions of the input during training can improve cross-lingual performance. The paper proposes a cross-lingual data augmentation method to improve the language inference and question answering tasks. This paper proposes to augment crosslingual data with heuristic swaps using aligned translations, like bilingual humans do in code-switching.
We propose a conditional variational autoencoder framework that mitigates the posterior collapse in scenarios where the conditioning signal strong enough for an expressive decoder to generate a plausible output from it. This paper considers strongly conditioned generative models, and proposes an objective function and a parameterisation of the variational distribution such that latent variables explicitly depend on input conditions. This paper argues that when the decoder is conditioned on the concatenation of latent variables and auxiliary information, then posterior collapse is more likely than in vanilla VAE.
We propose a study of the stability of several few-shot learning algorithms subject to variations in the hyper-parameters and optimization schemes while controlling the random seed. This paper studies reproducibility for few-shot learning.
We translate a bound on sub-optimality of representations to a practical training objective in the context of hierarchical reinforcement learning. The authors proposes a novel approach in learning a representation for HRL and state an intriguing connection between representation learning and bounding the sub-optimality which results in a gradient based algorithm . This paper proposes a way to handle sub-optimality in the context of learning representations which refer to the sub-optimality of hierarchical polity with respect to the task reward.
Metareasoning in a Situated Temporal Planner . This paper addresses the problem of situated temporal planning, proposing a further simplification on  greedy strategies previously proposed by Shperberg.
Robustness performance of PGD trained models are sensitive to semantics-preserving transformation of image datasets, which implies the trickiness of evaluation of robust learning algorithms in practice. Paper clarifies the difference between clean and robust accuracy and shows that changing the marginal distribution of the input data P(x) while preserving its semantic P(y|x) affects the robustness of the model. This paper investigates the origin of the lack of robustness of classifiers to perturbations of adversarial inputs under l-inf bounded perturbations.
Matching sentences by learning the latent constituency tree structures with a variant of the inside-outside algorithm embedded as a neural network layer. This paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences . This paper proposes a model of structured alignments between sentences as a means of comparing sentences by matching their latent structures.
Learn disentangle representation in an unsupervised manner. The authors present a framework in which an auto encoder (E, D) is regularized such that its latent representation to share mutual information with a generated latent space representation.
Conditional GANs trained to generate data augmented samples of their conditional inputs used to enhance vanilla classification and one shot learning systems such as matching networks and pixel distance . The authors propose a method to conduct data augmentation where the cross-class transformations are mapped to a low dimensional latent space using conditional GAN .
We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets. This paper proposes a practical improvement of the conditional randomization test and a new test statistic, proves f-divergence is one possible choice, and shows that KL-divergence cancels out some conditional distributions. This paper addresses the problem of finding useful features in an input that are dependent on a response variable even when conditioning on all other input variables. A model agnostic method to provide interpretation on the influence of input features on the response of a machine level model down to instance level, and proper test statistics for model agnostic feature selection.
A new approach for learning a model from noisy crowdsourced annotations. This paper proposes a method for learning from noisy labels, focusing on the case when data isn't redundantly labeled with theoretical and experimental validation . This paper focuses on the learning-from-crowds problem, where jointly updating the classifier weights and the confusion matrices of workers can help on the estimation problem with rare crowdsourced labels. Proposes a supervised learning algorithm for modeling label and worker quality and utilizes algorithm to study how much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better labels.
New way of explaining why a neural network has misclassified an image . This paper proposes a method for explaining the classification mistakes of neural networks. Aims to better understand the classification of neural networks and explores the latent space of a variational auto encoder and considers the perturbations of the latent space in order to obtain the correct classification.
A method for the automated construction of branched multi-task networks with strong experimental evaluation on diverse multi-tasking datasets. This paper proposes a novel soft parameter sharing Multi-task Learning framework based on a tree-like structure. This paper presents a method to infer multi-task networks architecture to determine which part of the network should be shared among different tasks.
It's possible to substitute the weight matrix in a convolutional layer to train it as a structured efficient layer; performing as well as low-rank decomposition. This work applies previous Structured Efficient Linear Layers to conv layers and proposes Structured Efficient Convolutional Layers as substitution of original conv layers.
We present SVDocNet, an end-to-end trainable U-Net based spatial recurrent neural network (RNN) for blind document deblurring.
We extend bilinear sparse coding and leverage video sequences to learn dynamic filters.
We propose a novel OOD detector that employ blurred images as adversarial examples . Our model achieve significant OOD detection performance in various domains. This paper presents the idea to use blurred images as regularizing examples to improve out-of-distribution detection performance based on Random Network Distillation. This paper tackles out-of-data distribution by leveraging RND applied to data augmentations by training a model to match the outputs of a random network with an augmentation as input.
A fast optimizer for general applications and large-batch training. In this paper, the authors made a study on large-batch training for the BERT, and successfully trained a BERT model in 76 minutes. This paper develops a layerwise adaptation strategy that allows training BERT models with large 32k mini-batches vs baseline 512.
We analyzed the role of two learning rates in model-agnostic meta-learning in convergence. The authors tackled the optimization instability problem in MAML by investigating the two learning rates. This paper studies a method to help tune the two learning rates used in the MAML training algorithm.
Task-independent neural model for learning associations between interrelated groups of words. The paper proposed a method for training function-specific word vectors, in which each word is represented with three vectors each in a different category (Subject-Verb-Object). This paper proposes a neural network to learn function-specific work representations and demonstrates the advantage over alternatives.
Using deep learning method to carry out automatic measurement of SEM images in semiconductor industry .
This paper describes and analyzes three methods to schedule non-fixed duration activities in the presence of consumptive resources. The paper presents three approaches for on-board scheduling of activities in a planetary rover under reservoir resource constraints.
Description of submission to NeurIPS2019 Disentanglement Challenge based on hyperspherical variational autoencoders .
An anomaly detection that:  uses random-transformation classification for generalizing to non-image data. This paper proposes a deep method for anomaly detection that unifies recent deep one-class classification and transformation-based classification approaches. This paper proposes an approach to classification-based anomaly detection for general data by using the affine transformation y = Wx+b.
We reduce sentiment biases based on counterfactual evaluation of text generation using language models. This paper measures sentiment bias in language models as reflected by text generated by the models, and adds other objective terms to the usual language modeling objective to reduce bias. This paper proposes to evaluate bias in pre-trained language models by using a fixed sentiment system and tests several different prefix templates. A method based on semantic simiilarity and a method based on sentiment similarity to debias the neural language models trained from large datasets.
A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks. This paper constructs an infinite Topic Model with Variational Auto-Encoders by combining Nalisnick & Smith's stick-breaking variational auto-encoder with latent Dirichlet allocation and several inference techniques used in Miao.
We present a novel framework of Knowledge Distillation utilizing peer samples as the teacher . Proposes a method for improving the effectiveness of knowledge distillation by softening the labels used and employing a dataset instead of a single sample. This paper proposes to address the extra computational cost of training with knowledge distillation, building on the recently proposed Snapshot Distillation technique.
learn hierarchal sub-policies through end-to-end training over a distribution of tasks . The authors consider the problem of learning a useful set of ‘sub policies’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution. This paper proposes a novel method for inducing temporal hierarchical structure in a specialized multi-task setting.
Convolutional neural network model for unsupervised document embedding. Introduces a new model for the general task of inducing document representations (embeddings) which uses a CNN architecture to improve computational efficiency. This paper proposes using CNNs with a skip-gram like objective as a fast way to output document embeddings .
We prove generalization bounds for convolutional neural networks that take account of weight-tying . Studies the generalization power of CNNs and improves the upper bounds of generalization errors, showing correlation between the generalization error of learned CNNs and the upper bound's dominant term. This paper presents a generalization bound for convolutional neural networks based on the number of parameters, the Lipschitz constant, and the distance of the final weights from initialization.
2x savings in model size, 28% energy reduction for MobileNets on ImageNet at no loss in accuracy using hybrid layers composed of conventional full-precision filters and ternary filters . Focuses on quantizing the MobileNets architecture to ternary values, lowering the required space and computation in order to make neural networks more energy efficient. The paper proposes layer-wise hybrid filter bank which only quantizes a fraction of convolutional filters to ternary values towards the MobileNets architecture.
We establish a benchmark of controlled real noise and reveal several interesting findings about real-world noisy data. This paper compares 6 existing noisy label learning methods in two training settings: from scratch, and finetuning. The authors establish a large dataset and benchmark of controlled real-world noise for performing controlled experiments on noisy data in deep learning.
We learn to solve the RNA Design problem with reinforcement learning using meta learning and autoML approaches. Used policy gradient optimization for generating RNA sequences which fold into a target secondary structure, resulting in clear accuracy and runtime improvements.
Training small networks beats pruning, but pruning finds good small networks to train that are easy to copy.
We study the problem of learning to predict the underlying diversity of beliefs present in supervised learning domains.
We introduced a strategy which enables inpainting models on datasets of various sizes . Help image inpainting using GANs by using a comparative augmenting filter and adding random noise to each pixel.
We find evidence that divergence minimization may not be an accurate characterization of GAN training. The submission aims to present empirical evidence that the theory of divergence minimization is more a tool to understand the outcome of training GANs than a necessary condition to be enforce during training itself . This paper studies non-saturating GANs and the effect of two penalized gradient approaches, considering several thought experiments to demonstrate observations and validate them on real data experiments.
A new & practical statistical test of dependency using neural networks, benchmarked on synthetic and a real fMRI datasets. Proposes a neural-network-based estimation of mutal information which can reliably work with small datasets, reducing the sample complexity by decoupling the network learning problem and the estimation problem.
Image captioning using two-dimensional word embedding.
LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. Introduces a method for creating mini batches for a student network by using a second learned representation space to dynamically select examples by their 'easiness and true diverseness'. Experiments the classification accuracy on MNIST, FashionMNIST, and CIFAR-10 datasets to learn a representation with curriculum learning style minibatch selection in an end-to-end framework.
We propose to construct macro actions by a genetic algorithm, which eliminates the dependency of the macro action derivation procedure from the past policies of the agent. This paper proposes a generic algorithm for constructing macro actions for deep reinforcement learning by appending a macro action to the primitive action space.
We propose an extension to LFADS capable of inferring spike trains to reconstruct calcium fluorescence traces using hierarchical VAEs.
We introduce the first successful method to train neural machine translation in an unsupervised manner, using nothing but monolingual corpora . The authors present a model for unsupervised NMT which requires no parallel corpora between the two languages of interest. This is a paper on unsupervised MT which trains a standard architecture using word embeddings in a shared embedding space only with bilingual word papers and an encoder-decoder trained using monolingual data.
We train generative adversarial networks in a progressive fashion, enabling us to generate high-resolution images with high quality. Introduces progressive growing and a simple parameter-free minibatch summary statistic feature for use in GAN training to enable synthesis of high-resolution images.
A graph-based spherical CNN that strikes an interesting balance of trade-offs for a wide variety of applications. Combines existing CNN frameworks based on the discretization of a sphere as a graph to show a convergence result which is related to the rotation equivalence on a sphere. The authors use the existing graph CNN formulation and a pooling strategy that exploits hierarchical pixelations of the sphere to learn from the discretized sphere.
We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces. Paper's concepts work in the discrete-time formalism, use the master equation, and  remove reliance on a locally quadratic approximation of the loss function or on any Gaussian asumptions of the SGD noise. The authors derive the stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in SGD and use the relations to set training schedule adaptively and analyze the loss-function landscape.
We propose a mechanism for denoising the internal state of an RNN to improve generalization performance.
For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks. The authors consider the problem of learning in input-driven environments, show how the PG theorem still applies for an input-aware critic, and show that input-dependent baselines are the best to use in conjecture with that critic. This paper introduces the notion of input-dependent baselines in Policy Gradient Methods in RL, and proposes different methods to train the input dependent baseline function to help clear variance from external factor perturbation.
Augmenting the top layer of a classifier network with a style memory enables it to be generative. This paper proposes to train a classifier neural network not just to classifiy, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance . The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation .
Per-example routing models benefit from architectural diversity, but still struggle to scale to a large number of routing decisions. Adds diversity to the type of architectural unit available for the router at each decision and scaling to deeper networks, achieving state of the art performance on Omniglot. This work extends routing networks to use diverse architectures across routed modules .
We present RNNs for training surrogate models of PDEs, wherein consistency constraints ensure the solutions are physically meaningful, even when the training uses much smaller domains than the trained model is applied to.
We propose the use of optimistic mirror decent to address cycling problems in the training of GANs. We also introduce the Optimistic Adam algorithm . This paper proposes the use of optimistic mirror descent to train WGANs . The paper proposes to use optimistic gradient descent for GAN training that avoids the cycling behavior observed with SGD and its variants and provides promising results in GAN training. This paper proposes a simple modification of standard gradient descent, claiming to improve the convergence of GANs and other minimax optimization problems.
A simple extension of generalized matrix factorization can outperform state-of-the-art approaches for recommendation. The work presents a matrix factorization framework for enforcing the effect of historical data when learning user preferences in collaborative filtering settings.
A method that build representations of sequential data and its dynamics through generative models with an active process . Combines neural networks and Gaussian distributions to create an architecture and generative model for images and video which minimizes the error between generated and supplied images. The paper proposes a Bayesian network model, realized as a neural network, that learns different data in the form of a linear dynamical system .
We propose polynomial as activation functions. The authors introduce learnable activation functions that are parameterized by polynomial functions and show results slightly better than ReLU.
A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.
We show that disentangled VAEs are more robust than vanilla VAEs to adversarial attacks that aim to trick them into decoding the adversarial input to a chosen target. We then develop an even more robust hierarchical disentangled VAE, Seatbelt-VAE. The authors propose a new VAE model called seatbelt-VAE, showing to be more robust for latent attack than benchmarks.
We demonstrate that function changes in the backpropagation is equivalent to an implicit learning rate .
A reinforcement learning approach to text style transfer . Introduces an RL-based method which leverages a pre-trained language model to transfer text style, without a disentanglement objective, while using style-transfer generations from another model. The authors propose a combination reward composed of fluency, content, and style for text style transfer.
We show that highly-structured semantic hierarchy emerges in the deep generative representations as a result for synthesizing scenes. Paper investigates the aspects encoded by the latent variables input into different layers in StyleGAN. The paper presents a visually-guided interpretation of activations of the convolution layers in the generator of StyleGAN on layout, scene category, scene attributes, and color.
We pool messages amongst multiple SMILES strings of the same molecule to pass information along all paths through the molecular graph, producing latent representations that significantly surpass the state-of-the-art in a variety of tasks. Method uses multiple inputs of SMILES strings, character-wise feature fusion across those strings, and network training through multiple output targets of SMILES strings, creating a robust fixed-length latent representation independent of SMILES variation. The authors describe a novel variational autoencoder like method for molecules which encode molecules as strings to reduce the operations needed to share information across atoms in the molecule.
We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs. The paper proposes a regularization term for the conditional GAN objective in order to promote diverse multimodal generation and prevent mode collapse. The paper proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction, which can be applied to various conditional synthesis frameworks for various tasks.
Equipping the transformer model with shortcuts to the embedding layer frees up model capacity for learning novel information.
We examine the relationship between probability density values and image content in non-invertible GANs. The authors try to estimate the probability distribution of the image with the help of GAN and develop a proper approximation to the PDFs in the latent space.
We propose spatially shuffled convolution that the regular convolution incorporates the information from outside of its receptive field. Proposes SS convulation which uses information outside of its RF, showing improved results when tested on multiple CNN models. The authors proposed a shuffle strategy for convolution layers in convolution layers in convolutional neural networks.
A method to model the generative distribution of sequences coming from graph connected entities. The authors propose a method to model sequential data from multiple interconnected sources using a mixture of common pool of HMM's.
Our work applies meta-learning to multi-agent Reinforcement Learning to help our agent efficiently adapted to new coming opponents. This paper focuses on fast adaptation to new behaviour of the other agents of the environment using a method based on MAML . The paper presents an approach to multi-agent learning based on the framework of model-agnostic meta learning for the task of opponent modeling for multi-agent RL.
We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. The paper is dedicated to computation of singular values of convolutional layers . Derives exact formulas for computing singular values of convolutional layers of deep neural networks and show that computing the singular values can be done much faster than computing the full SVD of the convolution matrix by appealing to fast FFT transformations.
VariBAD opens a path to tractable approximate Bayes-optimal exploration for deep RL using ideas from meta-learning, Bayesian RL, and approximate variational inference. This paper presents a new deep reinforcement learning method that can efficiently trade-off exploration and exploitation that combines meta-learning, variational inference, and bayesian RL.
We show metric learning can help reduce catastrophic forgetting . This paper applies metric learning to reduce catastrophic forgetting on neural networks by improving the expressiveness of the final layer, leading to better results in continual learning.
We present NormCo, a deep coherence model which considers the semantics of an entity mention, as well as the topical coherence of the mentions within a single document to perform disease entity normalization. Uses a GRU autoencoder to represent the "context" (related enitities of a given disease within the span of a sentence), solving the BioNLP task with significant improvements over the best-known methods.
We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others. Presents multiplicative interaction as a unified characterization for representing commonly used model architecture design components, showing empirical proof of superior performance on tasks like RL and sequence modeling. The paper explores different types of multiplicative interactions and finds MI models able to achieve a state-of-the-art performance on language modeling and reinforcement learning problems.
An effective text-conditioning GAN framework for generating videos from text . This paper presents a GAN-based method for video generation conditioned on text description, with a new conditioning method that generates convolution filters from the encoded text, and uses them for a convolution in the discriminator. This paper proposes conditional GAN models for text-to-video synthesis: developing text-feature-conditioned CNN filters and constructing moving-shape dataset with improved performance on video/image generation.
SplitLBI is applied to deep learning to explore model structural sparsity, achieving state-of-the-art performance in ImageNet-2012 and unveiling effective subnet architecture. Proposes an optimization based algorithm for finding important sparse structures of large-scale neural networks by coupling the learning of weight matrix and sparsity constraints, offering guaranteed convergence on nonconvex optimization problems.
We propose gated mechanisms to enhance learned ISTA for sparse coding, with theoretical guarantees on the superiority of the method. Proposes extensions to LISTA which address underestimation by introducing "gain gates" and including momentum with "overshoot gates", showing improved convergence rates. This paper is focused on solving sparse coding problems using LISTA-type networks by proposing a "gain gating function" to mitigate the weakness of the "no false positive" assumption.
We present an efficient and adaptive framework for comparing image classifiers to maximize the discrepancies between the classifiers, in place of comparing on fixed test sets. Error spotting mechanism which compares image classifiers by sampling their "most disagreed" test set, measuring disagreement through a semantics-aware distance derived form WordNet ontology.
We propose a technique that modifies CNN structures to enhance robustness while keeping high test accuracy, and raise doubt on whether current definition of adversarial examples is appropriate by generating adversarial examples able to fool humans. This paper proposes a simple technique for improving the robustness of neural networks against black-box attacks. The authors propose a simple method for increasing the robustness of convolutional neural networks against adversarial examples, with surprisingly good results.
We propose to compare semi-supervised and robust learning to noisy label under a shared setting . The authors propose a strategy based on mixup for training a model in a formal setting that includes the semi-supervised and the robust learning tasks as special cases.
This paper experimentally demonstrates the beneficial effect of top-down connections in Hierarchical Sparse Coding algorithm. This paper presents a study that compares techniques for Hierarchical Sparse Coding, showing that the top-down term is beneficial in reducing predictive error and can learn faster.
A black box approach for explaining the predictions of an image similarity model. Introduces method for image similarity model explanation which identifies attributes that contribute positively to the similarity score and pairs them with a generated saliency map. The paper proposes an explanation mechanism that pairs the typical saliency map regions together with attributes for similarity matching deep neural networks.
How you should evaluate adversarial attacks on seq2seq . The authors investigate ways of generating adversarial examples, showing that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting. The paper is about meaning-preserving adversarial perturbations in the context of Seq2Seq models .
An alternative normalization technique to batch normalization . Introduces a normalization technique, which normalizes the weights of convolutional layers. This manuscript introduces a new layer-wise transform, EquiNorm, to improve upon batch normalization that does not modify the inputs to the layers but rather the layer weights.
Represent each entity as a probability distribution over contexts embedded in a ground space. Proposes to construct word embeddings from a histogram over context words, instead of as point vectors, which allows for measuring distances between two words in terms of optimal transport between the histograms through a method that augments representation of an entity from standard "point in a vector space" to a histogram with bins located at some points in that vector space.
Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution. This paper proposes an alternative view for adversarial examples in high dimension spaces by considering the "error rate" in a Gaussian distribution centered at each test point.
Studies how self-supervised learning and knowledge distillation interact in the context of building compact models. Investigates training compact pre-trained language models via distillation and shows that using a teacher for distilling a compact student model performs better than directly pre-training the model. This submission shows that pre-training a student directly on masked language modeling is better than distillation, and the best is to combine both and distill from that pre-trained student model.
We introduce the universal deep neural network compression scheme, which is applicable universally for compression of any models and can perform near-optimally regardless of their weight distribution. Introduces a pipeline for network compression that is similar to deep compression and uses randomized lattice quantization instead of the classical vector quantization, and uses universal source coding (bzip2) instead of Huffman coding.
This paper tries to preliminarily address the disentanglement theoretically in the idealistic situation and practically through noise modelling perspective in the realistic case. Studies the importance of the noise modelling in Gaussian VAE and proposes to train the noise using Empirical-Bayes like fashion. Modifying how noise factors are treated when developing VAE models .
We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization. Discusses the effect of weight decay on the training of deep network models with and without batch normalization and when using first/second order optimization methods and hypothesizes that a larger learning rate has a regularization effect.
The very first freely available domain adaptation dataset for sound event detection.
Mutual information estimator based nonextensive statistical mechanics . This paper tries to establish novel variational lower bounds for mutual information by introducing parameter q and defining q-algebra, showing that the lower bounds have smaller variance and achieves high values.
We show that stochastic gradient descent ascent converges to a global optimum for WGAN with one-layer generator network. Attempts to prove that the Stochastic Gradient Decent-Ascent could converge to a global solution for the min-max problem of WGAN.
We empirically show that adversarial training is effective for removing universal perturbations, makes adversarial examples less robust to image transformations, and leaves them detectable for a detection approach. Analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples and how adversarial training affects detection. The authors show that adversarial training is effective in protecting against "shared" adversarial perturbation, in particular against universal perturbation, but less effective to protect against singular perturbations.
We introduce techniques to train a single once-for-all network that fits many hardware platforms. Method results in a network from which one can extract sub-networks for various resouce constraints (latency, memory) which perform well without a need for retraining. This paper tries to tackle the problem of searching best architectures for specialized resource constraint deployment scenarios with a prediction based NAS method.
Propose an approach for boosting generative models by cascading hidden variable models . This paper proposed a novel approach of cascaded boosting for boosting generative models which allows each each meta-model to be trained separately and greedily.
We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks. Proposes the "edge probing" method and focuses on the relationship between spans rather than individual words, enabling the authors to look at syntactic constituency, dependencies, entity labels, and semantic role labeling. Provides new insights on what is captured contextualized word embeddings by compiling a set of “edge probing” tasks.
We introduce DPFRL, a framework for reinforcement learning under partial and complex observations with a fully differentiable discriminative particle filter . Introduces ideas for training DLR agents with latent state variables, modeled as a belief distribution, so they can handle partially observed environments. This paper introduces a principled method for POMDP RL: Discriminative Particle Filter Reinforcement Learning that allows for reasoning with partial observations over multiple time steps, achieving state-of-the-art on benchmarks.
Monte Carlo Objectives are analyzed using auxiliary variable variational inference, yielding a new analysis of CPC and NCE as well as a new generative model. Proposes a different view on improving variational bounds with auxiliary latent variable models and explores the use of those models in the generative model.
We improve the running of all existing gradient descent algorithms. Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH. Considers SGD over an objective of the form of a sum over examples of a quadratic loss.
Limitations of current AI are generally recognized, but fewer people are aware that we understand enough about the brain to immediately offer novel AI formulations.
We use question-answering to evaluate how much knowledge about the environment can agents learn by self-supervised prediction. Proposes QA as a tool to investigate what agents learn about in the world, arguing this as an intuitive method for humans which allows for arbitrary complexity. The authors propose a framework to assess representations built by predictive models that contain sufficient information to answer questions about the environment they are trained on, showing those by SimCore contained sufficient information for the LSTM to answer questions accurately.
We develop a new method for imbalanced classification using adversarial examples . Proposes a new optimization objective that generates synthetic samples by over-sampling the majority classes instead of minority classes, solving the problem of overfitting minority classes. The authors propose to tackle imbalance classification using re-sampling methods, showing that adversarial examples in the minority class would help to train a new model that generalizes better.
An interesting application of CNN in soft condensed matter physics experiments. The authors demonstrate that a deep learning approach offers improvement to both the identification accuracy and rate at which defects can be identified of nematic liquid crystals. Apply a well known neural model (YOLO) to detect bounding boxes of objects in images.
An analysis of the effects of compositionality and locality on representation learning for zero-shot learning. Proposes evaluation framework for ZSL where the model is not allowed to be pretrained and instead, model parameters are randomly initialized for better understanding of what's happening in ZSL.
Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.
We present a formulation of curiosity as a visual representation learning problem and show that it allows good visual representations in agents. This paper formulates curiosity based RL training as learning a visual representation model, arguing that focusing on better LR and maximising model loss for novel scenes will get better overall performance.
From an incomplete RGB-D scan of a scene, we aim to detect the individual object instances comprising the scene and infer their complete object geometry. Proposes an end-to-end 3D CNN structure which combines color features and 3D features to predict the missing 3D structure of a scene from RGB-D scans. The authors propose a novel end-to-end 3D convolutional network which predicts 3D semantic instance completion as object bounding boxes, class labels and complete object geometry.
XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset. This paper proposes a new GAN-based model for unpaired image-to-image translation similar to DTN .
Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice. Presents a distributed implementation of signSGD with majority vote as aggregation.
We correct nuisance variation for image embeddings across different domains, preserving only relevant information. Discusses a method for adjusting image embeddings in order tease apart technical variation from biological signal. The authors present a method to remove domain-specific information while preserving the relevant biological information by training a network that minimizes the Wasserstein distance between distrbutions.
A scalable in sample size and dimensions mutual information estimator.
The new combination of reinforcement and supervised learning, dramatically decreasing the number of required samples for training on video . This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy .
Fast learning via episodic memory verified by a biologically plausible framework for prefrontal cortex-basal ganglia-hippocampus (PFC-BG) circuit .
In this work, we point to a new connection between DNNs expressivity and Sharkovsky’s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks . Shows how the expressive power of NN depends on its depth and width, furthering the understanding of the benefit of deep nets for representing certain function classes. The authors derive depth-width tradeoff conditions for when relu networks are able to represent periodic functions using dynamical systems analysis.
We investigate quantization-aware training in very low-bit quantized keyword spotters to reduce the cost of on-device keyword spotting. This submission proposes a combination of low-rank decomposition and quanitization approach to compress DNN models for keyword spotting.
A novel graph signal processing framework for quantifying the effects of experimental perturbations in single cell biomedical data. This paper introduces several methods to process experimental results on biological cells and proposes a MELD algorithm mapping hard group assignments to soft assignments, allowing relevant groups of cells to be clustered.
We propose a class of user models based on using Gaussian processes applied to a transformed space defined by decision rules .
We propose a Bayes-optimal Bayesian optimization algorithm for hyperparameter tuning by exploiting cheap approximations. Studies hyperparameter-optimization by Bayesian optimization, using the Knowledge Gradient framework and allowing the Bayesian optimizer to tune fidelity against cost.
We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack. Performs a careful study of mixed integer linear programming approaches for verifying robustness of neural networks to adversarial perturbations and proposes three enhancements to MILP formulations of neural network verification.
A set of methods to obtain uncertainty estimation of any given model without re-designing, re-training, or to fine-tuning it. Describes several approaches for measuring uncertainty in arbitrary neural networks when there is an absence of distortion during training.
Proposed higher order operation for context learning . Proposes a new 3D convolutional block which convolves video input with its context, based on the assumpton that relevant context is present around the image's object.
Consistency-based models for semi-supervised learning do not converge to a single point but continue to explore a diverse set of plausible solutions on the perimeter of a flat region. Weight averaging helps improve generalization performance. The paper proposes to apply Stochastic Weight Averaging to the semi-supervised learning context, arguing that the semi-supervised MT/Pi models are especially amenable to SWA and propose fast SWA to speed up training.
We successfully convert a popular detector RPN to a well-performed tracker from the viewpoint of loss function.
A neural architecture for scoring and ranking program repair candidates to perform semantic program repair statically without access to unit tests. Presents a neural network architecture consisting of the share, specialize and compete parts for repairing code in four cases.
Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? Yes! Exploits input-adaptive multiple early-exits for the field of adversarial attack and defense, reducing the average inference complexity without conflicting the larger capacity assumption.
We show that individual units in CNN representations learned in NLP tasks are selectively responsive to specific natural language concepts. Uses grammatical units of natural language that preserve meanings to show that the units of deep CNNs learned in NLP tasks could act as a natural language concept detector.
It is a mostly theoretical paper that describes the challenges in disentangling factors of variation, using autoencoders and GAN. This paper considers disentangling factors of variation in images, shows that in general, without further assumptions, one cannot tell apart two different variation factors, and suggests a novel AE+GAN architecture to try and disentangle variation factors. This paper studies the challenges of disentangling independent factors of variation under weakly labeled data and introduces the term reference ambiguity for data point mapping.
learning to rank with several embeddings and attentions . Proposes to use attention to combine multiple input representations for both query and search results in the learning to rank task.
We developed an algorithm that takes as input recordings of neural activity and returns clusters of neurons by cell type and models of neural activity constrained by these clusters.
We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights. Suggests training neural networks to imitate graph algorithms by learning primitives and subroutines rather than the final output.
We describe an architecture for generating diverse hypotheses for intermediate goals during robotic manipulation tasks. Evaluates the quality of a proposed generative predictive model to generate plans for robot execution. This paper proposes a method for learning a high-level transition function that is useful for task planning.
This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies. Develops algorithms for the solution of variational inequalities in the stochastic setting, proposing a variation of the extragradient method.
We learn sohpisticated trajectories of an object purely from pixels with a toy video dataset by using a VAE structure with a Gaussian process prior.
We investigate the neural basis of dream recall using convolutional neural network and feature visualization techniques, like tSNE and guided-backpropagation.
This paper proposes a new formulation and a new communication protocol for networked multi-agent control problems . Concerned with N-MARL's where agents update their policy based only on messages from neighboring nodes, showing that introducing a spatial discount factor stabilizes learning.
Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.
We effectively leverage a few keywords as weak supervision for training neural networks for aspect extraction. Discusses a variant of knowledge distillation which uses a "teacher" based on a bag-of-words classifier with seed words and a "student" which is an embedding-based neural network.
Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems. Using neural networks as a computational model of the brain, examines the efficiency of different strategies for solving two visual challenges.
We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2. Solves the GAN challenge in raw waveform synthesis and begins to close the existing performance gap between autoregressive models and GANs for raw audios.
we propose an algorithm of learning to prune network by enforcing structure sparsity penalties . This paper introduces an approach to pruning while training a network using lasso and split LBI penalties .
We introduce unsupervised continual learning (UCL) and a neuro-inspired architecture that solves the UCL problem. Proposes using hierachies of STAM modules to solve the UCL problem, providing evidence that the representations the modules learn are well-suited for few-shot classification.
New Signal Extraction Method in the Fourier Domain . Contributes a complex-valued convolutional version of the Feature-Wise Linear Modulation which allows parameter optimization and designs a loss which takes into account magnitude and phase.
We present a novel framework to learn the disentangled representation of content and style in a completely unsupervised manner. Propose model based on autoencoder framework to disentangle an object's representation, results show that model can produce representations capturing content and style.
We develop a CATE estimation strategy that takes advantage some of the intriguing properties of neural networks. Shows improvements to X-learner by modeling the treatment response function, the control response function, and the mapping from imputed treatment effect to the conditional average treatment effect, as neural networks. The authors propose the Y-learner to estimate conditional average treatment effect(CATE), which simultaneously updates the parameters of the outcome functions and the CATE estimator.
Device-agnostic Firmware Execution .
An architecture for tabular data, which emulates branches of decision trees and uses dense residual connectivity . This paper proposes deep neural forest, an algorithm which targets tabular data and integrates strong points of gradient boosting of decision trees. A novel neural network architecture mimicking how decision forests work to tackle the general problem of training deep models for tabular data and showcasing effectiveness on par with GBDT.
YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity. Proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed that state-of-the-art Adam algorithm.
Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs . This paper concerns security and machine learning and proposes a man-in-middle attack that alters the VAE encoding of input data so that decoded output will be misclassified.
An empirical study that examines the effectiveness of different encoder-decoder combinations for the task of dependency parsing . Empirically analyzes various encoders, decoders, and their dependencies for graph-based dependency parsing.
Teacher that trains meta-learners like humans .
We introduce an embedding space approach to constrain neural network output probability distribution. This paper introduces a method to perform semi-supervised learning with deep neural networks, and the model achieves relatively high accuracy, given a small training size. This paper incorporates label distribution into model learning when a limited number of training instances is available, and proposes two techniques for handling the problem of output label distribution being wrongly biased.
We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.
This work introduces a novel loss function for the robust training of temporal localization DNN in the presence of misaligned labels. A new loss for training models that predict where events occur in a training sequence with noisy labels by comparing smoothed label and prediction sequence.
We introduce the notion of mixed tensor decompositions, and use it to prove that interconnecting dilated convolutional networks boosts their expressive power. This paper theoretically validates that interconnecting networks with different dilations can lead to expressive efficiency using mixed tensor decomposition. The authors study dilated convolutional networks and show that intertwining two dilated convolutional networks A and B at various stages is more expressively efficient than not intertwining. Shows that the WaveNet's structural assumption of a single perfect binary tree is hindering its performance and that WaveNet-like architectures with more complex mixed tree structures perform better.
multi-task learning works . This paper presents a multi-task neural network for classification on MNIST-like datasets .
We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries. Investigates a minimax formulation of deep network learning to increase their robustness, using projected gradient descent as the main adversary. This paper proposes to look at making neural networks resistant to adversarial loss through the framework of saddle-point problems.
Many graph classification data sets have duplicates, thus raising questions about generalization abilities and fair comparison of the models. The authors discuss isomorphism bias in graph datasets, the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model, theoretically analogous to data leakage effects.
We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique. An algorithm called value iteration with negative sampling to address the covariate shift problem in imitation learning.
Contrastively-trained Structured World Models (C-SWMs) learn object-oriented state representations and a relational model of an environment from raw pixel input. The authors overcome the problem of using pixel-based losses in the construction and learning of structured world models by using a contrastive latent space.
Unsupervised methods for finding, analyzing, and controlling important neurons in NMT . This work proposes finding "meaningful" neurons in Neural Machine Translation models by ranking based on correlation between pairs of models, different epochs, or different datasets, and proposes a controlling mechanism for the models.
We present doubly sparse softmax, the sparse mixture of sparse of sparse experts, to improve the efficiency for softmax inference through exploiting the two-level overlapping hierarchy. The paper proposes the new Softmax algorithm implementation with two hierarchical levels of sparsity which speeds up the operation in language modeling.
This paper presents empirical evidence supporting the discovery of an indicator of generalization: the evolution across training of the cosine distance between each layer's weight vector and its initialization.
Models of source code that combine global and structural features learn more powerful representations of programs. A new method to model the source code for the bug repairing task using a sandwich model like [RNN GNN RNN] which significantly improves localization and repair accuracy.
Incremental-RNNs resolves exploding/vanishing gradient problem by updating state vectors based on difference between previous state and that predicted by an ODE. The authors address the problem of signal propagation in recurrent neural networks by building an attractor system for the signal transition and checking whether it converges to an equilibrium.
We provide evidence against classical claims about the bias-variance tradeoff and propose a novel decomposition for variance.
We proposed a novel deep learning image classification framework that can both accurately classify images and protect users' privacy. This paper proposes a framework which preserves the private information in the image and doesn’t compromise the usability of the image. This current work suggests using adversarial networks to obfuscate images and thus allow collecting them without privacy concerns to use them for training machine learning models.
a 2vec model for cryptocurrency transaction graphs . The paper proposes to use an autoencoder, networkX, and node2Vec to predict whether a Bitcoin address will become empty after a year, but the results are worse than an existing baseline.
Convergence proof of stochastic sub-gradients method and variations on convex-concave minimax problems . An anaysis of simultaneous stochastic subgradient, simultaneous gradient with optimism, and simultaneous gradient with anchoring in the context of minmax convex concave games. This paper analyzes the dynamics of stochastic gradient descent when applied to convex-concave games, as well as GD with optimism and a new anchored GD algorithm that converges under weaker assumptions than SGD or SGD with optimism.
We propose an algorithmic framework to schedule constellations of small spacecraft with 3-DOF re-orientation capabilities, networked with inter-sat links. This paper proposes a communication module to optimize the schedule of communication for the problem of spacecraft constellations, and compares the algorithm in distributed and centralized settings.
We proposed a novel compressed kernelized importance sampling algorithm.
We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching. A theoretical study of ridge regression by exploiting a new asymptotic characterisation of the ridge regression estimator.
We analyze the loss landscape of neural networks with attention and explain why attention is helpful in training neural networks to achieve good performance. This paper proves from the theoretical perspective that attention networks can generalize better than non-attention baselines for fixed-attention (single-layer and multi-layer) and self-attention in the single layer setting.
Mixture Model for Neural Disentanglement .
We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions . This paper studied the popular belief that deep neural networks do information compression for supervised tasks . This paper proposes a method for the estimation of mutual information for networks with unbounded activation functions and the use of L2 regularization to induce more compression.
We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer. A method for converting recordings of a specific musical instrument to another by applying CycleGAN, developed for image style transfer, to transfer spectrograms. The authors use multiple techniques/tools to enable neural timbre transfer (converting music from one instrument to another) without paired training examples. Describes a model for musical timbre transfer with the results indicating that the proposed system is effective for pitch and tempo transfer, as well as timbre adaptation.
The paper presents Deep Rewiring, an algorithm that can be used to train deep neural networks when the network connectivity is severely constrained during training. An approach to implement deep learning directly on sparsely connected graphs, allowing networks to be trained efficiently online and for fast and flexible learning. The authors provide a simple algorithm capable of training with limited memory .
Existing pruning methods fail when applied to GANs tackling complex tasks, so we present a simple and robust method to prune generators that works well for a wide variety of networks and tasks. The authors propose a modification to the classic distillation method for the task of compressing a network to address the failure of previous solutions when applied to generative adversarial networks.
we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. This paper proposes additional improvement over gradient dropping to improve communication efficiency .
We propose the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network which conditions the translation process on an exemplar image in the target domain. Discusses a core failing and need for I2I translation models. The paper explores the idea that an image has two components and applies an attention model where the feature masks that steer the translation process do not require semantic labels .
Imposing graph structure on neural network layers for improved visual interpretability. A novel regularizer to impose graph structure upon hidden layers of a Neural Network to improve the interpretability of hidden representations. Highlights the contribution of graph spectral regularizer to the interpretability of neural networks.
We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text. A proposed Residual Energy-based Model (EBM) for text generation which operates at the sentence level, and can therefore leverage BERT, and achieves lower perplexity and is preferred by human evaluation.
systematic study of large-scale cache-based image recognition models, focusing particularly on their robustness properties . This paper proposed to use memory cache to improve robustness against adversarial image examples, and concluded that using a large continous cache is not superior to hard attention.
The paper describes a flexible framework for building CNNs that are equivariant to a large class of transformations groups. A framework for building group CNN with an arbitrary Lie group G, which shows superiority over a CNN in tumor classification and landmark localization.
A benchmark of nine representative global pooling schemes reveals some interesting findings. For fine-grained classification tasks, this paper validated that maxpooling would encourage sparser feature maps than and outperform avgpooling.
Self-supervision improves few-shot recognition on small and challenging datasets without relying on extra data; Extra data helps only when it is from the same or similar domain. An empirical study of different self-supervised learning (SSL) methods, showing SSL helps more when the dataset is harder, that domain matters for training, and a method to choose samples from an unlabeled dataset.
We create abstract models of environments from experience and use them to learn new tasks faster. A methodology that uses the idea of MDP homomorphisms to transform a complex MDP with a continuous state space to a simpler one.
We expand Network Dissection to include action interpretation and examine interpretable feature paths to understand the conceptual hierarchy used to classify an action.
We propose a novel model to represent notes and their properties, which can enhance the automatic melody generation. This paper proposes a generative model of symbolic (MIDI) melody in western popular music which jointly encodes note symbols with timing and duration information to form musical "words". The paper proposes to facilitate generation of melody by representing notes as "words", representing all of the note's properties and thus allowing the generation of musical "sentences".
A method that automatically grows layers in neural networks to discover optimal depth. A framework to interleave training a shallower network and adding new layers which provides insights into the paradigm of 'growing networks'.
Exploration of in-domain representation learning for remote sensing datasets. This paper provided several standardized remote sensing data sets and showed that in-domain representation could produce better baseline results for remote sensing compared to fine-tuning on ImageNet or learning from scratch.
Avoid generating responses one word at a time by using weak supervision to training a classifier  to pick a full response. A way to generate responses for medical dialog using a classifier to select from expert-curated responses based on the conversation context.
Finite-width SGD trained CNNs vs. infinitely wide fully Bayesian CNNs. Who wins? The paper establishes a connection between infinite channel Bayesian convolutional neural network and Gaussian processes.
We scale Bayesian Inference to ImageNet classification and achieve competitive results accuracy and uncertainty calibration. An adaptive noise MCMC algorithm for image classification that dynamically adjusts the momentum and noise applied to each parameter update, and is robust to overfitting and provides an uncertainty measure with predictions.
An empirical study on fake images reveals that texture is an important cue that current fake images differ from real images. Our improved model capturing global texture statistics shows better cross-GAN fake image detection performance. The paper proposes a way to improve model performance for fake face detection in images generated by a GAN to be more generalizable based on texture information.
The Wasserstein distance is hard to minimize with stochastic gradient descent, while the Cramer distance can be optimized easily and works just as well. The manuscript proposes to use the Cramer distance to act as a loss when optimizing an objective function using stochastic gradient descent because it has unbiased sample gradients. The contribution of the article is related to performance criteria, in particular to the Wasserstein/Mallows metric .
We learn the arrow of time for MDPs and use it to measure reachability, detect side-effects and obtain a curiosity reward signal. This work proposes the h-potential as a solution to an objective that measures state-transition asymmetry in an MDP.
We formulated SGD as a Bayesian filtering problem, and show that this gives rise to RMSprop, Adam, AdamW, NAG and other features of state-of-the-art adaptive methods . The paper analyzes stochastic gradient descent through Bayesian filtering as a framework for analyzing adaptive methods. The authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior .
We introduce the idea of adversarial learning into automatic data augmentation to improve the generalization  of a targe network. A technique called Adversarial AutoAugment which dynamically learns good data augmentation policies during training using an adversarial approach.
The study introduces two approaches to enhance generalization of first-order meta-learning and presents empirical evaluation on few-shot image classification. The paper presents an empirical study of the first-order meta-learning Reptile algorithm, investigating a proposed regularization technique and deeper networks .
This paper proposes using matrix factorization at training time for neural machine translation, which can reduce model size and decrease training time without harming performance. This paper proposes to compress models using matrix factorization during training for deep neural networks of machine translation.
Different methods for analyzing BERT suggest different (but compatible) conclusions in a case study on NPIs.
In this work, we present V1Net -- a novel recurrent neural network modeling cortical horizontal connections that give rise to robust visual representations through perceptual grouping. The authors propose to modify a convolutional variant of LSTM to include horizontal connections inspired by known interactions in visual cortex.
We propose a link between permutation equivariance and compositional generalization, and provide equivariant language models . This work focuses on learning locally equivariant representations and functions over input/output words for the purposes of SCAN task.
The paper proposes an algorithm to increase the flexibility of the variational posterior in Bayesian neural networks through iterative optimization. A method for training flexible variational posterior distributions, applied to Bayesian neural nets to perform variation inference (VI) over the weights.
New state-of-the-art framework for image restoration . The paper proposes a convolutional neural network architecture that includes blocks for local and non-local attention mechanisms, which are claimed to be responsible for achieving excellent results in four image restoration applications. This paper proposes a residual non-local attention network for image restoration .
Hybrid approach to model acquisition that compensates a lack of available data with domain specific knowledge provided by experts . A domain acquisition approach that considers using a different representation for the partial domain model by using schematic mutex relations in place of pre/post conditions.
We release a dataset constructed from single-lead ECG data from 11,000 patients who were prescribed to use the {DEVICENAME}(TM) device. This paper describes a large-scale ECG dataset the authors intend to publish and provides unsupervised analysis and visualization of the dataset.
A novel Context-Gated Convolution which incorporates global context information into CNNs by explicitly modulating convolution kernels, and thus captures more representative local patterns and extract discriminative features. This paper uses global context to modulate the weights of convolutional layers and help CNNs capture more discriminative features with high performance and fewer parameters than feature map modulating.
We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping . Derives a formula for finding the minimum and maximum clipping values for uniform quantization which minimize the square error resulting from quantization, for either a Laplace or Gaussian distribution over pre-quantized value.
We propose a novel normalization method to handle small batch size cases. A method to deal with the small batch size problem of BN which applies moving average operation without too much overhead and reduces the number of statistics of BN for better stability.
ReLU MLP depth seperation proof with gemoteric arguments . A proof that deeper networks need less units than shallower ones for a family of problems.
A new GAN based few-shot learning algorithm by synthesizing  diverse and discriminative Features . A meta-learning method that learns a generative model that can augment the support set of a few-shot learner which optimizes a combination of losses.
We demonstrate how structure in data sets impacts neural networks and introduce a generative model for synthetic data sets that reproduces this impact. The paper studies how different settings of data structure affect learning of neural networks and how to mimic behavior on real datasets when learning on a synthetic one.
We train deep neural networks based on diagonal and circulant matrices, and show that this type of networks are both compact and accurate on real world applications. The authors provide a theoretical analysis of the expressive power of diagonal circulant neural networks (DCNN) and propose an initialization scheme for deep DCNNs.
We propose to leverage model distillation to learn global additive explanations in the form of feature shapes (that are more expressive than feature attributions) for models such as neural nets trained on tabular data. This paper incorporates Generalized Additive Models (GAMs) with model distillation to provide global explanations of neural nets.
A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations . This paper is about learning sentence embeddings by combining several training signals: skip-thought, predicting translation, classifying entailment relationships, and predicting the constituent parse.
We propose a neural bias annotator to benchmark models on their robustness to biased text datasets. A method to generate biased datasets for NLP, relying on a conditional adversarially regularized autoencoder (CARA).
We propose supervising VAE-style topic models by intelligently adjusting the prior on a per document basis. We find a logit-normal posterior provides the best performance. A flexible method of weakly supervising a topic model to achieve better alignment with user intuition.
First comprehensive information plane analysis of large scale deep neural networks using matrix based entropy and tensor kernels. The authors propose a tensor-kernel based estimator for mutual information estimation between high-dimensional layers in a neural network.
We propose a modular framework that can accomplish tasks specified by programs and achieve zero-shot generalization to more complex tasks. This paper investigates training RL agents with instructions and task decompositions formalized as programs, proposing a model for a program guided agent that interprets a program and proposes subgoals to an action module.
We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time. Studies the problem of learning a single convolutional filter using SGD and shows that under certain conditions, SGD learns a single convolutional filter. This paper extends the Gaussian distribution assumption to a more general angular smoothness assumption, which covers a wider family of input distributions .
The first data augmentation method specially designed for improving the general robustness of DNN without any hypothesis on the attacking algorithms. Proposes a data augmentation training method to gain model robustness against adversarial perturbations, by augmenting uniformly random samples from a fixed-radius sphere centered at training data.
Using Wasserstein-GANs to generate realistic neural activity and to detect the most relevant features present in neural population patterns. A method for simulating spike trains from populations of neurons which match empirical data using a semi-convolutional GAN. The paper proposes to use GANs for synthesizing realistic neural activity patterns .
Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance. Author experimentally found that the estimator of the existing work(STL) is biased and proposes to reduce the bias to improve the gradient estimator of the ELBO.
Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization. This paper proposes stable GradientLess Descent (GLD) algorithms that do not rely on gradient estimate.
We propose a new class of visual generative models: goal-conditioned predictors. We show experimentally that conditioning on the goal allows to reduce uncertainty and produce predictions over much longer horizons. This paper reformulates video prediction problem as interpolation instead of extrapolation by conditioning the prediction on the start and end (goal) frame, resulting in higher quality predictions.
We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.
Embedding layers are factorized with Tensor Train decomposition to reduce their memory footprint. This paper proposes a low-rank tensor decomposition model to parameterize the embedding matrix in Natural Language Processing (NLP), which compresses the network and sometimes increases test accuracy.
Fixing weight decay regularization in adaptive gradient methods such as Adam . Proposes idea to decouple the weight decay from the number of steps taken by the optimization process. The paper presents an alternative way to implement weight decay in Adam with empirical results shown . Investigates weight decay issues lied in the SGD variants and proposes the decoupling method between weight decay and the gradient-based update.
Lifelong distributional learning through a student-teacher architecture coupled with a cross model posterior regularizer.
Deep autoencoders to learn a good representation for geometric 3D point-cloud data; Generative models for point clouds. Approaches to learn GAN-type generative models using PointNet architecture and latent-space GAN.
We propose a novel method for suppressing the vulnerability of latent feature space to achieve robust and compact networks. This paper proposes "adversarial neural pruning" method of training a pruning mask and a new vulnerability suppression loss to improve accuracy and adversarial robustness.
We proposed two VAE modifications that account for negative data examples, and used them for semi-supervised anomaly detection. The papers propose two methods of VAE-like approaches for semi-supervised novelty detection, MML-VAE and DP-VAE.
New understanding of training dynamics and metrics of memorization hardness lead to efficient and provable curriculum learning. This paper formulates DIH as a curriculum leaning problem that can more effectively utilize the data to train DNNs, and derives theory on the approximation bound.
History of parallel developments in update laws and concepts between adaptive control and optimization in machine learning.
Recurrent convolution for model compression and a trick for training it, that is learning independent BN layres over steps. The author modifies the recurrent convolution neural network (RCNN) with independent batch normalization, with the experimental results on RCNN compatible with the ResNet neural network architecture when it contains the same number of layers.
Dynamic receptive fields with spatial Gaussian structure are accurate and efficient. This paper proposes a structured convolution operator to model deformations of local regions of an image, which significantly reduced the number of parameters.
A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers . A method for constructing adversarial attacks that are less detectable by humans without cost in image space by changing the target class to be similar to the original class of the image.
This paper presents noise type/position classification of various impact noises generated in a building which is a serious conflict issue in apartment complexes . This work describes the use of convolutional neural networks in a novel application area of building noise type and noise position classification.
Recurrent Neural Networks learn to  increase and reduce the dimensionality of their internal representation in a way that matches the task, depending on the dynamics of the initial network.
Instead of strict distribution alignments in traditional deep domain adaptation objectives, which fails when target label distribution shifts, we propose to optimize a relaxed objective with new analysis, new algorithms, and experimental validation. This paper suggests relaxed metrics for domain adaptation which give new theoretical bounds on the target error.
we explore the task of summary-to-article generation and propose a hierarchical generation scheme together with a jointly end-to-end reinforcement learning framework to train the hierarchical model. To address the issue of degeneration in summary-to-article generation, this paper proposes a hierarchical generation approach which first generates an intermediate sketch of the article and then the full article.
We propose counterfactual regularization to guard against adversarial domain shifts arising through shifts in the distribution of latent "style features" of images. The paper discusses ways to guard against adversarial domain shifts with counterfactual regularization by learning a classifier that is invariant to superficial changes (or "style" features) in imagess. This paper aims at robust image classification against adversarial domain shifts and the goal is achieved by avoiding using the changing style features.
We propose a meta-learner to adapt quickly on multiple tasks even one step in a few-shot setting. This paper proposes a method to meta-learn a gradient correction module in which preconditioning is parameterized by a neural network, and builds in a two-stage gradient update process during adaptation.
Question answering models that model the joint distribution of questions and answers can learn more than discriminative models . This paper proposes a generative approach to textual and visual QA, where a joint distribution over the question and answer space given the context is learned, which captures more complex relationships. This paper introduces a generative model for question answering and proposes to model p(q,a|c), factorized as p(a|c) * p(q|a,c). The authors proposes a generative QA model, which optimizes jointly the distribution of questions and answering given a document/context.
A new activation function called Displaced Rectifier Linear Unit is proposed. It is showed to enhance the training and inference performance of batch normalized convolutional neural networks. The paper compares and suggests against the usage of batch normalization after using rectifier linear units . This paper proposes an activation function, called displaced ReLU, to improve the performance of CNNs that use batch normalization.
We construct scale-equivariant convolutional neural networks in the most general form with both computational efficiency and proved deformation robustness. The authors propose a CNN architecture that is theoretically equivariant to isotropic scalings and translations by adding an extra scale-dimension to activation tensors.
We diagnose deep neural networks for 3D point cloud processing to explore the utility of different network architectures. The paper investigates different neural network architectures for 3D point cloud processing and proposes metrics for adversarial robustness, rotational robustness, and neighborhood consistency.
Utilizing the structure of distributions improves semi-implicit variational inference .
Self-imitation learning of diverse trajectories with trajectory-conditioned policy . This paper addresses hard exploration tasks by applying self-imitation to a diverse selection of trajectories from past experience, to drive more efficient exploration in sparse-reward problems, achieving SOTA results.
A method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost . A method to train a network with large capacity, only parts of which are used at inference time dependent on input, using fine-grained conditional selection and a new method of regularization, "batch shaping."
We present an end-to-end differentiable architecture that learns to map pixels to predicates, and evaluate it on a suite of simple relational reasoning tasks . A network architecture based on the multi-head self-attention module to learn a new form of relational representations, which improves data efficiency and generalization ability on curriculum learning.
We use neural networks to project superficial information out for natural language inference by defining and identifying the superficial information from the perspective of first-order logic. This paper tries to reduce superficial information in natural language inference to prevent overfitting, and introduces a graph neural network to model relation between premise and hypothesis. An approach to treat natural language inference using first-order logic and to infuse NLI models with logical information to be more robust at inference.
Algorithm for training individually fair classifier using adversarial robustness . This paper proposes a new definition of algorithmic fairness and an algorithm to provably find an ML model that satisfies the fairness contraint.
Is seeding and augmentation all you need for classifying digits in any language? This paper presents new datasets for five languages and proposes a new framework (SAT) for font image datasets generation for universal digit classification.
The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body. The paper finds that feature reuse is the dominant factor in the success of MAML, and propose new algorithms which spend much less computation than MAML.
We provide a method-agnostic algorithm for deciding when to incrementally train versus fully train and it provides a significant speedup over fully training and avoids catastrophic forgetting . This paper proposes an approach for deciding when to incrementally vs. fully retrain a model in the setting of iterative model development in slot filling tasks.
We develop a theoretical framework to characterize which reasoning tasks a neural network can learn well. The paper proposes a measure of classes of algorithmic alignment that measure how "close" neural networks are to known algorithms, proving the link between several classes of known algorithms and neural network architectures.
We explore cell-cell interactions across tumor environment contexts observed in highly multiplexed images, by image synthesis using a novel attention GAN architecture. A new method to model the data generated by multiplexed ion beam imaging by time-of-flight (MIBI-TOF) by learning the many-to-many mapping between cell types and protein markers' expression levels.
A two-stage approach consisting of sentence selection followed by span selection can be made more robust to adversarial attacks in comparison to a single-stage model trained on full context. This paper investigates an existing model and finds that a two-stage trained QA method is not more robust to adversarial attacks compared to other methods.
Verification of a human driver model based on a cognitive architecture and synthesis of a correct-by-construction ADAS from it.
A novel, hybrid deep learning approach provides the best solution to a limited-data problem (which is important to the conservation of the Hawaiian language)
We quantitatively study out-of-distribution detection in few-shot setting, establish baseline results with ProtoNet, MAML, ABML, and improved upon them. The paper proposes two new confidence scores which are more suitable for out-of-distribution detection of few-shot classification and shows that a distance metric-based approach improves performance.
This paper introduces progressive knowledge distillation for learning generative models that are recognition task oriented . This paper demonstrates easy-to-hard curriculum learning to train a generative model to improve few-shot classification.
We propose a new method for enhancing the transferability of adversarial examples by using the noise-reduced gradient. This paper postulates that an adversarial perturbation consists of a model-specific and data-specific component, and that amplification of the latter is best suited for adversarial attacks. This paper focuses on enhancing the transferability of adversarial examples from one model to another model.
We present the iterative two-pass CP decomposition flow to effectively accelerate existing convolutional neural networks (CNNs). The paper proposes a novel workflow for acceleration and compression of CNNs and also proposes a way to determine the target rank of each layer given the target overall acceleration. This paper addresses the problem of learning a low rank tensor filter operation for filtering layers in deep neural networks (DNNs).
LP-based upper bounds on the Lipschitz constant of Neural Networks . The authors study the problem of estimating the Lipschitz constant of a deep neural network with ELO activation function, formulating it as a polynomial optimisation problem.
We address multi-domain few-shot classification by building multiple models to represent this complex task distribution in a collective way and simplifying task-specific adaptation as a selection problem from these pre-trained models. This paper tackles few-shot classification with many different domains by building a pool of embedding models to capture domain-invariant and domain-specific features without a significant increase in the number of parameters.
Neural-based removal of document ink artifacts (underlines, smudges, etc.) using no manually annotated training data .
We propose a query-efficient black-box attack which uses Bayesian optimisation in combination with Bayesian model selection to optimise over the adversarial perturbation and the optimal degree of search space dimension reduction. The authors propose to use Bayesian optimization with a GP surrogate for adversarial image generation, by exploiting additive structure and using Bayesian model selection to determine an optimal dimensionality reduction.
We propose a model to learn factorized multimodal representations that are discriminative, generative, and interpretable. This paper presents 'Multimodal Factorization model' that factorizes representations into shared multimodal discriminative factors and modality specific generative factors.
We develop a hierarchical, actor-critic algorithm for compositional transfer by sharing policy components and demonstrate component specialization and related direct benefits in multitask domains as well as its adaptation for single tasks. A combination of different learning techniques for acquiring structure and learning with asymmetric data, used to train an HRL policy. The authors introduce a hierarchical policy structure for use in both single task and multitask reinforcement learning, and assess the structure's usefulness on complex robotic tasks.
We empirically count the number of linear regions of rectifier networks and refine upper and lower bounds. This paper presents improved bounds for counting the number of linear regions in ReLU networks.
We analyze the memorization properties by a convnet of the training set and propose several use-cases where we can extract some information about the training set. Illuminates the generalization/memorization properties of large and deep ConvNets and tries to develop procedures related to identifying whether an input to a trained ConvNet has actually been used to train the network.
GANs can in principle learn distributions sample-efficiently, if the discriminator class is compact and has strong distinguishing power against the particular generator class. Proposes the notion of restricted approximability, and provides a sample complexity bound, polynomial in the dimension, which is useful in investigating lack of diversity in GANs. Analyzes that the Integral Probability Metric can be a good approximation of Wasserstein distance under some mild assumptions.
In the early phase of training of deep neural networks there exists a "break-even point" which determines properties of the entire optimization trajectory. This work analyzes the optimization of deep neural networks by considering how the batch size and step-size hyper-parameters modify learning trajectories.
We propose HWGCN to mix the relevant neighborhood information at different orders to better learn node representations. The authors propose a variant of GCN, HWGCN, to consider convolution beyond 1-step neighbors, which is comparable to state-of-the-art methods.
We introduce a novel measure of flatness at local minima of the loss surface of deep neural networks which is invariant with respect to layer-wise reparameterizations and we connect flatness to feature robustness and generalization. The authors propose a notion of feature robustness which is invariant with respect to rescaling the weight and discuss the notion's relationship to generalization. This paper defines a notion of feature-robustness and combines it with epsilon representativeness of a function to describe a connection between flatness of minima and generalization in deep neural networks.
We propose to sparsify preactivations of gates and information flow in LSTM to make them constant and boost the neuron sparsity level . This paper proposed a sparsification method for recurrent neural networks by eliminating neurons with zero preactivations to obtain compact networks.
We introduce a neural network approach to assist partial differential equation solvers. The authors aim at improving the accuracy of numerical solvers by training a neural network on simulated reference data which corrects the numerical solver.
a confederated learning method that train model from horizontally and vertically separated medical data . A "confederated" machine learning method that learns across divides in medical data separated both horizontally and vertically.
This paper proposes Stochastic Quantized Activation that solves overfitting problems in FGSM adversarial training and fastly achieves the robustness comparable to multi-step training. The paper proposes a model to improve adversarial training by introducing random perturbations in the activations of one of the hidden layers .
We study the structure of noise in the brain and find it may help generalization by moving representations along in-class stimulus variations.
We present a new design, i.e., Self-Ensembling with Category-agnostic Clusters, for both closed-set and open-set domain adaptation. A new approach to open set domain adaptation, where the source domain categories are contained in the target domain categories in order to filter out outlier categories and enable adaptation within the shared classes.
We show how to learn spectral decompositions of linear operators with deep learning, and use it for unsupervised learning without a generative model. The authors propose to use a deep learning framework to solve the computation of the largest eigenvectors. This paper presents a framework to learn eigenfunctions via a stochastic process and proposes to tackle the challenge of computing eigenfunctions in a large-scale context by approximating then using a two-phase stochastic optimization process.
Applying the Riemannian SGD (RSGD) algorithm for training Tensor-Train RNNs to further reduce model parameters. The paper proposes to use Riemannian stochastic gradient algorithm for low-rank tensor train learning in deep networks. Proposes an algorithm for optimizing neural networks parametrized by Tensor Train decomposition based on the Riemannian optimization and rank adaptation, and designs a bidirectional TT LSTM architecture.
We propose a new algorithm that learns constraint-satisfying policies, and provide theoretical analysis and empirical demonstration in the context of reinforcement learning with constraints. This paper introduces a constrained policy optimization algorithm using a two-step optimization process, where policies that do not satisfy the constraint can be projected back into the constraint set.
We propose a gradient-based representation for characterizing information that deep networks have not learned. The authors present creating representations based on gradients with respect to the weights to supplement information missing from the training dataset for deep networks.
We introduce a “Zero-Shot” medical image Artifact Reduction framework, which leverages the power of deep learning but without using general pre-trained networks or any clean image reference.
We apply the informational bottleneck concept to attribution. The paper proposes a novel perturbation-based method for computing attribution/saliency maps for deep neural network based image classifiers, by injecting crafted noise into an early layer of the network.
We show the RNNs can be pruned to induce block sparsity which improves speedup for sparse operations on existing hardware . The authors propose a block sparsity pruning approach to compress RNNs, using group LASSO to promote sparsity and to prune, but with a very specialized schedule as to the pruning and pruning weight.
We propose an improvement to value iteration networks, with applications to planetary rover path planning. This paper learns a reward function based on expert trajectories using a Value Iteration Module to make the planning step differentiable .
A novel attention layer that combines self-attention and feed-forward sublayers of Transformer networks. This paper proposes a modification to the Transformer model by incorporating attention over "persistent" memory vectors into the self-attention layer, resulting in performance on par with existing models while using fewer parameters.
We efficiently find a subset of images that have higher than expected activations for some subset of nodes.  These images appear more anomalous and easier to detect when viewed as a group. The paper proposed a scheme to detect the presence of anomalous inputs based on a "subset scanning" approach to detect anomalous activations in the deep learning network.
Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks. Studies the stability of RNNs and investigation of spectral normalization to sequential predictions.
Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance. Proposing ArbNets to study weight sharing in a more systematic way by defining the weight sharing function as a hash function.
We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network. The paper provides an extension to Markov Logic Networks by removing their dependency on pre-defined first-order logic rules to model more domains in knowledge-base completion tasks.
A scalable method for learning an expressive prior over neural networks across multiple tasks. The paper presents a method for training a probabilistic model for Multitasks Transfer Learning by introducing a latent variable per task to capture the commonality in the task instances. The work proposes a variational approach to meta-learning that employs latent variables corresponding to task-specific datasets. Aims to learn a prior over neural networks for multiple tasks.
DISENTANGLED STATE SPACE MODELS . The paper presents a generative state space model using a global latent variable E to capture environment-specific information.
Bregman divergence learning for few-shot learning.
We introduce a network framework which can modify its structure during training and show that it can converge to various ML network archetypes such as MLPs and LCNs.
Domain guided augmentation of data provides a robust and stable method of domain generalization . This paper proposes a domain generalization approach by domain-dependent data augmentation . The authors introduce the CrossGrad method, which trains both a label classification task and a domain classification task.
We investigate alternative to traditional pixel image modelling approaches, and propose a generative model for vector images. This paper introduces a neural network architecture for generating sketch drawings inspired by the variational autoencoder.
We provide a study trying to see how the recent online learning rate adaptation extends the conclusion made by Wilson et al. 2018 about adaptive gradient methods, along with comparison and sensitivity analysis. Reports the results of testing several stepsize adjustment related methods including vanilla SGD, SGD with Neserov momentum, and ADAM and compares those methods with hypergradient and without.
We investigate the large-sample behaviors of the Q-value estimates and proposed an efficient exploration strategy that relies on estimating the relative discrepancies among the Q estimates. This paper presents a pure-exploration algorithm for reinforcement learning based on an asymptotic analysis of Q-values and their convergence to central limit distribution, outperforming benchmark exploration algorithms.
We train an image to image translation network that take as input the source image and a sample from a prior distribution to generate a sample from the target distribution . This paper formalizes the problem of unsupervised translation and proposes an augmented GAN framework which uses the mutual information to avoid the degenerate case . This paper formulates the problem of unsupervised one-to-many image translation and addresses the problem by minimizing the mutual information.
Learning to extract distinguishable keypoints from a proxy task, outlier rejection. This paper is devoted to the self-supervised learning of local features using Neural Guided RANSAC as an additional auxillary loss provider for improving descriptor interpolation.
We propose a formulation of intrinsic motivation that is suitable as an exploration bias in multi-agent sparse-reward synergistic tasks, by encouraging agents to affect the world in ways that would not be achieved if they were acting individually. The paper focuses on using intrinsic motivation to improve the exploration process of reinforcement learning agents in tasks that require multi-agent to achieve.
An probabilistic inference algorithm driven by neural network for graph-structured models . This paper introduces policy message passing, a graph neural network with an inference mechanism that assigns messages to edges in a recurrent fashion, indicating competitive performance on visual reasoning tasks.
We show how you can boost performance in a multitask network by tuning an adaptive multitask loss function that is learned through directly balancing network gradients. This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks.
DNNs for image segmentation can implement solutions for the insideness problem but only some recurrent nets could learn them with a specific type of supervision. This paper introduces insideness to study semantic segmentation in deep learning era, and the results can help models generalize better.
Given a pre-trained model, we explored the per-sample gradients of the model parameters relative to a task-specific loss, and constructed a linear model that combines gradients of model parameters and the activation of the model. This paper proposes to use the gradients of specific layers of convolutional networks as features in a linearized model for transfer learning and fast adaptation.
We train our face reconstruction model with adversarial loss in semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections. This paper proposes a semi-supervised and adversarial training process to exact nonlinear disentangled representations from a face image with loss functions, achieving state-of-the-art performance in face reconstruction.
This paper presents ConceptFlow that explicitly models the conversation flow in commonsense knowledge graph for better conversation generation. The paper proposes a system for generating a single-turn response to a posted utterance in an open-domain dialog setting using the diffiusion into the neighbors of the grounded concepts.
We examine the hypothesis that the entropy of solution spaces for constraints on synaptic weights (the "flexibility" of the constraint) could serve as a cost function for neural circuit development.
Infinite ensembles of infinitely wide neural networks are an interesting model family from an information theoretic perspective.
We conduct a comparative study of cross-lingual alignment vs joint training methods and unify these two previously exclusive paradigms in a new framework. This paper compares approaches to bilingual lexicon induction and shows which method performs better on lexicon, induction, and NER and MT tasks.
Combining orthogonal model compression techniques to get significant reduction in model size and number of flops required during inferencing. This paper proposes combining Tucker Decomposition with Filter pruning.
Introduces JAUNE: a methodology to replace BLEU and ROUGE score with multidimensional, model-based evaluators for assessing summaries . This paper proposes a new JAUNE metric for the evaluation of machine translation and text summarization systems, showing that their model corresponds better to ground truth similarity labels than BLEU.
new GNN formalism + extensive experiments; showing differences between GGNN/GCN/GAT are smaller than thought . The paper proposes a new Graph Neural Network architecture that uses Feature-wise Linear Modulation to condition the source-to-target node message-passing based on the target node representation.
This paper propose a novel matrix decomposition framework for simultaneous attributed network data embedding and clustering. This paper proposes an algorithm to perform jointly attribute network embedding and clustering together.
We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis while considering view-dependent effects. This submission proposes a method to handle view-dependent effects in neural rendering, which improves the robustness of existing neural rendering methods.
GANs are evaluated on synthetic datasets .
We propose an efficient and effective step size adaptation method for the gradient methods. A new step size adaptation in first-order gradient methods that establishes a new optimization problem with the first-order expansion of the loss function and regularization, where step size is treated as variable.
We shot that a wide class of manifolds can be generated by ReLU and sigmoid networks with arbitrary precision. This paper provides certain basic guarantees on when manifolds can be written as the image of a map approximated by a neural net, and stitches together theorems from manifold geometry and standard universal approximation results. This paper theoretically shows that neural-network-based generative models can approximate data manifolds, and proves that under mild assumptions neural networks can map a latent space onto a set close to the given data manifold within a small Hausdorff distance.
We design model-based reinforcement learning algorithms with theoretical guarantees and achieve state-of-the-art results on Mujuco benchmark tasks when one million or fewer samples are permitted. The paper proposed a framework to design model-based RL algorithms based on OFU that achieves SOTA performance on MuJoCo tasks.
We present additional techniques to use knowledge distillation to compress U-net by over 1000x. The authors introduced a modified distillation strategy to compress a U-net architecture by over 1000x while retaining an accuracy close to the original U-net.
This paper provides an approach to address catastrophic forgetting via Hessian-free curvature estimates . The paper proposes an approximate Laplace's method in neural network training in the continual learning setting with a low space complexity.
Method to improve simple models performance given a (accurate) complex model. The paper proposes a means of improving the predictions of a low-capacity model which shows benefits over existing approaches.
A simple and practical algorithm for learning a margin-maximizing translation-invariant or spherically symmetric kernel from training data, using tools from Fourier analysis and regret minimization. The paper proposes to learn a custom translation or rotation invariant kernal in the Fourier representation to maximize the margin of SVM. The authors propose an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together . The authors consider learning directly Fourier representations of shift/translation invariant kernels for machine learning applications with the alignment of the kernel to data as the objective function to optimize.
Probabilistic Programming that Natively Supports Causal, Counterfactual Inference .
Inference of a mean field game (MFG) model of large population behavior via a synthesis of MFG and Markov decision processes. The authors deal with inference in models of collective behavior by using inverse reinforcement learning to learn the reward functions of agents in the model.
We blend deep generative models with programmatic weak supervision to generate coordinated multi-agent trajectories of significantly higher quality than previous baselines. Proposes multi-agent sequential generative models. The paper proposes training generative models that produce multi-agent trajectories using heuristic functions that label variables that would otherwise be latent in training data .
Learn to rank learning curves in order to stop unpromising training jobs early. Novelty: use of pairwise ranking loss to directly model the probability of improving and transfer learning across data sets to reduce required training data. The paper proposes a method to rank learning curves of neural networks that can model learning curves across different datasets, achieving higher speed-ups on image classification tasks.
We show that, in continual learning settings, catastrophic forgetting can be avoided by applying off-policy RL to a mixture of new and replay experience, with a behavioral cloning loss. Proposes a particular variant of experience replay with behavior cloning as a method for continual learning.
We present a method which learns to integrate temporal information and ambiguous visual information in the context of interacting agents. The authors propose Graph VRNN which models the interaction of multiple agents by deploying a VRNN for each agent . This paper presents a graph neural network based architecture that is trained to locate and model the interactions of agents in an environment directly from pixels and show advantage of model for tracking tasks and forecasting agent locations.
We consider a simplified deep convolutional neural network model. We show that all layers of this network can be approximately learned with a proper application of tensor decomposition. Provides theoretical guarantees for learning deep convolutional neural networks using rank-one tensor decomposition. This paper proposes a learning method for a restricted case of deep convolutional networks, where the layers are limited to the non-overlapping case and have only one output channel per layer . Analyzes the problem of learning a very special class of CNNs: each layers consists of a single filter, applied to non-overlapping patches of the input.
Feedforward neural networks that can have weights pruned after training could have had the same weights pruned before training . Shows that there exists sparse subnetworks that can be trained from scratch with good generalization performance and proposes a unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. The paper examines the hypothesis that randomly initialized neural networks contain sub-networks that converge equally fast or faster and can reach the same or better classification accuracy .
In this paper we highlight  the difficulty of training sparse neural networks by doing interpolation experiments in the energy landscape .
Weight-space symmetry in neural network landscapes gives rise to numerous number of saddles and flat high-dimensional subspaces. The paper presented a low-loss method for studying the loss function with respect to parameters in a neural network from the perspective of weight-space symmetry.
signal propagation theory applied to continuous surrogates of binary nets;  counter intuitive initialisation; reparameterisation trick not helpful . The authors investigate the training dynamics of binary neural networks when using continuous surrogates, study what properties networks should have at initialization to best train, and provide concrete advice about stochastic weights at initialization. An in-depth exploration of stochastic binary networks, continuous surrogates, and their training dynamics, with insights on how to initialize weights for best performance.
We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework. This paper focuses on semi-supervised semantic dependency parsing using the CRF-autoencoder to train the model in a semi-supervised style, indicating effectiveness on low resource labeled data tasks.
DeFINE uses a deep, hierarchical, sparse network with new skip connections to learn better word embeddings efficiently. This paper describes a new method for learning deep word-level representations efficiently by using a hierarchical structure with skip-connections for the use of low dimensional input and output layers.
We successfully reproduce and give remarks on the comparison with baselines of a meta-learning approach for few-shot classification that works by backpropagating through the solution of a closed-form solver.
Dynamic parameter-reallocation enables the successful direct training of compact sparse networks, and it plays an indispensable role even when we know the optimal sparse network a-priori .
3 thrusts serving as stepping stones for robot experiential learning of vision module . Investigates is performance of existing image classifiers and object detectors.
All but the first two layers of our CNNs based acoustic models demonstrated some degree of language-specificity but freeze training enabled successful transfer between languages. The paper measures the transferability of features for each layer in CNN-based acoustic models across languages, concluding that AMs trained with 'the freeze training' technique outperformed other transferred models.
Linking Wasserstein-trust region entropic policy gradients, and the heat equation. The paper explores the connections between reinforcement learning and the theory of quadratic optimal transport . The authors studied policy gradient with change of policies limited by a trust region of Wasserstein distance in the multi-armed bandit setting, showing that in the small steps limit, the policy dynamics are governed by the heat equation (Fokker-Planck equation).
The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.
We adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model, and improves the previously best known result. This paper considered a Q-learning algorithm with UCB exploration policy for infinite-horizon MDP.
Perturbations can be used to train feedback weights to learn in fully connected and convolutional neural networks . This paper proposes a method that addresses the "weight transport" problem by estimating the weights for the backward pass using a noise-based estimator .
We identify some universal patterns (i.e., holding across architectures) in the behavior of different surrogate losses (CE, MSE, 0-1 loss) while training neural networks and present supporting empirical evidence.
