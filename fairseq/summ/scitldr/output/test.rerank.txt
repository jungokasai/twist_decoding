In this paper, we propose FearNet for incremental class learning.
We present two multi-view frameworks for learning sentence representations in an unsupervised fashion.
We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.
We extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more
We show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM),
We compare five different deep neural network architectures for character based text classification with each other.
We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations.
In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including
We propose a simple yet effective method to improve robustness of convolutional neural networks to adversarial attacks by using data dependent adaptive convolution kernels.
We propose a simple and novel approach to the problem of few-shot learning, based on ridge regression and logistic regression components.
We propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask.
We embed data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric.
We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly.
We propose a new pruning method, spatial-Winograd pruning.
Federated learning with neural networks .
We present a general-purpose method to train Markov chain Monte Carlo kernels that converge and mix quickly to their target distribution.
We develop a novel adversarial evaluation approach that can find catastrophic failures of learned agents in minutes to hours rather than days.
We propose an extremely simple modification to VAE training to reduce inference lag, we aggressively optimize the inference network before performing each model update.
We introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity
We analyze the objective of variational autoencoders and develop a simple VAE enhancement that produces crisp samples and stable FID scores.
We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.
This paper introduces a collection of new methods for end-to-end learning in latent variable models using the continuous Sinkhorn operator.
We present a differentiable neural architecture search (DNAS) framework to solve the mixed precision quantization problem.
Top-k Classification with Smooth Loss Functions .
We introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest.
We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification.
In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings.
We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework.
We bridge the gap between Adam and SGD in terms of generalization, and propose a new algorithm called normalized direction-preserving Adam, which can
We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.
We approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets.
We introduce a biologically inspired visual working memory architecture and a fully differentiable Short Term Attentive Working Memory model which uses transformational attention to learn a
We apply variational auto-Encoders to the task of musical timbre transfer and achieve state-of-the-art results.
Deep Autoencoders with Random Weight .
Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and kernel smoothing we construct a new gener
We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution.
We propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality.
We develop a framework for understanding and improving recurrent neural networks using max-affine spline operators (MASOs).
We propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs), a neuro-symbolic system for reasoning over
We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images
We present Generative Adversarial Privacy and Fairness, a data-driven framework for learning private and fair representations of the data.
Evaluation of confidence thresholding models against linear models .
We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition.
This paper proposes a neural end-to-end text- to-speech model which can control latent attributes in the generated speech that are rarely annotated
Visual Question Answering with Counting .
We propose a simple and robust training-free approach for building sentence representations.
We propose novel extensions of Prototypical Networks that are augmented with the ability to use unlabeled examples when producing prototypes.
We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models.
We proposed an end-to-end DNN for abnormality detection in medical imaging.
Are Deep Reinforcement Learning algorithms doing mapping and/or path-planning?
We propose a method to learn heteroscedastic noise models end-to-end through differentiable Bayesian Filtering.
We find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently
We develop a capsule-based neural network model to solve the semantic segmentation problem.
We propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN.
This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler.
We propose a new, multi-component energy function for energy-based Generative Adversarial Networks based on methods from the image quality assessment literature.
We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients.
We present the Recurrent Discounted Attention (RDA) unit that builds on the RWA by additionally allowing the discounting of the past.
We introduce variance layers, a different kind of stochastic layers that store information only in the variances of its weights.
We propose a novel architecture, Network of Graph Convolutional Networks (N-GCN), for semi-supervised learning on graphs.
In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation.
In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder
We argue that feature is the most important knowledge from teacher. We further present an efficient learning strategy to mimic features stage by stage.
We augment adversarial training (AT) with worst case adversarialtraining (WCAT) which improves adversarial robustness by 11% over the current
Reading Comprehension with Multiple Choice Questions .
In this paper, we start from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent
We propose a method to reduce the communication overhead of distributed deep learning.
Deep Learning for Unsupervised Domain Adaptation .
We extend the conceptor-aided backpropagation algorithm to deep feedforward networks and apply it to the MNIST tasks.
Neural Response Generation from the Perspective of Human-to-Human Conversational Corpora .
We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code.
We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition.
We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem.
Online distillation enables us to use extra parallelism to fit very large datasets about twice as fast.
We propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training.
We provide non-convex convergence rates for sign stochastic gradient descent on general non-Convex functions under transparent conditions.
We introduce a transparent middleware layer for neural network acceleration.
We propose a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response
We propose a foveated generative model that is based on a mixture of peripheral representations and style transfer forward-pass algorithms.
Over-parametrization improves the normalized margin and generalization error bounds for deep networks.
We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible.
We analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R).
In this paper, we propose phrase-based attention methods to model n-grams of tokens as attention entities.
We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling for improving novel confidence.
We present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially
We present a method to train models whose weights are a mixture of bitwidths, that allows us to more finely tune the accuracy/speed trade-
We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units.
We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables
We extend the batch adaptive stochastic gradient descent to momentum algorithm and evaluate both the BA-SGD and the batch Adaptive momentum on two deep
We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.
We investigate the impact of explicit modularity and structure on systematic generalization of neural models.
We develop Relational Forward Models, networks that learn to make accurate predictions of agents' future behavior in multi-agent environments.
We show that gradient descent on an unregularized logistic regression problem converges to the same direction as the max-margin solution.
Out-of-sample domain generalization with gray-level co-occurrence matrix .
In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild.
We present a differentiable decision tree that we connect to a variational autoencoder and a simple, differentiable loss function that allows for end
A principled and practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain.
We use a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset.
We learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest.
We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training.
In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization
We present an efficient convolution kernel for Convolutional Neural Networks on unstructured grids using parameterized differential operators while focusing on spherical signals.
We reframe the time-agnostic prediction problem as a bottlenecks problem, so we can predict frames no matter when they occur.
We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process.
We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER
We extend factorization schemes to codebook strategies, allowing compact second-order models with the same dimensionality as first order models.
We propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision.
We introduce contextual explanation networks, a class of models that learn to predict by generating and leveraging intermediate explanations.
We propose a model free, off-policy IL algorithm for continuous control.
We propose a novel graph convolutional network that generalizes CNN architectures to graph-structured data.
We investigate the learning dynamics of neural networks as they train on single classification tasks.
We present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.
We introduce state aligned vector rewards which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.
We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation.
A novel Siamese Deep Neural Network architecture is able to effectively learn from data in the presence of multiple adverse events.
We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query.
We propose the augment-REINFORCE-merge estimator that is unbiased, exhibits low variance, and has low computational complexity.
We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD.
We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model.
We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states directly.
We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities and observations.
We propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings.
We present a technique to select a representative subset of examples that is sufficient to synthesize a correct program, yet small enough to solve efficiently.
We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results.
We study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM.
Bag-of-Feature Classifiers for ImageNet .
Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state
This paper presents the formal release of MedMentions, a new manually annotated resource for the recognition of biomedical concepts.
In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm.
We propose a new Integral Probability Metric (IPM) between distributions and show how it can be used to train Generative Adversarial
We propose a new approach to train the Generative Adversarial Nets with a mixture of generators to overcome the mode collapsing problem.
We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning.
We propose a novel algorithm that jointly learns and compresses a neural network.
We show that the stochastic variance reduction technique and related approaches fail, and explore why.
We apply deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere.
We train neural networks with discrete weights using stochastic parameters.
We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.
We propose an efficient federated learning framework based on variational dropout.
We prove a boosting theory for Residual Neural Networks and propose a new algorithm to train them.
We design a non-convex objective function whose landscape is guaranteed to have the following properties:
In this paper, we release, describe, and analyze an OIE corpus called OPIEC, which was extracted from the text of English Wikipedia.
We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.
We develop a new method termed as ``"WAGE" to discretize both training and inference, where weights (W), activations (A
Fast on-the-fly sparsification of CNNs without retraining.
In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order.
We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\
We present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential
In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational
We use rate-distortion theory to solve the problem of optimizing priors in latent variable models.
We investigate backpropagating error terms only linearly.
We investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm.
We present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution.
We present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance.
We show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than
We derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task.
We derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing.
We take a first step towards a principled study of the GAN dynamics itself.
We study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting.
A variational framework for Bayesian phylogenetic analysis.
This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive models for raw audio waveform generation.
In this paper, we propose a novel scheme for both interpretation as well as explanation of deep models.
We reformulate the problem of predicting the context in which a sentence appears as a classification problem.
We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework.
We propose a novel algorithm, Deep Temporal Clustering (DTC), to naturally integrate dimensionality reduction and temporal clustering into a single end
We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios.
We propose new loss components that improve the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.
We study the interaction between nearest neighbor algorithms and neural networks in more detail.
We introduce a differentiable quantization procedure that can be optimized with gradient descent.
We propose a distributional framework for adversarial training of neural networks that operates on samples, rather than on single observations.
We propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte
We extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint.
We introduce a new approach to estimate continuous actions using actor-critic algorithms for continuous control.
We craft a specialized dropout method from three aspects, dropout location, drop out granularity, and dropout probability.
We propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples.
Binary weights and activations work because of the high-dimensional geometry of binary vectors.
We use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations.
We consider the learning of algorithmic tasks by mere observation of input-output pairs.
We propose a General and One-sample gradient that applies to many distributions associated with non-reparameterizable continuous random variables.
We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer.
We propose a general objective function to adapt the robust training method of Wong & Kolter to optimize for cost-sensitive robustness.
We propose a method to learn a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells in the pr
We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping.
We propose a simple and efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.
We propose the Maximal Divergence Sequential Auto-Encoder for binary code vulnerability detection.
We show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.
In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function.
We present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations.
We propose a large, standardized dataset and a set of 19 benchmarks to evaluate models for molecule generation and design.
We show that even the simplest neural networks can learn to make analogies with visual and symbolic inputs.
We propose a new setting in goal-oriented dialogue system to tighten the gap between information isolation on individual models between two agents.
We propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix.
We make progress towards a deeper understanding of ADAM and RMSProp in two ways.
We introduce a novel automated countermeasure called Parallel Checkpointing Learners to thwart the potential adversarial attacks and significantly improve the reliability (safety)
We present ProxylessNAS, a neural architecture search algorithm that can directly search the architectures for large-scale target tasks and target hardware platforms.
We propose a variational Dirichlet framework with entropy-based uncertainty measure to detect out-of-distribution examples.
We address the problem of learning an agent’s action space purely from visual observation.
We propose a framework for training agents to negotiate and form teams using deep reinforcement learning.
We develop unsupervised methods for discovering important neurons in neural machine translation models, and we apply these methods to control NMT translations in predictable ways.
We propose a framework that disentangles task and environment specific knowledge by separating them into two units.
We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images.
We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.
In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal.
We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields.
This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set.
We extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of "overlaps" in the convolutional
We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.
We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.
We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle.
We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature.
We propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos.
We propose a generic framework capable of learning from a noisy demonstration set, via evaluating the suitability of imitated skills.
We introduce causal implicit generative models that allow sampling from not only the true observational but also the true interventional distributions.
We provide a theoretical justification to the self-normalization properties of language models trained using Noise Contrastive Estimation.
We use affect scores from Warriner's affect lexicon to improve word representations learnt from an unlabelled corpus.
We present an unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation
We argue that the generalization performance of linear graph embedding methods is not due to the dimensionality constraint as commonly believed, but rather the small norm
We propose the quasi-hyperbolic momentum algorithm as an extremely simple alteration of SGD, averaging a plain SGD step with a momentum step.
We propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weight
We proposed a new driving model which is composed of perception module for see and think and driving module for behave, and trained it with multi-task-
We propose the MultI-Level Embedding (MILE) framework to scale up graph embedding methods to large graphs.
We propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs.
We present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective.
We construct an unbiased estimator for the maximum mean discrepancy between two probability measures P and Q and use it to train a generative neural network.
We propose Bayesian Deep Q-Network, a practical Thompson sampling based Reinforcement Learning (RL) Algorithm.
We propose the polynomial convolutional neural network, as a new design of a weight-learning efficient variant of the traditional CNN.
We propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary gener
We adapt notions of similarity using weak labels over multiple hierarchical levels to boost classification performance.
We propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is
We propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning.
We apply attention to Neural Processes and show that this improves the accuracy of predictions and expands the range of functions that can be modelled.
We propose a simple, efficient, yet effective method, known as the pixel deconvolutional layer (PixelDCL), to solve the checker
Bayesian network compression method for simultaneous pruning and few-bit quantization.
We propose a progressive weight pruning approach based on ADMM to deal with non-convex optimization problems with potentially combinatorial constraints.
We present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series.
We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as
We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other
We use the group properties of transformations to constrain the abstract representation of motion.
This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general.
We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence.
We exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of
Jackknife Variational Inference for variational autoencoders .
We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy
We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively.
We solve the graph embedding problem using variational autoencoders and apply it to the task of conditional molecule generation.
We propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1.
We present a personalized recommender system using neural network for recommending products, such as eBooks, audio-books, Mobile Apps, Video and Music.
We propose the task-GAN which extends GAN based image restoration framework to perform additional pathology recognition/classification task.
In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection.
We introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision.
This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning.
In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images.
We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of
We define the active learning problem for deep CNNs as core-set selection, i.e. choosing set of points such that a model learned over
We introduce a simple stochastic algorithm that is specific to LSTM optimization and targeted towards addressing this problem.
We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization
We systematically analyze the effect of data augmentation on some popular neural networks and conclude that data Augmentation alone---without any other explicit regularization techniques---
Gedit: A system of on-keyboard gestures for convenient mobile text editing.
DNN is a recursive solution towards maximum entropy principle, and the connection between DNN and maximum entropy well explains why typical designs such as shortcut and regular
We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic
We investigate the behavior of CNNs under class-dependent simulated label noise, which is generated based on the conceptual distance between classes of a large dataset.
We demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient
We propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation.
WAAT: A 3D Authoring Tool for the Training of Assembly Line Operators .
We prove that the widely used first-order iterative method in training GANs converges to a stationary solution with a sublinear rate.
We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards).
We develop a simple yet effective quantization scheme, nested dithered quantized SG (NDQSG), that can reduce the communication significantly without requiring
In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework.
We propose to use ensemble methods as a defense strategy against adversarial perturbations.
We propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in a dialogue system without image input.
We proposed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure.
We provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization.
We improve convergence in data-parallel neural network training by combining local and global gradients.
We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.
We cast the representation learning problem in terms of learning to communicate.
This paper introduces MetaMimic, a method to learn high-fidelity one-shot imitation policies by off-policy RL.
We extend batch normalization to more than a single mean and variance, and we detect modes of data on-the-fly, jointly normalizing samples
We propose a distillation-based approach to boost the accuracy of multilingual machine translation.
This paper investigates the role of human priors for solving video games.
We propose a new open loop hyperparameter optimization method based on k-determinantal point processes.
We investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.
We have shown that block diagonal inner product layers can reduce network size, training time and final execution time without significant harm to the network performance.
We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.
We propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards.
We analyze one and two dimensional gated recurrent units as a continuous dynamical system, and classify the dynamical features obtainable with such system.
We propose a Multi-Scale Stacked Hourglass Network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.
We present a new unsupervised method for learning general-purpose sentence embeddings.
We explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.
We use multi-way encoding to make deep models more robust to black-box and white-box attacks.
We train a neural machine translation model that produces its outputs in parallel, allowing an order of magnitude lower latency during inference.
We study the problem of how to train a neural network with one hidden layer that is robust to small adversarial perturbations.
We formulate an information-based optimization problem for supervised classification.
We train a generative model by tying a variational autoencoder to a machine, resulting in an almost perfect reconstruction of the data.
This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions.
Stochastic gradient descent implicitly performs variational inference, but not in the classical sense.
We pursue an alternative paradigm for imitation learning where an agent first explores the world without any expert supervision and then distills its experience into a goal-condition
We provide comparison between two methods for post process improvements to the baseline DSM vectors.
We present a method to quantize both weights and activations of recurrent neural networks into multiple binary codes.
A method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states.
We propose a single-shot analysis of a trained CNN that uses Principal Component Analysis to determine the number of filters that are doing significant transformations per layer,
We provide an in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.
We study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes.
We extend softmax layer with an additional constant input to represent the uncertainty of the network.
This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption.
In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in Coq.
We study how the combination of both weight and gradient quantization affects convergence of weight-quantized networks in a distributed environment.
We postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added.
We built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities.
In this paper, we propose a novel and efficient multishot framework for embedding learning in generalized knowledge graphs.
We reduce minimax curriculum learning to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods.
We develop implicit causal models, a class of causal models that leverages neural architectures with an implicit density.
In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.
We show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages.
Autodidactic Iteration is able to solve the Rubik's Cube and the 15-puzzle without relying on human data.
A differentiable model for interpreting questions, which is inspired by formal approaches to semantics.
We present DLVM, a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a
We focus on the problem of grounding language by training an agent to follow a set of natural language instructions and navigate to a 2D grid environment.
We propose a new Q\&A architecture which does not require recurrent networks.
We introduce the building blocks for constructing spherical CNNs.
We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks.
We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting
In this paper, we show how Convolutional Neural   Networks (CNNs) can be implemented using binary representations.
We propose a differentiable greedy network for the task of subset selection.
We introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.
We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep neural networks into adversarially robust classifiers.
Hierarchical Exploration for Reinforcement Learning in Large State-Action Spaces .
We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks.
We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training in this work.
To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments.
We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers.
In this paper, we propose to learn a proper prior from data for adversarial autoencoders .
This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f
We introduced UORO, an algorithm for online learning of general recurrent computational graphs that works in a streaming fashion and avoids backtracking through past activations
We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level
We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions.
We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure.
We introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes.
In this work, we propose a novel `"volumetric convolution" operation that can effectively convolve arbitrary functions in B^3.
We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on synthetic instructions.
We propose a method for sharing label information across languages by means of a language independent text encoder.
We propose the deep inside-outside recursive autoencoder, a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within
We show that short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-Horizon bias.
We present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic repres
We develop neural persistence, a topological measure for neural network structural complexity based on topological data analysis.
We introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations.
We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.
We propose a novel covariance estimator based on the Gaussian Process Latent Variable Model.
We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution.
We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of
We propose "Active Neural Localizer", a fully differentiable neural network that learns to localize efficiently.
In this paper, we propose to leverage the hints from a well-trained ART model to train the NART model.
In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non- linear activation
We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning.
We propose an easy method to train VAEs with binary or categorically valued latent representations.
We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized
This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training.
We introduce an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness.
We investigate methods to attack graph convolutional networks by adding fake nodes.
In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer.
We present a method to address model uncertainty in continuous Bayes-Adaptive Markov Decision Processs using a Bayesian filter and an augmented state-
We use neural network divergences to evaluate unconditional image generation, and we show how using them can improve the notion of generalization.
We make the first step to open the black box by introducing dialogue acts into open domain dialogue generation.
We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity.
We present a novel approach for the certification of neural networks which combines scalable overapproximation methods with precise (mixed integer) linear programming.
We investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization.
We propose a novel regularization method that penalizes covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement
This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning.
We study the effect of network structure on halting time and show that larger models can potentially train faster despite the increasing computational requirements of each training step.
We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs.
We advocate for a unified treatment of the two problems of classification and clustering in computer vision and suggest that hierarchical frameworks that progressively build complex patterns on top
We develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.
We undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities.
We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations
We propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.
We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction.
We propose a new negative sampling scheme for the set of problems with positive and unlabeled data, and we extend the softmax formulation to make it
In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation.
This paper aims to establish formal connections between GANs and VAEs through a new formulation of them.
We present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to learn representative data prototypes.
We propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space
We report on a new modular network architecture that applies an attentional mechanism to sensor selection for multi-sensor setups.
We propose PrivyNet to enable cloud-based DNN training while protecting the data privacy simultaneously.
We propose a Recurrent GAN and Recurrent Conditional GAN to produce realistic real-valued multi-dimensional time series, with an emphasis on
Emphasis Effects in Visualization .
We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures.
We propose an efficient way to significantly reduce the number of parameters while at the same time improving the performance of deep convolutional networks.
We train quantized neural networks by noise injection and learned clamping, which improve the accuracy.
We present Leap, a framework that achieves this by transferring knowledge across learning processes.
We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones.
This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks.
We impose hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures.
We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies and a zero-sum dynamic game for designing robust deep RL policies.
In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robust
In this paper, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in
We analyze the modularity of generative models based on the counterfactual manipulation of their internal variables.
We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries.
We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks.
We propose a method which can visually explain the classification decision of deep neural networks.
We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping.
We show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models that provides robustness.
We present a novel neural network and training algorithm for multi-task learning and demonstrate significant improvement over cross-stitch networks on MNIST, mini-
We use a collection of non-negative stochastic gates to determine which weights to set to zero.
We propose a graph neural network that removes all the intermediate fully-connected layers, and replaces them with attention mechanisms that respect the structure of the graph.
We train a generative autoencoder with maximum likelihood, without restrictions on architectures or latent dimensionalities.
We learn embeddings in a Riemannian product manifold combining hyperbolic, spherical, and Euclidean components to obtain a space of
We propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models.
We formulate a new variation of variational auto-encoders with a Spike and Slab prior distribution to obtain truly sparse representations.
We propose a simple method to address the degradation problem in plain feed-forward networks by posing the learning of weights in deep networks as a constrained optimization problem
We propose a method to reduce both the number of weights and bits-depth of deep learning models without sacrificing the accuracy.
In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received
In this paper, we propose deep convolutional generative adversarial networks that learn to produce a mental image of the input image as internal representation of
We present a framework that combines techniques in \textit{formal methods} with hierarchical reinforcement learning (HRL) to learn hierarchical
In this paper we propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy
When a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want.
We present a novel, systematic, unifying taxonomy to categorize existing methods for deep learning.
We prove the expressive power theorem for a class of recurrent neural networks that correspond to the Tensor Train decomposition.
We propose ProbGAN, a novel probabilistic framework for GANs, which iteratively learns a distribution over generators with a carefully crafted prior.
In this paper, we propose taking into account the inherent confidence information produced by models when studying adversarial perturbations.
This paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method.
We propose two new compression methods, which combine quantization and distillation of larger teacher networks into smaller student networks.
We aim to improve neural machine translation via source side dependency syntax but without explicit annotation.
We accelerate model-free RL in sparse reward settings using a single trajectory to construct a curriculum for a given task.
We propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states.
We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation.
We accelerate recurrent neural networks on a GPU by up to 6x over the next best algorithm for a hidden layer of size 2304, batch size 4
In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless
This paper proposes a novel framework for efficient multi-task reinforcement learning.
We propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful sub
We propose a principled approach to train networks with significantly improved resistance to large variations between training and testing data.
We propose a simple modification to standard neural network architectures, thermometer encoding, which significantly increases the robustness of the network to adversarial examples.
We prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic.
We consider the problem of exploration in meta reinforcement learning.
We propose a new class of probabilistic neural-symbolic models that provide interpretable explanations of their decision making in the form of programs.
We use formal verification techniques to assess the effectiveness of attack and defense techniques against adversarial examples.
This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other.
This paper proposes stacked u-nets, which iteratively combine features from different resolution scales while maintaining resolution.
We propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text.
We introduce a new computational approach that decodes movement intent from a low-dimensional latent representation of the neural data.
We apply experimental paradigms from developmental psychology to neural network-based language learning, exploring the conditions under which established human biases and learning effects emerge.
We propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos.
We perform a systematic study of system resource efficiency for super-resolution, within the context of a variety of architectural and low-precision approaches originally developed
Fourier Analysis of Deep ReLU Networks .
Hedged Instance Embedding for Image Recognition .
We present tensor contraction layers which can replace the ordinary fully-connected layers in a neural network.
We explore ways of incorporating bilingual dictionaries to enable semi-supervised neural machine translation.
We propose a new curiosity method which uses episodic memory to form the novelty bonus.
We compare neural networks' ability to capture and exploit the structure of logical expressions against an entailment prediction task.
We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network.
We accelerate the training of RNNs with only linear sequential dependencies over the sequence length using the parallel scan algorithm.
We propose an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context.
We compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario.
We study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orth
We prove that learnable gates in a recurrent model formally provide quasi-invariance to general time transformations in the input data.
We argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should
We present DL2, a system for training and querying neural networks with logical constraints.
We present Genetic Policy Optimization (GPO), a genetic algorithm for sample-efficient deep policy optimization.
We propose a principled alternative approach, called ProxQuant, that formulates quantized network training as a regularized learning problem instead and optimizes it
We prove that, with high probability in the limit of N\rightarrow\infty datapoints, the volume of differentiable regions of
We identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models.
We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$.
We present a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of
We show that encoder-decoder GANs cannot guarantee learning of the full distribution.
We propose a framework to generate natural and legible adversarial examples that lie on the data manifold, using the recent advances in generative adversarial networks
We extend the Kronecker-factor Approximate Curvature method to handle recurrent neural networks.
We propose a new type of prior distribution for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a
We use deep learning techniques to classify cyanobacterial cells sampled by hyperspectral imaging.
We introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three differentiators.
We extend the theory of Random Features to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs.
We propose a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data.
We propose new online learning approaches for supervised dimension reduction.
This paper presents a storage-efficient learning model for embedded and mobile devices having a limited amount of on-chip data storage.
We define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data.
This paper describes SCAN, a new framework for learning such abstractions in the visual domain.
We propose a Latent Topic Conversational Model that augments the seq2seq model with a neural topic component to better model human-human conversations.
We extend the graph embedding method to the graph data-set and propose a new graph pooling method.
In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution
This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time
We combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs
We present a framework for interpretable continual learning (ICL) and propose a new metric to assess their quality.
Mixed-Precision CNN Training for ImageNet-1K .
We present an end-to-end speech recognition system, leveraging a simple letter-based ConvNet acoustic model.
We propose Invariant Encoding Generative Adversarial Networks, a novel GAN framework that introduces such a mapping for individual samples from the data
We propose a method for learning the dependency structure between latent variables in deep latent variable models.
Scale-invariant deep neural network architecture for changepoint detection .
We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning.
We provide a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is.
We use automatic search techniques to discover novel activation functions for deep neural networks.
We introduce a novel bottom-up approach to expand representations in fixed-depth architectures.
We train a feedforward network without backpropagation in a similar fashion to BID19's Equilibrium Propagation.
We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation.
We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample
We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient,
We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.
We address several limitations of the baseline negated CNN architecture by proposing two further optimized architectures employing (feature) group-level fusion weights.
We develop a mean field theory for batch normalization in fully-connected feedforward neural networks.
We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.
A deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow.
We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates.
Convolutional neural networks do not have a shape-bias.
We propose a generative model and a conditional variant built on such a disentangled latent space.
We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights,
In this work we develop a novel policy gradient method for the automatic learning of policies with options.
The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data.
We introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.
This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.
We explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.
We show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average
We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds.
We propose to execute DNNs with dynamic and sparse graph structure for compressive memory and accelerative execution during both training and inference.
We extend the idea of frequentist Information-Directed Sampling to a practical RL exploration algorithm that can account for heteroscedasticity.
We address the problem of learning structured policies for continuous control.
We propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization.
We introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD).
We apply spectral energy analysis to the problem of neural network compression and show how this can be used for simultaneous compression and domain adaptation.
We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering.
We proposed the notion of Conditional Network Embeddings (CNEs), which seek an embedding of a network that maximally adds information with
We provide an analysis framework and sufficient conditions that guarantee the convergence of the Adam-type methods for non-convex stochastic optimization.
This paper describes a simplistic architecture named AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data.
We propose TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer.
We propose NASH, a simple and fast method for automated neural architecture search based on a hill climbing strategy, network morphisms, and short optimization runs
We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.
We propose a method based on the Generative Adversarial Networks (GAN) framework for the task of novelty detection.
We present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks.
We introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE
We present principled schemes to control the arrangement of examples into minibatches through randomized microbatches.
We propose an algorithm which combines the general pre-trained word embedding vectors with those generated on the task-specific training set to address this issue.
We propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer
We propose a novel asynchronous distributed algorithm that reduces or completely eliminates worker idle time due to communication overhead.
This paper aims to raise people's awareness about the security of the quantized models.
We extend existing RNN models by learning to skip state updates and shortens the effective size of the computational graph.
We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers.
We learn strong heuristics for several combinatorial optimization problems using attention layers and a simple rollout method.
We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters.
We find that LSTMs, when properly regularised, outperform more recent models.
We investigate the role of residual and highway connections in deep neural networks for speech enhancement.
We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks.
We present a framework that combines techniques in formal methods with reinforcement learning (RL) that allows for the convenient specification of complex temporal temporal dependent tasks with logical
We apply multi-modal generative models by means of a Variational Auto Encoder to sensor fusion and bi-directional modality exchange.
We develop a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.
A two-timescale network architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale.
We propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost
We apply a 3-branch Siamese Convolutional Neural Network to the problem of detecting image duplication and re-use in scientific work.
We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data.
We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional
We propose Convolutional Random Fields (ConvCRFs), a novel CRF design that can be trained in terms of convolutions and can be
We propose a framework which validates robustness of any Question Answering model through model explainers.
We propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators.
We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images.
We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples.
We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.
We develop a recursive mini-batch algorithm for learning deep hard-threshold networks, including the popular but poorly justified straight-through estimator.
Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks.
In this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism for visual active tracking.
We propose a method to learn a hierarchical word embedding in a speciﬁc order to capture the hypernymy.
We study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of aSelf-organ
We propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precise computation.
We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data.
We demonstrate three caveats to the information bottleneck method and propose a functional to recover the IB curve in all cases.
We prove, under two sufficient conditions, that idealised models can have no adversarial examples.
We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output.
We propose a new measure that can predict the generalization gap of deep neural networks, based on the concept of margin distribution.
We propose a new algorithm to learn a one-hidden-layer convolutional neural network with commonly used overlapping patches.
We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable.
We apply the Wasserstein-2 metric proximal on the generators of GANs and obtain an easy-to-implement regularizer.
We develop a weighted mini-bucket approach for bounding the maximum expected utility (MEU) of influence diagrams.
We revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. the input.
We show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively.
We propose a method to learn stochastic activation functions for use in probabilistic neural networks.
We prove that Deep diagonal-circulant ReLU networks with bounded width and small depth can approximate a deep ReLU network.
StarHopper: A Touch Screen Interface for Efficient Camera Drone Navigation .
We propose a model, called "bi-directional block self-attention network (Bi-BloSAN)", for RNN/CNN-
We propose the Coarse-grain Fine-grain Coattention Network, a new question answering model that combines information from evidence across multiple documents.
We present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework.
We propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use.
We propose InstaGAN, a generative adversarial network that incorporates the instance information and improves multi-instance transfiguration.
The parameter-function map of deep neural networks should be exponentially biased towards simple functions.
We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.
We address the task of forward prediction on sequences of probability distributions with a recurrent architecture.
We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional
We present a principled method for learning reduced network architectures in a data-driven way using reinforcement learning.
We propose a new set of losses named moment reconstruction losses that simply replace the reconstruction loss.
We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other.
This paper outlines a new approach for learning underlying game state embeddings irrespective of the visual rendering of the game.
Stochastic Gradient Descent for Recurrent Neural Networks .
By properly defining the neuron manifold, we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network.
We train a single neural network executable at different widths, permitting instant and adaptive accuracy-efficiency trade-offs at runtime.
We analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and study how these models generalize across
We propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model for domain adaptation.
We develop GraphWave, a method that represents each node’s local network neighborhood via a low-dimensional embedding using spectral graph wavelet diffusion
Mixed Reality Driving Simulator .
We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods.
We leverage natural language produced by people in an interactive communication task to develop neural listener and speaker models with strong capacity for generalization.
We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties.
We present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.
Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential
We view molecule optimization as a graph-to-graph translation problem and propose a novel method to learn to translate molecular graphs.
We propose an approach to learn a fast iterative solver tailored to a specific domain.
We introduce functional variational Bayesian neural networks, which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes.
We embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry.
We investigate a canonical architecture, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings.
We compute deep convolutional network generators by inverting a fixed embedding operator.
We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference.
Inspired by the prediction error minimization and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN).
We revisit the robustness arguments of Xu & Mannor, and present ensemble robustness, to characterize the generalization performance of deep learning algorithms.
We show how neural attention and meta learning can be used in combination with autoregressive models to enable effective few-shot density estimation.
We provide the first theoretical guarantees for generalization of over-parameterized neural networks.
We identify decision states by examining where the model accesses the goal state through the bottleneck.
We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search.
This paper studies the unsupervised problem of a generative model exploiting graph convolution.
We leverage the implicit regularization effect of stochastic gradient descent to identify and on-the-fly discard mislabeled examples.
We study the problem of attacking a BNN through the lens of combinatorial and integer optimization.
We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token.
We introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically.
We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples.
We show that a discriminator set is discriminative whenever its linear span is dense in the set of bounded continuous functions.
We show how to train deep residual networks reliably without normalization.
We make two proposals to learn better metric than SeqGAN: partial reward function and expert-based reward function training.
We propose Locally Disentangled Factors (LDF), a powerful new approach to decomposing the training of generative models.
In this paper, we propose a new way to leverage visual knowledge for sentence representations.
